<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tai&#39;s Blog</title>
    <link>https://yunpengtai.top/</link>
    <description>Recent content on Tai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 25 Oct 2023 13:16:00 +0800</lastBuildDate><atom:link href="https://yunpengtai.top/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>å¤§æ¨¡å‹çš„æ•°å­¦ä¹‹è·¯</title>
      <link>https://yunpengtai.top/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E4%B9%8B%E8%B7%AF/</link>
      <pubDate>Wed, 25 Oct 2023 13:16:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E4%B9%8B%E8%B7%AF/</guid>
      <description>é—®é¢˜ LLM é€šè¿‡å¤§é‡çš„è¯­æ–™æ¥å»ºæ¨¡ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡ï¼Œè¿™ç§è®­ç»ƒæ–¹å¼ä¿ƒæˆ LLM æˆä¸ºä¸€ä¸ªã€Œæ–‡ç§‘ç”Ÿã€ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ç¦å¯¹ä»¥ä¸‹å‡ ä¸ªé—®é¢˜å¥½å¥‡ï¼š LLM ç›®å‰åœ¨æ•°å­¦é—®é¢˜ä¸Šå–å¾—çš„è¿›å±•</description>
    </item>
    
    <item>
      <title>Efficient Tricks for LLMs</title>
      <link>https://yunpengtai.top/posts/efficient-tricks-for-llm/</link>
      <pubDate>Fri, 13 Oct 2023 17:26:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/efficient-tricks-for-llm/</guid>
      <description>å¦‚ä½•é«˜æ•ˆè®­ç»ƒæˆ–æ¨ç†å¤§æ¨¡å‹ä¸€èˆ¬åœ¨ä¸¤ç‚¹ï¼šå¦‚ä½•è£…å¾—ä¸‹ä»¥åŠå¦‚ä½•æ›´å¿« è¿™é‡Œè®²ä¸€äº›ä¸»è¦çš„å¹¶è¡Œæ¦‚å¿µï¼Œä¸ä¼šæ·±æŒ–åŸç†ï¼Œåªä¼šä»‹ç» key pointsï¼Œçœ‹å®ƒä»¬åˆ†åˆ«ä¸ºåŠ é€Ÿå’Œ</description>
    </item>
    
    <item>
      <title>æ”¾å¤§é•œä¸‹çš„ InfoNCE</title>
      <link>https://yunpengtai.top/posts/infonce/</link>
      <pubDate>Fri, 14 Jul 2023 18:02:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/infonce/</guid>
      <description>åŒºåˆ†çœŸå®æ ·æœ¬ å‰é¢çš„ä¸¤ç§æ˜¯ä¸ºäº†å»ä¼°è®¡é…åˆ†å‡½æ•°ï¼Œæ¥ä¸‹æ¥è¦ä»‹ç»çš„ InfoNCE è™½ç„¶å¸¦ä¸ª NCEï¼Œä½†è¿™ä¸ªçš„ç›®çš„ä¸æ˜¯è¦é¢„ä¼°é…åˆ†å‡½æ•°ï¼Œä»–æ˜¯ç›´æ¥åƒä¸Šç¯‡åº”ç”¨ NCE çš„æ–¹æ³•ä¸€æ ·ï¼Œ</description>
    </item>
    
    <item>
      <title>NCEçš„æœ‹å‹ä»¬</title>
      <link>https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/</link>
      <pubDate>Sat, 08 Jul 2023 21:52:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/</guid>
      <description>åœ¨Noise Contrastive Estimationä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº† NCE ç®—æ³•ï¼Œå…¶å®è¿˜æœ‰å¾ˆå¤šè·Ÿå®ƒç±»ä¼¼çš„ç®—æ³•ï¼Œç»§ç»­ä»¥æ–‡æœ¬ç”Ÿæˆä¸ºä¾‹ï¼ŒåŸºäºä¸Šä¸‹æ–‡$\boldsymbo</description>
    </item>
    
    <item>
      <title>Numerical Stability</title>
      <link>https://yunpengtai.top/posts/numerical-stability/</link>
      <pubDate>Sun, 25 Jun 2023 21:13:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/numerical-stability/</guid>
      <description>Why å½“è®¡ç®—æ¶‰åŠåˆ°å®æ•°åŸŸæ—¶ï¼Œæ¯”å¦‚åœ†å‘¨ç‡çš„$\pi$ï¼Œå› ä¸ºå°æ•°éƒ¨åˆ†æ˜¯æ— ç©·çš„ï¼Œè®¡ç®—æœºæ˜¯æ— æ³•å‡†ç¡®è¡¨ç¤ºï¼Œå› è€Œåªä¼šç”¨è¿‘ä¼¼çš„å€¼è¿›è¡Œæ›¿ä»£ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œè¯¯å·®ç›¸å¯¹</description>
    </item>
    
    <item>
      <title>Bias Variance Decomposition</title>
      <link>https://yunpengtai.top/posts/bias-variance-decomposition/</link>
      <pubDate>Wed, 21 Jun 2023 11:05:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/bias-variance-decomposition/</guid>
      <description>å¼•è¨€ æˆ‘ä»¬è§„å®šï¼Œè®­ç»ƒé›†è®°ä¸º$\mathcal{D}$ï¼Œæˆ‘ä»¬ä»ä¸­å–ä¸€ä¸ªæ ·æœ¬$\boldsymbol{x}$ï¼Œå…¶è®­ç»ƒé›†æ ‡ç­¾ä¸º$y_{\mathca</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>https://yunpengtai.top/posts/noise-contrastive-estimation/</link>
      <pubDate>Mon, 29 May 2023 14:40:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/noise-contrastive-estimation/</guid>
      <description>éš¾ä»¥æ‰¿å—ä¹‹é‡ æ–‡æœ¬ç”Ÿæˆæ˜¯ NLP ä»»åŠ¡ä¸­æ¯”è¾ƒå…¸å‹çš„ä¸€ç±»ï¼Œè®°å‚æ•°ä¸º$\boldsymbol{\theta }$ï¼Œç»™å®šçš„ context ä¸º$\boldsymbol{c}$</description>
    </item>
    
    <item>
      <title>Fast Greedy MAP Inference for DPP</title>
      <link>https://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/</link>
      <pubDate>Tue, 16 May 2023 10:50:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/</guid>
      <description>é—®é¢˜ å…ˆè§„å®šä¸€äº›æœ¯è¯­ï¼šè®°é€‰ä¸­å…ƒç´ æ„æˆçš„é›†åˆä¸º$\mathcal{S}$ï¼Œæœªé€‰ä¸­æ„æˆçš„å…ƒç´ è®°ä¸º$\mathcal{R}$ï¼Œ$\mathbf{L}</description>
    </item>
    
    <item>
      <title>Determinantal Point Process</title>
      <link>https://yunpengtai.top/posts/determinantal-point-process/</link>
      <pubDate>Fri, 21 Apr 2023 21:05:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/determinantal-point-process/</guid>
      <description>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé¢ä¸´ä¸€ä¸ªé—®é¢˜ï¼šç»™å®šä¸€ä¸ªé›†åˆ$\mathbf{S}$ï¼Œä»ä¸­å¯»æ‰¾$k$ä¸ªæ ·æœ¬æ„æˆå­é›†$\mathbf{V}$ï¼Œå°½é‡ä½¿å¾—å­</description>
    </item>
    
    <item>
      <title>Generalized Linear Models</title>
      <link>https://yunpengtai.top/posts/generalized-linear-models/</link>
      <pubDate>Fri, 17 Feb 2023 10:52:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/generalized-linear-models/</guid>
      <description>å®šä¹‰ è‹¥ä¸€ä¸ªåˆ†å¸ƒèƒ½å¤Ÿä»¥ä¸‹è¿°æ–¹å¼è¿›è¡Œè¡¨ç¤ºï¼Œåˆ™ç§°ä¹‹ä¸ºæŒ‡æ•°æ—ï¼ˆ Exponential Familyï¼‰çš„ä¸€å‘˜ $$ p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) $$ å…¶ä¸­$\eta$è¢«ç§°ä¸ºåˆ†å¸ƒçš„è‡ªç„¶å‚æ•°ï¼ˆnat</description>
    </item>
    
    <item>
      <title>æ–°çš„ä¸»é¢˜</title>
      <link>https://yunpengtai.top/posts/hello-world/</link>
      <pubDate>Sun, 19 Jun 2022 11:23:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/hello-world/</guid>
      <description>è¿™æ˜¯åŸºäº Hugo ç³»åˆ—ä¸»é¢˜ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œå› ä¸ºä¹‹å‰æ˜¯åœ¨ Jekyll ä¸Šè¿›è¡Œæ¸²æŸ“ï¼Œæ•…è€Œ Hello Worldä¹Ÿæœ‰æ›´æ–° ä¸ºå•¥å˜åŠ¨ é‚£ä¹ˆä¸ºå•¥ä» Jekyll å˜åˆ° Hugo å‘¢ï¼ŸåŸå› å…¶å®æœ‰å‡ ç‚¹ï¼š ä¹‹å‰ç”¨çš„ä¸»é¢˜çœ‹</description>
    </item>
    
    <item>
      <title>Diving in distributed training in PyTorch</title>
      <link>https://yunpengtai.top/posts/diving-in-distributed-training/</link>
      <pubDate>Sun, 20 Nov 2022 21:37:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/diving-in-distributed-training/</guid>
      <description>é‰´äºç½‘ä¸Šæ­¤ç±»æ•™ç¨‹æœ‰ä¸å°‘æ¨¡ç³Šä¸æ¸…ï¼Œå¯¹åŸç†ä¸å¾—å…¶æ³•ï¼Œä»£ç ä¹Ÿéš¾è·‘é€šï¼Œæ•…è€ŒèŠ±äº†å‡ å¤©ç»†ç©¶äº†ä¸€ä¸‹ç›¸å…³åŸç†å’Œå®ç°ï¼Œæ¬¢è¿æ‰¹è¯„æŒ‡æ­£ï¼ å…³äºæ­¤éƒ¨åˆ†çš„ä»£ç ï¼Œå¯ä»¥å»è¿™</description>
    </item>
    
    <item>
      <title>Going Deeper into Back-propagation</title>
      <link>https://yunpengtai.top/posts/back-propagation/</link>
      <pubDate>Wed, 07 Sep 2022 11:10:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/back-propagation/</guid>
      <description>1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.
$$ \mathbf{w}^{\tau + 1} = \mathbf{w}^{\tau} - \eta \nabla_{\mathbf{w}^{\tau}} E \tag{1.1} $$
where \(\eta, \tau, E\) label learning rate (\(\eta &amp;gt; 0\)), the iteration step and the loss function.</description>
    </item>
    
    <item>
      <title>Tips for Training Neural Networks</title>
      <link>https://yunpengtai.top/posts/tips-for-training-neural-networks/</link>
      <pubDate>Sat, 30 Jul 2022 19:43:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/tips-for-training-neural-networks/</guid>
      <description>Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog&amp;rsquo;s interesting part.
Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others&amp;rsquo; work (e.</description>
    </item>
    
    <item>
      <title>Quotes of Mathematicians</title>
      <link>https://yunpengtai.top/posts/quotes-of-mathematicians/</link>
      <pubDate>Sat, 23 Jul 2022 09:56:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/quotes-of-mathematicians/</guid>
      <description>Life is complex, and it has both real and imaginary parts. â€” Someone
Basically, I&amp;rsquo;m not interested in doing research and I never have been&amp;hellip; I&amp;rsquo;m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. â€” David Blackwell
To not know maths is a severe limitation to understanding the world. â€” Richard Feynman</description>
    </item>
    
    <item>
      <title>Retrieval-Enhanced Transformer</title>
      <link>https://yunpengtai.top/posts/retrieval-enhanced-transformer/</link>
      <pubDate>Sun, 19 Jun 2022 21:06:00 +0800</pubDate>
      
      <guid>https://yunpengtai.top/posts/retrieval-enhanced-transformer/</guid>
      <description>Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training &amp;amp; Evaluation set:
\(\text{MassiveText}\) for both training &amp;amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \(128K\) tokens During training, we retrieving \(600B\) tokens from the training The evaluation contains \(1.75T\) tokens Test set leakage:
Due to the huge retrieving database, the test set may have appeared in the training set.</description>
    </item>
    
    <item>
      <title>ğŸ™‹ğŸ»â€â™‚ï¸About</title>
      <link>https://yunpengtai.top/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yunpengtai.top/about/</guid>
      <description>Hi! You can check my CV here.
Here is Yunpeng Tai. I am a geek about mathematics, machine learning and NLP.
Iâ€™m currently trying to get a PhD offer from ML or NLP. If you like my post, you can contact me. I major in Computer Science in Suzhou University of Science and Technology. Currently I am focusing on the In-Context Learning of LLMs. And I am familiar with the topics below:</description>
    </item>
    
    
    
    <item>
      <title>ğŸ¤Friends</title>
      <link>https://yunpengtai.top/friends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yunpengtai.top/friends/</guid>
      <description>ç§‘å­¦ç©ºé—´ è‡´åŠ›äºåˆ†äº«ç§‘å­¦ä¹‹ç¾ æ—ä¸€äºŒçš„Wiki æ—ä¸€äºŒçš„æ¨¡å› å’Œæƒ³æ³• å­¦ä¹ è€…çš„æ•°å­—èŠ±å›­ pimgeekçš„ç¬”è®°ä¸æ€è€ƒ å±±èŒ¶èŠ±èˆ å•æ¥ªåœ¨è®°å½•è‡ªå·±çš„ç”Ÿæ´» å°é¹¿ç”Ÿæ´»å¿—</description>
    </item>
    
    <item>
      <title>ğŸ¥‚Sponsor</title>
      <link>https://yunpengtai.top/sponsor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yunpengtai.top/sponsor/</guid>
      <description>é€šå¸¸å†™ä¸€ç¯‡æ–‡ç« ä»æ„æ€ï¼Œä»£ç å®ç°ï¼Œæ–‡ç« æ’ç‰ˆï¼Œæ–¹å‘è°ƒç ”éƒ½ä¼šèŠ±è´¹ä¸å°‘æ—¶é—´ï¼ŒåŒæ—¶ç½‘ç«™å¹³æ—¶è¿è¥å¼€é”€ä¹Ÿæœ‰ä¸€äº›ï¼Œå¦‚æœä½ è®¤ä¸ºæœ‰æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¯ä»¥æ‰“èµæˆ‘ï¼Œé¼“</description>
    </item>
    
  </channel>
</rss>
