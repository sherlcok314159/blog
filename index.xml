<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tai&#39;s Blog</title>
    <link>http://yunpengtai.top/</link>
    <description>Recent content on Tai&#39;s Blog</description>
    <image>
      <title>Tai&#39;s Blog</title>
      <url>http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 24 May 2024 18:10:00 +0800</lastBuildDate><atom:link href="http://yunpengtai.top/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to prompt LLM better?</title>
      <link>http://yunpengtai.top/posts/better-prompt/</link>
      <pubDate>Fri, 24 May 2024 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/better-prompt/</guid>
      <description>prompt åœ¨äººä¸ LLM çš„äº’åŠ¨ä¸­èµ·ç€å…³é”®çš„ä½œç”¨ï¼Œå¥½çš„ prompt å¯ä»¥è®© LLMã€Œæ€è€ƒã€æ›´å¤šä¸€äº›ï¼Œé‚£ä¹ˆå¦‚ä½•æ›´å¥½åœ°ç†è§£ prompt çš„ç»„æˆï¼Œä»¥åŠè®¾è®¡ prompt æ¥å®Œæˆæƒ³è¦çš„ä»»åŠ¡ä¾¿æˆäº†ä¸»è¦çš„ç›®æ ‡</description>
    </item>
    
    <item>
      <title>å¤§æ¨¡å‹çš„æ•°å­¦ä¹‹è·¯</title>
      <link>http://yunpengtai.top/posts/llm-math/</link>
      <pubDate>Wed, 25 Oct 2023 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/llm-math/</guid>
      <description>é—®é¢˜ LLM é€šè¿‡å¤§é‡çš„è¯­æ–™æ¥å»ºæ¨¡ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡ï¼Œè¿™ç§è®­ç»ƒæ–¹å¼ä¿ƒæˆ LLM æˆä¸ºä¸€ä¸ªã€Œæ–‡ç§‘ç”Ÿã€ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ç¦å¯¹ä»¥ä¸‹å‡ ä¸ªé—®é¢˜å¥½å¥‡ï¼š LLM ç›®å‰åœ¨æ•°å­¦é—®é¢˜ä¸Šå–å¾—çš„è¿›å±•</description>
    </item>
    
    <item>
      <title>Efficient Tricks for LLMs</title>
      <link>http://yunpengtai.top/posts/efficient-tricks-for-llms/</link>
      <pubDate>Fri, 13 Oct 2023 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/efficient-tricks-for-llms/</guid>
      <description>å¦‚ä½•é«˜æ•ˆè®­ç»ƒæˆ–æ¨ç†å¤§æ¨¡å‹ä¸€èˆ¬åœ¨ä¸¤ç‚¹ï¼šå¦‚ä½•è£…å¾—ä¸‹ä»¥åŠå¦‚ä½•æ›´å¿« è¿™é‡Œè®²ä¸€äº›ä¸»è¦çš„å¹¶è¡Œæ¦‚å¿µï¼Œä¸ä¼šæ·±æŒ–åŸç†ï¼Œåªä¼šä»‹ç» key pointsï¼Œçœ‹å®ƒä»¬åˆ†åˆ«ä¸ºåŠ é€Ÿå’Œ</description>
    </item>
    
    <item>
      <title>æ”¾å¤§é•œä¸‹çš„ InfoNCE</title>
      <link>http://yunpengtai.top/posts/infonce/</link>
      <pubDate>Fri, 14 Jul 2023 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/infonce/</guid>
      <description>åŒºåˆ†çœŸå®æ ·æœ¬ å‰é¢çš„ä¸¤ç§æ˜¯ä¸ºäº†å»ä¼°è®¡é…åˆ†å‡½æ•°ï¼Œæ¥ä¸‹æ¥è¦ä»‹ç»çš„ InfoNCE è™½ç„¶å¸¦ä¸ª NCEï¼Œä½†è¿™ä¸ªçš„ç›®çš„ä¸æ˜¯è¦é¢„ä¼°é…åˆ†å‡½æ•°ï¼Œä»–æ˜¯ç›´æ¥åƒä¸Šç¯‡åº”ç”¨ NCE çš„æ–¹æ³•ä¸€æ ·ï¼Œ</description>
    </item>
    
    <item>
      <title>NCE çš„æœ‹å‹ä»¬</title>
      <link>http://yunpengtai.top/posts/nce-friends/</link>
      <pubDate>Sat, 08 Jul 2023 09:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/nce-friends/</guid>
      <description>åœ¨Noise Contrastive Estimationä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº† NCE ç®—æ³•ï¼Œå…¶å®è¿˜æœ‰å¾ˆå¤šè·Ÿå®ƒç±»ä¼¼çš„ç®—æ³•ï¼Œç»§ç»­ä»¥æ–‡æœ¬ç”Ÿæˆä¸ºä¾‹ï¼ŒåŸºäºä¸Šä¸‹æ–‡$\boldsymbo</description>
    </item>
    
    <item>
      <title>Numerical Stability</title>
      <link>http://yunpengtai.top/posts/numerical-stability/</link>
      <pubDate>Sun, 25 Jun 2023 21:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/numerical-stability/</guid>
      <description>Why å½“è®¡ç®—æ¶‰åŠåˆ°å®æ•°åŸŸæ—¶ï¼Œæ¯”å¦‚åœ†å‘¨ç‡çš„$\pi$ï¼Œå› ä¸ºå°æ•°éƒ¨åˆ†æ˜¯æ— ç©·çš„ï¼Œè®¡ç®—æœºæ˜¯æ— æ³•å‡†ç¡®è¡¨ç¤ºï¼Œå› è€Œåªä¼šç”¨è¿‘ä¼¼çš„å€¼è¿›è¡Œæ›¿ä»£ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œè¯¯å·®ç›¸å¯¹</description>
    </item>
    
    <item>
      <title>Bias Variance Decomposition</title>
      <link>http://yunpengtai.top/posts/bias-variance-decomposition/</link>
      <pubDate>Wed, 21 Jun 2023 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/bias-variance-decomposition/</guid>
      <description>å¼•è¨€ æˆ‘ä»¬è§„å®šï¼Œè®­ç»ƒé›†è®°ä¸º$\mathcal{D}$ï¼Œæˆ‘ä»¬ä»ä¸­å–ä¸€ä¸ªæ ·æœ¬$\boldsymbol{x}$ï¼Œå…¶è®­ç»ƒé›†æ ‡ç­¾ä¸º$y_{\mathca</description>
    </item>
    
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>http://yunpengtai.top/posts/noise-contrastive-estimation/</link>
      <pubDate>Mon, 29 May 2023 10:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/noise-contrastive-estimation/</guid>
      <description>éš¾ä»¥æ‰¿å—ä¹‹é‡ æ–‡æœ¬ç”Ÿæˆæ˜¯ NLP ä»»åŠ¡ä¸­æ¯”è¾ƒå…¸å‹çš„ä¸€ç±»ï¼Œè®°å‚æ•°ä¸º$\boldsymbol{\theta }$ï¼Œç»™å®šçš„ context ä¸º$\boldsymbol{c}$</description>
    </item>
    
    <item>
      <title>Fast Greedy MAP Inference for DPP</title>
      <link>http://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/</link>
      <pubDate>Tue, 16 May 2023 10:20:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/</guid>
      <description>é—®é¢˜ å…ˆè§„å®šä¸€äº›æœ¯è¯­ï¼šè®°é€‰ä¸­å…ƒç´ æ„æˆçš„é›†åˆä¸º$\mathcal{S}$ï¼Œæœªé€‰ä¸­æ„æˆçš„å…ƒç´ è®°ä¸º$\mathcal{R}$ï¼Œ$\mathbf{L}</description>
    </item>
    
    <item>
      <title>Determinantal Point Process</title>
      <link>http://yunpengtai.top/posts/determinantal-point-process/</link>
      <pubDate>Fri, 21 Apr 2023 15:20:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/determinantal-point-process/</guid>
      <description>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé¢ä¸´ä¸€ä¸ªé—®é¢˜ï¼šç»™å®šä¸€ä¸ªé›†åˆ$\mathbf{S}$ï¼Œä»ä¸­å¯»æ‰¾$k$ä¸ªæ ·æœ¬æ„æˆå­é›†$\mathbf{V}$ï¼Œå°½é‡ä½¿å¾—å­</description>
    </item>
    
    <item>
      <title>Generalized Linear Models</title>
      <link>http://yunpengtai.top/posts/generalized-linear-models/</link>
      <pubDate>Fri, 17 Feb 2023 14:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/generalized-linear-models/</guid>
      <description>å®šä¹‰ è‹¥ä¸€ä¸ªåˆ†å¸ƒèƒ½å¤Ÿä»¥ä¸‹è¿°æ–¹å¼è¿›è¡Œè¡¨ç¤ºï¼Œåˆ™ç§°ä¹‹ä¸ºæŒ‡æ•°æ—ï¼ˆ Exponential Familyï¼‰çš„ä¸€å‘˜ $$ \begin{equation} p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) \end{equation} $$ å…¶ä¸­$\eta$è¢«ç§°ä¸ºåˆ†å¸ƒçš„è‡ªç„¶å‚æ•°ï¼ˆn</description>
    </item>
    
    <item>
      <title>Diving in distributed training in PyTorch</title>
      <link>http://yunpengtai.top/posts/dive-in-distributed-training/</link>
      <pubDate>Sun, 20 Nov 2022 10:20:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/dive-in-distributed-training/</guid>
      <description>é‰´äºç½‘ä¸Šæ­¤ç±»æ•™ç¨‹æœ‰ä¸å°‘æ¨¡ç³Šä¸æ¸…ï¼Œå¯¹åŸç†ä¸å¾—å…¶æ³•ï¼Œä»£ç ä¹Ÿéš¾è·‘é€šï¼Œæ•…è€ŒèŠ±äº†å‡ å¤©ç»†ç©¶äº†ä¸€ä¸‹ç›¸å…³åŸç†å’Œå®ç°ï¼Œæ¬¢è¿æ‰¹è¯„æŒ‡æ­£ï¼ä»£ç å¼€æºåœ¨æ­¤ï¼š DL-Tools Cache effective tools for deep</description>
    </item>
    
    <item>
      <title>Going Deeper into Back-Propagation</title>
      <link>http://yunpengtai.top/posts/deep-back-propagation/</link>
      <pubDate>Wed, 07 Sep 2022 18:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/deep-back-propagation/</guid>
      <description>1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.
$$ \boldsymbol{w}^{\tau + 1} = \boldsymbol{w}^{\tau} - \eta \nabla_{\boldsymbol{w}^{\tau}} E \tag{1.1} $$
where $\eta, \tau, E$ label learning rate ($\eta &amp;gt; 0$), the iteration step and the loss function. Wait!</description>
    </item>
    
    <item>
      <title>Tips for Training Neural Networks</title>
      <link>http://yunpengtai.top/posts/tips-for-training-nn/</link>
      <pubDate>Sat, 30 Jul 2022 11:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/tips-for-training-nn/</guid>
      <description>Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog&amp;rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,</description>
    </item>
    
    <item>
      <title>Quotes of Mathematicians</title>
      <link>http://yunpengtai.top/posts/quotes/</link>
      <pubDate>Sat, 23 Jul 2022 11:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/quotes/</guid>
      <description>Life is complex, and it has both real and imaginary parts. â€” Someone Basically, Iâ€™m not interested in doing research and I never have beenâ€¦ Iâ€™m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. â€” David Blackwell To not know maths is</description>
    </item>
    
    <item>
      <title>æ–°çš„ä¸»é¢˜</title>
      <link>http://yunpengtai.top/posts/hello-world/</link>
      <pubDate>Sun, 19 Jun 2022 11:10:00 +0800</pubDate>
      
      <guid>http://yunpengtai.top/posts/hello-world/</guid>
      <description>è¯¥ç½‘ç«™æ‰€ç”¨æ‰€æœ‰æºç å‡åœ¨æ­¤ä»“åº“ï¼Œæ¬¢è¿ä½¿ç”¨ï¼š MyPaperMod This is the demo of my improved PaperMod theme. You can visit the introduction: https://yunpengtai.top/posts/hello-world/ HTML è¿™æ˜¯åŸºäº Hugo ç³»åˆ—ä¸»é¢˜ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œå› ä¸ºä¹‹å‰æ˜¯åœ¨ Jekyll ä¸Šè¿›è¡Œæ¸²æŸ“ï¼Œæ•…è€Œ Hello World ä¹Ÿ</description>
    </item>
    
    <item>
      <title>ğŸ™‹ğŸ»â€â™‚ï¸About</title>
      <link>http://yunpengtai.top/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://yunpengtai.top/about/</guid>
      <description>about</description>
    </item>
    
    
    
    <item>
      <title>ğŸ¤å‹äºº</title>
      <link>http://yunpengtai.top/friends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://yunpengtai.top/friends/</guid>
      <description>friends</description>
    </item>
    
  </channel>
</rss>
