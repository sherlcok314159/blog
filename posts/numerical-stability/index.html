<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Numerical Stability | Tai's Blog</title><meta name=keywords content="Algorithm Analysis"><meta name=description content="Why 当计算涉及到实数域时，比如圆周率的$\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=https://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{tags:"all",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Numerical Stability"><meta property="og:description" content="Why 当计算涉及到实数域时，比如圆周率的$\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对"><meta property="og:type" content="article"><meta property="og:url" content="https://yunpengtai.top/posts/numerical-stability/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-25T21:13:00+08:00"><meta property="article:modified_time" content="2023-06-25T21:13:00+08:00"><meta property="og:site_name" content="Tai's Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Numerical Stability","item":"https://yunpengtai.top/posts/numerical-stability/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Numerical Stability","name":"Numerical Stability","description":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对","keywords":["Algorithm Analysis"],"articleBody":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对较小影响不大；然而，如果数值大于某个特定值之后变成了$\\inf$（数值上溢 Overflow ），还有一种情况是当数值特别小时，会被近似为$0$（数值下溢 Underflow），这两种情形若继续计算误差将会进一步累积，那么就可能导致原本理论上成立的到实现时就不行了，此时就有必要对数值稳定性进行分析\n举个例子：\nimport numpy a = np.array([65599.], dtype=np.float16) print(a) b = np.array([1e-10], dtype=np.float16) print(b) [inf] # Overflow [0.] # Underflow 基础知识 再进一步解释解决方法时还是先铺垫基础知识：\n众所周知，各种数值在计算机底层是通过比特（bit）来进行表示的，有$0$和$1$，比如用$8$个比特就可以表示$[-2^{7}, 2^{7}-1]$（有一位是符号位）\n那么 float16 的意思是用$16$位比特来表示浮点数，同理 float32 的意思是用$32$位比特\n按照表示精度来看： $$ \\text{float64 \u003e float32 \u003e float16} $$ 按照占用空间来看：\n$$ \\text{float16 \u003c float32 \u003c float64 } $$\n$\\inf$的意思是超过了可以表示的范围，有$-\\inf$和$\\inf$两种，而NaN的产生大概可以分为几种情况：\n对负数开根号 对$\\inf$进行运算 除以$0$ 那么如何查看不同表示的范围呢？\nnp.finfo(np.float16) finfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16) 当然，numpy 这里有个小坑，就是你输入的值较大或略微超过范围时，反而会用另一个数来表示，只有大到一定程度时，才会用$\\inf$，详见官网的release notes\nFloating-point arrays and scalars use a new algorithm for decimal representations, giving the shortest unique representation. This will usually shorten float16 fractional output, and sometimes float32 and float128 output. float64 should be unaffected.\na = np.array([65504.], dtype=np.float16) print(a) b = np.array([65388.], dtype=np.float16) print(b) c = np.array([65700.], dtype=np.float16) print(c) [65500.] [65380.] [inf] 注意，这里的大是相对于计算精度来说的，而不是你感觉的，当你把精度调成float32，上面都会打印原来输入的结果\ne之殇 在上高中时，特别喜爱$e^{x}$，有很多好的性质，比如求导等于本身，然而在很多数值溢出的情形，总有它的参与，这是因为机器学习中很多东西都会和它挂钩，比如各种激活函数便有它的影子\n看个例子：\na = np.exp(np.array([654.], dtype=np.float16)) print(a) [inf] 那么我如何知道输入大概多大会导致溢出呢？可以参见下述公式，当输入为$x$，计算$e^{x}$用十进制数表示大概有多少位\n$$ \\log_{10}(e^x) = x \\log_{10}(e) $$\n举个例子，上面我们看到float16当最大位数是$5$位（科学计数法后面得$+1$），那么根据上述公式你就可以算出最大可被接受的$x$，进而进行一些后处理：\ndef compute_max(n): return int(n * math.log(10)) print(compute_max(5)) # 11 我们来试试看：\na = np.exp(np.array([11.], dtype=np.float16)) print(a) b = np.exp(np.array([12.], dtype=np.float16)) print(b) [59870.] [inf] 接下来按照消除溢出的方法看看几大类常见例子：\n归一化指数 当$\\exp(x_{i})$形式出现，就会考虑通过代数恒等变换使得指数上多一些部分来进行归一化\nsoftmax 不妨假设$\\boldsymbol{x} \\in \\mathbb{R}^{n}$，当某个$x_{i}$特别大时，$\\exp(x_{i})$就会出现数值上溢，然后当分子分母都是$\\inf$的时候就会出现NaN，$0$是因为分母是$\\inf$导致的\nimport numpy as np def softmax(x): exp = np.exp(x) return exp / exp.sum(-1) x = np.array([-1., 20000, 0.1], dtype=np.float16) softmax(x) # array([ 0., nan, 0.]) 那么，有什么好的方法来防止这种数值溢出呢，我们通过一些不改变原式的代数运算可以做到：\n$$ \\begin{align} \\mathrm{softmax}(x_{j}) \u0026 = \\frac{e^{x_{j}}}{\\sum_{i} e^{x_{i}}} \\\\ \u0026= {\\color{blue}\\frac{c}{c}} \\cdot \\frac{ e^{x_{j}}}{ \\sum_{i} e^{x_{i}}} \\\\ \u0026= \\frac{e^{x_{j} + \\log c}}{ \\sum_{i} e^{x_{i} +\\log c}} \\end{align} $$ 观察上式，不难发现，我们可以控制常数$c$来对$x_{i}$进行规范化，相当于加上偏移量（offset）\n比较简单的做法即为设置$\\log c = -\\max(\\boldsymbol{x})$\n那么：\n$$ \\mathrm{softmax}(x_{j}) = \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i} - \\max(\\boldsymbol{x})}} $$\n代码实现即为：\ndef softmax(x): x -= max(x) exp = np.exp(x) return exp / exp.sum(-1) softmax(x) # array([0., 1., 0.]) 因为最大的数减去自身变为了$0$，$e^{0} = 1$，就不会有什么影响了\n这个与PyTorch官方实现也是一致的：\nimport torch import torch.nn.functional as F x = torch.tensor([-1., 20000, 0.1]) F.softmax(x) # tensor([0., 1., 0.]) Logsumexp 同理再看一个类似的：\n$$ \\begin{align} \\text{Logsumexp}(\\boldsymbol{x}) \u0026= \\log \\sum_{i} e^{x_{i}} \\\\ \u0026= \\log \\sum_{i} e^{x_{i}} \\frac{c}{c} \\\\ \u0026= \\log \\left( \\frac{1}{c} \\sum_{i} e^{x_{i}} e^{\\log c}\\right) \\\\ \u0026= -\\log c + \\log \\sum_{i} e^{x_{i} + \\log c} \\end{align} $$\n取$\\log c=-\\max(\\boldsymbol{x})$，那么：\n$$ \\text{logsumexp}(\\boldsymbol{x}) = \\max(\\boldsymbol{x}) + \\log \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})} $$\ndef logsumexp(x): maximum = max(x) x -= maximum exp = np.exp(x) return maximum + np.log(exp.sum(-1)) 跟 PyTorch 官方实现一致：\nprint(torch.logsumexp(x, dim=-1, keepdim=False)) print(logsumexp(x.numpy())) # tensor(200000.) # 200000.0 展开log内部 就是将log明显可能出现数值溢出的部分拆出来，刚刚logsumexp就是利用了这个道理\nlog-softmax 尽管softmax现在稳定了，然而log-softmax还是有风险溢出，比如上面的$\\log 0$，这也是数值上溢\n$$ \\mathrm{LogSoftmax}(x_{j}) = \\log\\left(\\frac{e^{x_{j}}}{\\sum_{i} e ^{x_{i}}}\\right) $$ 我们将其拆开： $$ \\begin{align} \\mathrm{LogSoftmax}(x_{j}) \u0026 = \\log \\left( \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}} \\right) \\\\ \u0026= \\log(e^{x_{j} - \\max(\\boldsymbol{x})}) - \\log \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) \\\\ \u0026= x_{j} - \\max(\\boldsymbol{x}) - \\log \\underbrace{ \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) }_{ \\ge 1 } \\end{align} $$\n因为所有的$x_{i}$中肯定有最大的一个，那么$\\exp(x_{i} -\\max(\\boldsymbol{x}))=1$，剩下的肯定是正数\ndef log_softmax(x): x -= max(x) exp = np.exp(x) return x - np.log(exp.sum(-1)) 同样与PyTorch一致，后面是两个框架显示机制不同，从这个角度也可以看出，都是32位时，PyTorch会更精准\nx = torch.tensor([-1., 200000, 0.1]) print(F.log_softmax(x, dim=-1)) print(log_softmax(x.numpy())) # tensor([-200001.0000, 0.0000, -199999.9062]) # array([-200001. , 0. , -199999.9], dtype=float32) 截断 当输入大于某种阈值，直接输出原来的输入\nSoftplus 下面是PyTorch官方对于Softplus的实现\n$$ \\text{Softplus}({x}) = \\begin{cases} \\log (1 + e^{x}), \u0026 x \\leq \\text{threshold} \\\\ x, \u0026 \\text{otherwise} \\end{cases} $$ 官方的意思很简单，当$x$大于阈值（这里是置之为$20$），直接不变输出\neps 急救包 当分母可能出现为$0$时，给它加上一个较小的正数$\\varepsilon$\nLayer Normalization $$ \\text{LayerNorm}(\\boldsymbol{x}) = \\gamma \\left(\\frac{\\boldsymbol{x} - \\bar{\\boldsymbol{x}}}{\\sigma + {\\color{blue}\\varepsilon}} \\right) + \\beta $$ 分母加上一个$\\varepsilon$来防止变为$0$：\nclass LayerNorm(nn.Module): def init(self, features, eps=1e-6): super().__init__() self.gamma = nn.Parameter(torch.ones(features)) self.beta = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.gamma * (x - mean) / (std + self.eps) + self.beta ","wordCount":"2093","inLanguage":"en","datePublished":"2023-06-25T21:13:00+08:00","dateModified":"2023-06-25T21:13:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunpengtai.top/posts/numerical-stability/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"https://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://yunpengtai.top/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yunpengtai.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yunpengtai.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yunpengtai.top/friends/ title=Friends><span>Friends</span></a></li><li><a href=https://yunpengtai.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=https://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Numerical Stability</h1><div class=post-meta><span title='2023-06-25 21:13:00 +0800 CST'>June 25, 2023</span>&nbsp;·&nbsp;2093 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#why aria-label=Why>Why</a></li><li><a href=#%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86 aria-label=基础知识>基础知识</a></li><li><a href=#e%e4%b9%8b%e6%ae%87 aria-label=e之殇>e之殇</a></li><li><a href=#%e5%bd%92%e4%b8%80%e5%8c%96%e6%8c%87%e6%95%b0 aria-label=归一化指数>归一化指数</a><ul><li><a href=#softmax aria-label=softmax>softmax</a></li><li><a href=#logsumexp aria-label=Logsumexp>Logsumexp</a></li></ul></li><li><a href=#%e5%b1%95%e5%bc%80log%e5%86%85%e9%83%a8 aria-label=展开log内部>展开log内部</a><ul><li><a href=#log-softmax aria-label=log-softmax>log-softmax</a></li></ul></li><li><a href=#%e6%88%aa%e6%96%ad aria-label=截断>截断</a><ul><li><a href=#softplus aria-label=Softplus>Softplus</a></li></ul></li><li><a href=#eps-%e6%80%a5%e6%95%91%e5%8c%85 aria-label="eps 急救包">eps 急救包</a><ul><li><a href=#layer-normalization aria-label="Layer Normalization">Layer Normalization</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h3 id=why>Why<a hidden class=anchor aria-hidden=true href=#why>#</a></h3><p>当计算涉及到实数域时，比如圆周率的<code>$\pi$</code>，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对较小影响不大；然而，如果数值大于某个特定值之后变成了<code>$\inf$</code>（数值上溢 Overflow ），还有一种情况是当数值特别小时，会被近似为<code>$0$</code>（数值下溢 Underflow），这两种情形若继续计算误差将会进一步累积，那么就可能导致原本理论上成立的到实现时就不行了，此时就有必要对数值稳定性进行分析</p><p>举个例子：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>65599.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>1e-10</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>inf</span><span class=p>]</span> <span class=c1># Overflow</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>0.</span><span class=p>]</span> <span class=c1># Underflow</span>
</span></span></code></pre></div><h3 id=基础知识>基础知识<a hidden class=anchor aria-hidden=true href=#基础知识>#</a></h3><p>再进一步解释解决方法时还是先铺垫基础知识：</p><p>众所周知，各种数值在计算机底层是通过比特（bit）来进行表示的，有<code>$0$</code>和<code>$1$</code>，比如用<code>$8$</code>个比特就可以表示<code>$[-2^{7}, 2^{7}-1]$</code>（有一位是符号位）</p><p>那么 <code>float16</code> 的意思是用<code>$16$</code>位比特来表示浮点数，同理 <code>float32</code> 的意思是用<code>$32$</code>位比特</p><p>按照表示精度来看：
<code>$$ \text{float64 > float32 > float16} $$</code>
按照占用空间来看：</p><p><code>$$ \text{float16 &lt; float32 &lt; float64 } $$</code></p><p><code>$\inf$</code>的意思是超过了可以表示的范围，有<code>$-\inf$</code>和<code>$\inf$</code>两种，而NaN的产生大概可以分为几种情况：</p><ul><li>对负数开根号</li><li>对<code>$\inf$</code>进行运算</li><li>除以<code>$0$</code></li></ul><p>那么如何查看不同表示的范围呢？</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>finfo</span><span class=p>(</span><span class=n>resolution</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=nb>min</span><span class=o>=-</span><span class=mf>6.55040e+04</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=mf>6.55040e+04</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div><p>当然，numpy 这里有个小坑，就是你输入的值较大或略微超过范围时，反而会用另一个数来表示，只有大到一定程度时，才会用<code>$\inf$</code>，详见官网的<a href=https://docs.scipy.org/doc/numpy-1.14.0/release.html#many-changes-to-array-printing-disableable-with-the-new-legacy-printing-mode>release notes</a></p><blockquote><p>Floating-point arrays and scalars use a new algorithm for decimal representations, giving the shortest unique representation. This will usually shorten float16 fractional output, and sometimes float32 and float128 output. float64 should be unaffected.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>65504.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>65388.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>65700.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>65500.</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>65380.</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>inf</span><span class=p>]</span>
</span></span></code></pre></div><p>注意，这里的大是相对于计算精度来说的，而不是你感觉的，当你把精度调成<code>float32</code>，上面都会打印原来输入的结果</p><h3 id=e之殇>e之殇<a hidden class=anchor aria-hidden=true href=#e之殇>#</a></h3><p>在上高中时，特别喜爱<code>$e^{x}$</code>，有很多好的性质，比如求导等于本身，然而在很多数值溢出的情形，总有它的参与，这是因为机器学习中很多东西都会和它挂钩，比如各种激活函数便有它的影子</p><p>看个例子：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>654.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>inf</span><span class=p>]</span>
</span></span></code></pre></div><p>那么我如何知道输入大概多大会导致溢出呢？可以参见下述公式，当输入为<code>$x$</code>，计算<code>$e^{x}$</code>用十进制数表示大概有多少位</p><p><code>$$ \log_{10}(e^x) = x \log_{10}(e) $$</code></p><p>举个例子，上面我们看到<code>float16</code>当最大位数是<code>$5$</code>位（科学计数法后面得<code>$+1$</code>），那么根据上述公式你就可以算出最大可被接受的<code>$x$</code>，进而进行一些后处理：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_max</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>n</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>compute_max</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span> <span class=c1># 11</span>
</span></span></code></pre></div><p>我们来试试看：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>11.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>12.</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>59870.</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>inf</span><span class=p>]</span>
</span></span></code></pre></div><p>接下来按照消除溢出的方法看看几大类常见例子：</p><h3 id=归一化指数>归一化指数<a hidden class=anchor aria-hidden=true href=#归一化指数>#</a></h3><p>当<code>$\exp(x_{i})$</code>形式出现，就会考虑通过代数恒等变换使得指数上多一些部分来进行归一化</p><h4 id=softmax>softmax<a hidden class=anchor aria-hidden=true href=#softmax>#</a></h4><p>不妨假设<code>$\boldsymbol{x} \in \mathbb{R}^{n}$</code>，当某个<code>$x_{i}$</code>特别大时，<code>$\exp(x_{i})$</code>就会出现数值上溢，然后当分子分母都是<code>$\inf$</code>的时候就会出现NaN，<code>$0$</code>是因为分母是<code>$\inf$</code>导致的</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>exp</span> <span class=o>/</span> <span class=n>exp</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>20000</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># array([ 0., nan, 0.])</span>
</span></span></code></pre></div><p>那么，有什么好的方法来防止这种数值溢出呢，我们通过一些不改变原式的代数运算可以做到：</p><p><code>$$ \begin{align} \mathrm{softmax}(x_{j}) & = \frac{e^{x_{j}}}{\sum_{i} e^{x_{i}}} \\ &= {\color{blue}\frac{c}{c}} \cdot \frac{ e^{x_{j}}}{ \sum_{i} e^{x_{i}}} \\ &= \frac{e^{x_{j} + \log c}}{ \sum_{i} e^{x_{i} +\log c}} \end{align} $$</code>
观察上式，不难发现，我们可以控制常数<code>$c$</code>来对<code>$x_{i}$</code>进行规范化，相当于加上偏移量（offset）</p><p>比较简单的做法即为设置<code>$\log c = -\max(\boldsymbol{x})$</code></p><p>那么：</p><p><code>$$ \mathrm{softmax}(x_{j}) = \frac{e^{x_{j} - \max(\boldsymbol{x})}}{\sum_{i} e^{x_{i} - \max(\boldsymbol{x})}} $$</code></p><p>代码实现即为：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=nb>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>exp</span> <span class=o>/</span> <span class=n>exp</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># array([0., 1., 0.])</span>
</span></span></code></pre></div><p>因为最大的数减去自身变为了<code>$0$</code>，<code>$e^{0} = 1$</code>，就不会有什么影响了</p><p>这个与PyTorch官方实现也是一致的：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>20000</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># tensor([0., 1., 0.])</span>
</span></span></code></pre></div><h4 id=logsumexp>Logsumexp<a hidden class=anchor aria-hidden=true href=#logsumexp>#</a></h4><p>同理再看一个类似的：</p><p><code>$$ \begin{align} \text{Logsumexp}(\boldsymbol{x}) &= \log \sum_{i} e^{x_{i}} \\ &= \log \sum_{i} e^{x_{i}} \frac{c}{c} \\ &= \log \left( \frac{1}{c} \sum_{i} e^{x_{i}} e^{\log c}\right) \\ &= -\log c + \log \sum_{i} e^{x_{i} + \log c} \end{align} $$</code></p><p>取<code>$\log c=-\max(\boldsymbol{x})$</code>，那么：</p><p><code>$$ \text{logsumexp}(\boldsymbol{x}) = \max(\boldsymbol{x}) + \log \sum_{i} e^{x_{i}-\max(\boldsymbol{x})} $$</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>logsumexp</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>maximum</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=n>maximum</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>maximum</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>exp</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></div><p>跟 PyTorch 官方实现一致：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>logsumexp</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>False</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>logsumexp</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>numpy</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># tensor(200000.)</span>
</span></span><span class=line><span class=cl><span class=c1># 200000.0</span>
</span></span></code></pre></div><h3 id=展开log内部>展开log内部<a hidden class=anchor aria-hidden=true href=#展开log内部>#</a></h3><p>就是将log明显可能出现数值溢出的部分拆出来，刚刚logsumexp就是利用了这个道理</p><h4 id=log-softmax>log-softmax<a hidden class=anchor aria-hidden=true href=#log-softmax>#</a></h4><p>尽管softmax现在稳定了，然而log-softmax还是有风险溢出，比如上面的<code>$\log 0$</code>，这也是数值上溢</p><p><code>$$ \mathrm{LogSoftmax}(x_{j}) = \log\left(\frac{e^{x_{j}}}{\sum_{i} e ^{x_{i}}}\right) $$</code>
我们将其拆开：
<code>$$ \begin{align} \mathrm{LogSoftmax}(x_{j}) & = \log \left( \frac{e^{x_{j} - \max(\boldsymbol{x})}}{\sum_{i} e^{x_{i}-\max(\boldsymbol{x})}} \right) \\ &= \log(e^{x_{j} - \max(\boldsymbol{x})}) - \log \left( \sum_{i} e^{x_{i}-\max(\boldsymbol{x})}\right) \\ &= x_{j} - \max(\boldsymbol{x}) - \log \underbrace{ \left( \sum_{i} e^{x_{i}-\max(\boldsymbol{x})}\right) }_{ \ge 1 } \end{align} $$</code></p><p>因为所有的<code>$x_{i}$</code>中肯定有最大的一个，那么<code>$\exp(x_{i} -\max(\boldsymbol{x}))=1$</code>，剩下的肯定是正数</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=nb>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>exp</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></div><p>同样与PyTorch一致，后面是两个框架显示机制不同，从这个角度也可以看出，都是32位时，PyTorch会更精准</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>200000</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>numpy</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=c1># tensor([-200001.0000, 0.0000, -199999.9062])</span>
</span></span><span class=line><span class=cl><span class=c1># array([-200001. , 0. , -199999.9], dtype=float32)</span>
</span></span></code></pre></div><h3 id=截断>截断<a hidden class=anchor aria-hidden=true href=#截断>#</a></h3><p>当输入大于某种阈值，直接输出原来的输入</p><h4 id=softplus>Softplus<a hidden class=anchor aria-hidden=true href=#softplus>#</a></h4><p>下面是PyTorch官方对于Softplus的<a href=https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus>实现</a></p><p><code>$$ \text{Softplus}({x}) = \begin{cases} \log (1 + e^{x}), & x \leq \text{threshold} \\ x, & \text{otherwise} \end{cases} $$</code>
官方的意思很简单，当<code>$x$</code>大于阈值（这里是置之为<code>$20$</code>），直接不变输出</p><h3 id=eps-急救包>eps 急救包<a hidden class=anchor aria-hidden=true href=#eps-急救包>#</a></h3><p>当分母可能出现为<code>$0$</code>时，给它加上一个较小的正数<code>$\varepsilon$</code></p><h4 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h4><p><code>$$ \text{LayerNorm}(\boldsymbol{x}) = \gamma \left(\frac{\boldsymbol{x} - \bar{\boldsymbol{x}}}{\sigma + {\color{blue}\varepsilon}} \right) + \beta $$</code>
分母加上一个<code>$\varepsilon$</code>来防止变为<code>$0$</code>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>init</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>std</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span>
</span></span></code></pre></div></div><blockquote class=quote-copyright>Author: Yunpengtai<p>Link: https://yunpengtai.top/posts/numerical-stability/<p>License: CC BY-NC-SA 4.0. You must provide a link to the source.</blockquote><footer class=post-footer><ul class=post-tags><li><a href=https://yunpengtai.top/tags/algorithm-analysis/>Algorithm Analysis</a></li></ul><nav class=paginav><a class=prev href=https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/><span class=title>« Prev</span><br><span>NCE的朋友们</span></a>
<a class=next href=https://yunpengtai.top/posts/bias-variance-decomposition/><span class=title>Next »</span><br><span>Bias Variance Decomposition</span></a></nav></footer><!doctype html><head><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://unpkg.com/katex@v0.16/dist/katex.min.css></head><body><div id=waline></div><script type=module>
    import { init } from 'https://unpkg.com/@waline/client@v3/dist/waline.js';
    import katex from 'https://unpkg.com/katex@0.16/dist/katex.mjs';

    init({
      el: '#waline',
      lang: 'en-US',
      reaction: true,
      avatar: "mp",
      serverURL: 'https:\/\/example.yunpengtai.top\/',
      AUTHOR_EMAIL: '2111783652@qq.com',
      texRenderer: (blockMode, tex) =>
          katex.renderToString(tex, {
            displayMode: blockMode,
            throwOnError: false,
            output: 'html'
      }),
      emoji: [
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs',
        'https://unpkg.com/@waline/emojis@1.2.0/tw-emoji',
        'https://unpkg.com/@waline/emojis@1.2.0/bilibili'
    ],
      reaction:[
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheart.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatrainbow.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatattentionreverse.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheartbroken.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatopenmouth.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatcoffee.png'
      ],
      locale: {
        placeholder: '可以匿名评论哦~ QQ邮箱可自动获取头像 Anything to say? It can be anonymous.',
        level0: '潜水',
        level1: '冒泡',
        level2: '摸鱼',
        level3: '话痨',
        level4: '话满天',
        level5: '龙王',
      }
  });
  </script></body></html></article></main><footer class=footer><span>&copy; 2024 <a href=https://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("//cdn.bootcss.com/pangu/4.0.7/pangu.min.js",function(){pangu.spacingPage()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>