<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Retrieval-Enhanced Transformer | Tai's Blog</title><meta name=keywords content="retrieval,transformer,PLM"><meta name=description content="Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training & Evaluation set:
\(\text{MassiveText}\) for both training & retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \(128K\) tokens During training, we retrieving \(600B\) tokens from the training The evaluation contains \(1.75T\) tokens Test set leakage:
Due to the huge retrieving database, the test set may have appeared in the training set."><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=https://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{tags:"all",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Retrieval-Enhanced Transformer"><meta property="og:description" content="Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training & Evaluation set:
\(\text{MassiveText}\) for both training & retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \(128K\) tokens During training, we retrieving \(600B\) tokens from the training The evaluation contains \(1.75T\) tokens Test set leakage:
Due to the huge retrieving database, the test set may have appeared in the training set."><meta property="og:type" content="article"><meta property="og:url" content="https://yunpengtai.top/posts/retrieval-enhanced-transformer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-19T21:06:00+08:00"><meta property="article:modified_time" content="2022-06-19T21:06:00+08:00"><meta property="og:site_name" content="ExampleSite"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Retrieval-Enhanced Transformer","item":"https://yunpengtai.top/posts/retrieval-enhanced-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retrieval-Enhanced Transformer","name":"Retrieval-Enhanced Transformer","description":"Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training \u0026amp; Evaluation set:\n\\(\\text{MassiveText}\\) for both training \u0026amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \\(128K\\) tokens During training, we retrieving \\(600B\\) tokens from the training The evaluation contains \\(1.75T\\) tokens Test set leakage:\nDue to the huge retrieving database, the test set may have appeared in the training set.","keywords":["retrieval","transformer","PLM"],"articleBody":"Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training \u0026 Evaluation set:\n\\(\\text{MassiveText}\\) for both training \u0026 retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \\(128K\\) tokens During training, we retrieving \\(600B\\) tokens from the training The evaluation contains \\(1.75T\\) tokens Test set leakage:\nDue to the huge retrieving database, the test set may have appeared in the training set. Thus, the authors apply 13-gram Jaccard Similarity between the training and test documents to filter those training documents similar to the test documents (i.e., the similarity is \\(\\geq \\textbf{0.80}\\))\nRetrieval Modeling Key-Value Format of the Database:\n\\(\\text{Key} \\Rightarrow\\) frozen BERT Embedding \\(\\text{Value} \\Rightarrow\\) raw chunks of the tokens using the SCaNN library\nthe similarity depends on the \\(\\text{L2 Distance}\\):\n$$ ||x-y||_2 = \\sqrt{\\sum_i (x_i - y_i)^2} $$\npre-compute the frozen BERT Embedding to save the computation and the Embedding is averaged with time.\nretrieving targets are the corresponding chunks and their continuation in the orig document\nThe architecture Assume the input sequence \\(\\text{X}\\) contains \\(9\\) tokens, it can be split into \\(3\\) chunks (i.e., \\(C_1, C_2, C_3\\)) whose sizes are \\(3\\) respectively. Then the chunks are embedded through the frozen BERT embedding. We can retrieve neighbours of those input chunks. We also embed the input sequence and then apply self-attention mechanism on them to get the hidden states \\(H(X)\\) Furthermore, we need to encode the neighbours. Here, the transformer encoder is bi-directional. And it outputs the representations of the neighbours by conditioning on the hidden states of the input chunks. After we get the representations of the neighbours, we let them attend the input chunks as the \\(\\text{K and V}\\) while the input chunk is \\(\\text{Q}\\). The attending network is called CCA(\\(\\textbf{C}\\)hunked \\(\\textbf{C}\\)ross \\(\\textbf{A}\\)ttention). I introduce it in the following part. When the neighbours finish attending the input chunks, the input chunks can be represented by the retrieved neighbours. The representations are going through the FFW(\\(\\textbf{F}\\)eed \\(\\textbf{F}\\)or\\(\\textbf{W}\\)ard). Thus, a Retro-Block contains self-attention mechanism, CCA and FFW. Take the green chunk as the example, we retrieve its neighbours from the database and we let them attend with the concatenation between the green chunk and its next chunk. To put it more precisely, assume we retrieve the neighbours \\(E(mi)\\) for the chunk \\(m_i\\) which contains \\(n\\) tokens: \\({m{i1}, m*{i2}, \\dots, m*{in}}\\), we concatenate the last token of \\(mi\\) with the next chunk \\(m_j\\) except the last token \\(\\Rightarrow \\text{Concatenate}(m{in}, m_{j1, \\dots, jn-1})\\). After the concatenation, we apply CA(\\(\\textbf{C}\\)ross \\(\\textbf{A}\\)ttention). CA is the common attention mechanism. Finally, we concatenate the outputs and pad them. Note, the relative positional encoding is applied.\nExperiment Scaling the Retro The scale of the Retro and the retrieved tokens are proportional to the performance. The number of neighbours has an upped bound: somewhere near \\(40\\). Maybe too many neighbours reduce the retrieval quality. Improvement Comparison Among some tasks, Retro can outperform the models whose parameters are much more than the Retro’s.\nPerplexity on Wikitext103 Retro’s perplexity can be SOTA on the Wikitext103 Interestingly, the external memory can also have the phenomenon of the underfitting. When using MassiveText(1%), it can underfit the training set. And its performance is worse than the kNN-LM. Retro Finetuning Training from scratch is the most powerful way.\nQuestion Answering Results FID + Distill is the SOTA in the Open-Domain Question Answering when the retrieval involves in the training.\nAblation Studies The continuation of the retrieved chunks do help. CA positions are every 3 from 1 or mid layer. Why work? To summarize, the Retro incorporates the external neighbours of the input sequence into the Large Language Modelling to scale down the model size while maintaining the performance.\nLessons \u0026 Imaginations Performance can get improved either by improving the model size or training more data. Huge amount of data don’t need too big model to fit in. We can scale down the PLM by attending the external information. CCA is applied because the external knowledge need to be merged. When applying in MRC, the external information can be: the chunked passages the broken passages the past similar to question-passage pairs the knowledge among the input the evidence The BM25, Edit Distance and LDA can also perform not bad in the retieval. ","wordCount":"718","inLanguage":"en","datePublished":"2022-06-19T21:06:00+08:00","dateModified":"2022-06-19T21:06:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunpengtai.top/posts/retrieval-enhanced-transformer/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"https://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://yunpengtai.top/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yunpengtai.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yunpengtai.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yunpengtai.top/friends/ title=Friends><span>Friends</span></a></li><li><a href=https://yunpengtai.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=https://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Retrieval-Enhanced Transformer</h1><div class=post-meta><span title='2022-06-19 21:06:00 +0800 CST'>June 19, 2022</span>&nbsp;·&nbsp;718 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problems-to-solve aria-label="Problems To Solve">Problems To Solve</a></li><li><a href=#how aria-label=How?>How?</a><ul><li><a href=#data-construction aria-label="Data Construction">Data Construction</a></li><li><a href=#retrieval-modeling aria-label="Retrieval Modeling">Retrieval Modeling</a></li><li><a href=#the-architecture aria-label="The architecture">The architecture</a></li></ul></li><li><a href=#experiment aria-label=Experiment>Experiment</a><ul><li><a href=#scaling-the-retro aria-label="Scaling the Retro">Scaling the Retro</a></li><li><a href=#improvement-comparison aria-label="Improvement Comparison">Improvement Comparison</a></li><li><a href=#perplexity-on-wikitext103 aria-label="Perplexity on Wikitext103">Perplexity on Wikitext103</a></li><li><a href=#retro-finetuning aria-label="Retro Finetuning">Retro Finetuning</a></li><li><a href=#question-answering-results aria-label="Question Answering Results">Question Answering Results</a></li><li><a href=#ablation-studies aria-label="Ablation Studies">Ablation Studies</a></li></ul></li><li><a href=#why-work aria-label="Why work?">Why work?</a></li><li><a href=#lessons--imaginations aria-label="Lessons &amp;amp; Imaginations">Lessons & Imaginations</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=problems-to-solve>Problems To Solve<a hidden class=anchor aria-hidden=true href=#problems-to-solve>#</a></h2><ol><li>To <b>Scale Down</b> the model size while maintaining the performances.</li><li>To incorporate External Memory Retrieval in the Large Language Model Modeling.</li></ol><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/ecSmGwTuBbzYnDX.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/ecSmGwTuBbzYnDX.png#center width=600 height=300></figure></a><h2 id=how>How?<a hidden class=anchor aria-hidden=true href=#how>#</a></h2><h3 id=data-construction>Data Construction<a hidden class=anchor aria-hidden=true href=#data-construction>#</a></h3><p><strong>Training & Evaluation set:</strong></p><ol><li>\(\text{MassiveText}\) for both training & retrieval data (contains 5 trillion tokens)
<script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/JUpDF8y9LCqnRNW.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/JUpDF8y9LCqnRNW.png#center width=680 height=150></figure></a></li><li>SentencePiece with a vocabulary of \(128K\) tokens</li><li>During training, we retrieving \(600B\) tokens from the training</li><li>The evaluation contains \(1.75T\) tokens</li></ol><p><strong>Test set leakage:</strong></p><p>Due to the huge retrieving database, the test set may have appeared in the training set. Thus, the authors apply 13-gram Jaccard Similarity between the training and test documents to filter those training documents similar to the test documents (i.e., the similarity is \(\geq \textbf{0.80}\))</p><h3 id=retrieval-modeling>Retrieval Modeling<a hidden class=anchor aria-hidden=true href=#retrieval-modeling>#</a></h3><ol><li><p>Key-Value Format of the Database:</p><ul><li>\(\text{Key} \Rightarrow\) frozen BERT Embedding</li><li>\(\text{Value} \Rightarrow\) raw chunks of the tokens</li></ul></li><li><p>using the SCaNN library</p></li><li><p>the similarity depends on the \(\text{L2 Distance}\):</p><p>$$
||x-y||_2 = \sqrt{\sum_i (x_i - y_i)^2}
$$</p></li><li><p><strong>pre-compute</strong> the frozen BERT Embedding to save the computation and the Embedding is <strong>averaged with time.</strong></p></li><li><p>retrieving targets are the corresponding chunks and their continuation in the orig document</p></li></ol><h3 id=the-architecture>The architecture<a hidden class=anchor aria-hidden=true href=#the-architecture>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/SMKJbATvzyqRE3c.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/SMKJbATvzyqRE3c.png#center width=520 height=450></figure></a><ol><li>Assume the input sequence \(\text{X}\) contains \(9\) tokens, it can be split into \(3\) chunks (i.e., \(C_1, C_2, C_3\)) whose sizes are \(3\) respectively.</li><li>Then the chunks are embedded through the frozen BERT embedding. We can retrieve neighbours of those input chunks.</li><li>We also embed the input sequence and then apply self-attention mechanism on them to get the hidden states \(H(X)\)</li><li>Furthermore, we need to encode the neighbours. Here, the transformer encoder is bi-directional. And it outputs the representations of the neighbours by conditioning on the hidden states of the input chunks.</li><li>After we get the representations of the neighbours, we let them attend the input chunks as the \(\text{K and V}\) while the input chunk is \(\text{Q}\). The attending network is called CCA(\(\textbf{C}\)hunked \(\textbf{C}\)ross \(\textbf{A}\)ttention). I introduce it in the following part.</li><li>When the neighbours finish attending the input chunks, the input chunks can be represented by the retrieved neighbours. The representations are going through the FFW(\(\textbf{F}\)eed \(\textbf{F}\)or\(\textbf{W}\)ard). Thus, a Retro-Block contains self-attention mechanism, CCA and FFW.</li></ol><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/EJ4GsShCHox2dmn.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/EJ4GsShCHox2dmn.png#center width=520 height=450></figure></a><ol start=7><li>Take the green chunk as the example, we retrieve its neighbours from the database and we let them attend with the concatenation between the green chunk and its next chunk. To put it more precisely, assume we retrieve the neighbours \(E(m<em>i)\) for the chunk \(m_i\) which contains \(n\) tokens: \({m</em>{i1}, m*{i2}, \dots, m*{in}}\), we concatenate the last token of \(m<em>i\) with the next chunk \(m_j\) except the last token \(\Rightarrow \text{Concatenate}(m</em>{in}, m_{j1, \dots, jn-1})\).</li><li>After the concatenation, we apply CA(\(\textbf{C}\)ross \(\textbf{A}\)ttention). CA is the common attention mechanism.</li><li>Finally, we concatenate the outputs and pad them.</li></ol><div class="notice notice-info"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512" fill="hsl(30, 80%, 70%)"><path d="M256 8a248 248 0 100 496 248 248 0 000-496zm0 110a42 42 0 110 84 42 42 0 010-84zm56 254c0 7-5 12-12 12h-88c-7 0-12-5-12-12v-24c0-7 5-12 12-12h12v-64h-12c-7 0-12-5-12-12v-24c0-7 5-12 12-12h64c7 0 12 5 12 12v1e2h12c7 0 12 5 12 12v24z"/></svg></div><p>Note, the relative positional encoding is applied.</p></div><h2 id=experiment>Experiment<a hidden class=anchor aria-hidden=true href=#experiment>#</a></h2><h3 id=scaling-the-retro>Scaling the Retro<a hidden class=anchor aria-hidden=true href=#scaling-the-retro>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/2Pl7NrxWuyzkEwO.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/2Pl7NrxWuyzkEwO.png#center width=720 height=200></figure></a><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/DzKGVWuH2r4Tvdj.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/DzKGVWuH2r4Tvdj.png#center width=720 height=200></figure></a><ol><li>The scale of the Retro and the retrieved tokens are proportional to the performance.</li><li>The number of neighbours has an upped bound: somewhere near \(40\). Maybe too many neighbours reduce the retrieval quality.</li></ol><h3 id=improvement-comparison>Improvement Comparison<a hidden class=anchor aria-hidden=true href=#improvement-comparison>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/yeOLafErnChsq3K.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/yeOLafErnChsq3K.png#center width=680 height=300></figure></a><p>Among some tasks, Retro can outperform the models whose parameters are much more than the Retro&rsquo;s.</p><h3 id=perplexity-on-wikitext103>Perplexity on Wikitext103<a hidden class=anchor aria-hidden=true href=#perplexity-on-wikitext103>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/Fueq18xZWtO5CwV.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/Fueq18xZWtO5CwV.png#center width=680 height=200></figure></a><ol><li>Retro&rsquo;s perplexity can be SOTA on the Wikitext103</li><li>Interestingly, the external memory can also have the phenomenon of the underfitting. When using MassiveText(1%), it can underfit the training set. And its performance is worse than the kNN-LM.</li></ol><h3 id=retro-finetuning>Retro Finetuning<a hidden class=anchor aria-hidden=true href=#retro-finetuning>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/oBCmrJ5XPqZuezL.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/oBCmrJ5XPqZuezL.png#center width=680 height=200></figure></a><p>Training from scratch is the most powerful way.</p><h3 id=question-answering-results>Question Answering Results<a hidden class=anchor aria-hidden=true href=#question-answering-results>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/9NX3QnemqWwY7oC.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/9NX3QnemqWwY7oC.png#center width=380 height=200></figure></a><p>FID + Distill is the SOTA in the Open-Domain Question Answering when the retrieval involves in the training.</p><h3 id=ablation-studies>Ablation Studies<a hidden class=anchor aria-hidden=true href=#ablation-studies>#</a></h3><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2022/06/19/WaxCPTmzJSw8dDi.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2022/06/19/WaxCPTmzJSw8dDi.png#center width=380 height=300></figure></a><ol><li>The continuation of the retrieved chunks do help.</li><li>CA positions are every 3 from 1 or mid layer.</li></ol><h2 id=why-work>Why work?<a hidden class=anchor aria-hidden=true href=#why-work>#</a></h2><p>To summarize, the Retro incorporates the external neighbours of the input sequence into the Large Language Modelling to scale down the model size while maintaining the performance.</p><h2 id=lessons--imaginations>Lessons & Imaginations<a hidden class=anchor aria-hidden=true href=#lessons--imaginations>#</a></h2><ol><li>Performance can get improved either by improving the model size or training more data.</li><li>Huge amount of data don&rsquo;t need too big model to fit in.</li><li>We can scale down the PLM by attending the external information.</li><li>CCA is applied because the external knowledge need to be merged. When applying in MRC, the external information can be:<ol><li>the chunked passages</li><li>the broken passages</li><li>the past similar to question-passage pairs</li><li>the knowledge among the input</li><li>the evidence</li></ol></li><li>The BM25, Edit Distance and LDA can also perform not bad in the retieval.</li></ol></div><blockquote class=quote-copyright>Author: Yunpengtai<p>Link: https://yunpengtai.top/posts/retrieval-enhanced-transformer/<p>License: CC BY-NC-SA 4.0. You must provide a link to the source.</blockquote><footer class=post-footer><ul class=post-tags><li><a href=https://yunpengtai.top/tags/retrieval/>retrieval</a></li><li><a href=https://yunpengtai.top/tags/transformer/>transformer</a></li><li><a href=https://yunpengtai.top/tags/plm/>PLM</a></li></ul><nav class=paginav><a class=prev href=https://yunpengtai.top/posts/quotes-of-mathematicians/><span class=title>« Prev</span><br><span>Quotes of Mathematicians</span></a></nav></footer><!doctype html><head><link rel=stylesheet href=https://unpkg.com/@waline/client@v2/dist/waline.css></head><body><div id=waline></div><script type=module>
    import { init } from 'https://unpkg.com/@waline/client@v2/dist/waline.mjs';

    init({
      el: '#waline',
      lang: 'en',
      reaction: true, 
      theme: 'dark',
      avatar: "mp",
      serverURL: 'https:\/\/example.yunpengtai.top\/',
      emoji: [
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs',
        'https://unpkg.com/@waline/emojis@1.1.0/tw-emoji',
        'https://unpkg.com/@waline/emojis@1.1.0/bmoji'
    ],
      reaction:[
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheart.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatrainbow.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatattentionreverse.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheartbroken.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatopenmouth.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatcoffee.png'
      ],
      locale: {
        placeholder: '可以匿名评论哦~ QQ邮箱可自动获取头像 Anything to say? It can be anonymous.',
        level0: '潜水',
        level1: '冒泡',
        level2: '摸鱼',
        level3: '话痨',
        level4: '话满天',
        level5: '龙王',
      }
  });
  </script></body></html></article></main><footer class=footer><span>&copy; 2024 <a href=https://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("//cdn.bootcss.com/pangu/4.0.7/pangu.min.js",function(){pangu.spacingPage()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>