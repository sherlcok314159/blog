<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generalized Linear Models | Tai's Blog</title><meta name=keywords content="Linear Models,Exponential Family"><meta name=description content="定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) $$ 其中$\eta$被称为分布的自然参数（nat"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=https://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{tags:"all",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Generalized Linear Models"><meta property="og:description" content="定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) $$ 其中$\eta$被称为分布的自然参数（nat"><meta property="og:type" content="article"><meta property="og:url" content="https://yunpengtai.top/posts/generalized-linear-models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-17T10:52:00+08:00"><meta property="article:modified_time" content="2023-02-17T10:52:00+08:00"><meta property="og:site_name" content="Tai's Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Generalized Linear Models","item":"https://yunpengtai.top/posts/generalized-linear-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generalized Linear Models","name":"Generalized Linear Models","description":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) $$ 其中$\\eta$被称为分布的自然参数（nat","keywords":["Linear Models","Exponential Family"],"articleBody":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员\n$$ p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) $$ 其中$\\eta$被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而$T(y)$被称为统计充分量（sufficient statistic），通常而言：$T(y) = y$；$a(\\eta)$是对数配分函数（log partition）\n例子 接下来展示下伯努利分布和高斯分布都是指数族的一员\n期望为$\\phi$的伯努利分布且$y \\in \\{1, 0 \\}$，那么：\n$$ p(y=1) = \\phi; p(y=0) = 1-\\phi $$\n我们将上式压缩一下：\n$$ \\begin{align} p(y;\\phi ) \u0026 = \\phi^{y}(1-\\phi )^{1-y} \\\\ \u0026= \\exp \\bigg(\\log(\\phi^{y}(1-\\phi )^{1-y})\\bigg) \\\\ \u0026= \\exp(y\\log \\phi + (1-y)\\log(1-\\phi )) \\\\ \u0026= \\exp\\left( \\left( \\log \\frac{\\phi }{1-\\phi } \\right)y + \\log(1-\\phi) \\right) \\end{align} $$\n那么可以对比着指数族的定义，易得$b(y)=1$以及 取倒数可以很方便简化分式 ：\n$$ \\begin{align} \\eta \u0026 = \\log\\left( \\frac{\\phi }{1-\\phi } \\right) \\\\ e^{\\eta } \u0026 = \\frac{\\phi}{1-\\phi } \\\\ {\\color{red}e^{-\\eta }} \u0026 = \\frac{1-\\phi}{\\phi } = \\frac{1}{\\phi } - 1 \\\\ \\phi \u0026 = \\frac{1}{1+e^{-\\eta }} \\end{align} $$\n发现很有趣的一点，$\\phi(\\eta)$不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：\n$$ \\begin{align} a(\\eta ) \u0026 = -\\log(1-\\phi ) = -\\log\\left( 1-\\frac{1}{1+e^{-\\eta }} \\right) \\\\ \u0026= \\log (1+e^{\\eta }) \\end{align} $$\n接下来讨论高斯分布，因为$\\sigma^{2}$对$\\theta, h_{\\theta }(x)$是不影响的（相当于常数） 通过最大似然，发现$\\sigma$对目标函数的形式无影响 ，这里为了简化表示，约定$\\sigma^{2}=1$\n$$ \\begin{align} p(y;\\mu ) \u0026 = \\frac{1}{\\sqrt{ 2\\pi }}\\exp\\left( -\\frac{(y-\\mu )^{2}}{2} \\right) \\\\ \u0026= \\underbrace{ \\frac{1}{\\sqrt{ 2\\pi }} \\exp\\left( -\\frac{y^{2}}{2} \\right) }_{ b(y) }\\exp\\left( \\mu y - \\frac{\\mu^{2}}{2} \\right) \\end{align} $$\n比对定义，可以发现：\n$$ \\eta = \\mu; a(\\eta ) = -\\frac{\\eta^{2}}{2} $$\n性质 $$ p(y;\\eta ) = b(y)\\exp (\\eta^{\\mathbf{T}}T(y) - a(\\eta)) $$ 性质1：指数族分布的期望是$a(\\eta)$对$\\eta$的一阶微分\n下面来证明上述观点：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) \u0026 = b(y) \\exp(\\eta^{\\mathbf{T}}y - a(\\eta )) \\left( y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\right) \\\\ \u0026= yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\end{align} $$\n那么：\n$$ y p(y;\\eta ) = \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) $$ 又因为：\n$$ \\begin{align} \\mathbb{E}[Y;\\eta ] \u0026 = \\mathbb{E}[Y|X;\\eta] \\\\ \u0026= \\int yp(y;\\eta ) \\,dy \\\\ \u0026= \\int \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026= \\int \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\, dy + \\int p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\int p(y;\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } \\cdot 1 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\cdot 1 \\\\ \u0026= 0 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } a(\\eta) \\quad \\blacksquare \\end{align} $$\n性质2：指数族分布的方差是$a(\\eta)$对$\\eta$的二阶微分\n下面来证明方差：\n$$ \\begin{align} \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) \u0026 = \\frac{ \\partial }{ \\partial \\eta } \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg) \\\\ \u0026= y\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg)\\bigg(y- \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= y^{2}p(y;\\eta ) - 2yp(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) + p(y;\\eta ) (\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) )^{2} - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) -p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\end{align} $$\n那么： $$ \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) +p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) $$\n又因为：\n$$ \\begin{align} \\mathbb{V}[Y;\\eta ] \u0026 = \\mathbb{V}[Y|X;\\eta] \\\\ \u0026= \\int \\left( y - \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\right)^{2} p(y;\\eta)\\, dy \\\\ \u0026= \\int \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }p(y;\\eta) + p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }a(\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta )\\int p(y;\\eta ) \\, dy \\\\ \u0026= 0 + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\quad \\blacksquare \\end{align} $$ 性质3：指数族分布的NLL loss是concave的\n接下来证明NLL Loss是凸函数，其中$a(\\eta) \\in \\mathbb{R}^{m}, \\mathbf{y} \\in \\mathbb{R}^{m}$：\n$$ \\begin{align} J(\\eta) \u0026 = -\\log \\sum_{i}^{m} p(y^{(i)};\\eta_{i} ) \\\\ \u0026= -\\log \\sum_{i}^{m} b(y^{(i)})\\exp(\\eta_{i} T(y^{(i)}) - a(\\eta_{i})) \\\\ \u0026= -\\log \\sum_{i}^{m} b(y^{(i)}) \\exp( \\eta_{i} y^{(i)}- a(\\eta_{i})) \\\\ \u0026= a(\\eta) -\\sum_{i}^{m} \\log b(y^{(i)}) + \\eta_{i}y^{(i)} \\end{align} $$\n同时因为自身的协方差矩阵是「半正定」的：\n$$ \\begin{align} \\nabla_{\\eta }^{2} J(\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) = \\mathbb{V}[\\mathbf{y};\\eta ] \\implies PSD\\quad \\blacksquare \\end{align} $$ 当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点\n构建GLM 在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：\n$y|x; \\theta \\sim \\mathrm{ExponentialFamily}(\\eta )$，也就是说，给定$x, \\theta$ 我们可以得到$y$的分布就是带有参数$\\eta$的指数族分布 给定$x$，我们需要去预测$y$，在指数族中，$y=T(y)$，也就是说我们得去预测期望，即学得的假设$h$需要去预测期望，即：$h(x) = \\mathbb{E}[y|x]$，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子： $$ h_{\\theta }(x) = p(y=1|x;\\theta ) = 0 \\cdot p(y=0|x;\\theta) + 1 \\cdot p(y=1|x;\\theta) = \\mathbb{E}[y|x;\\theta ] $$ 自然参数$\\eta$与$x$的联系是线性的，即$\\eta = \\theta^{\\mathbf{T}}x$，若$\\eta \\in \\mathbb{R}^{n}$，则$\\eta_{i} = \\theta_{i}^{\\mathbf{T}}{x}$ GLM例子 OLS Ordinary Least Squares（线性回归）中的$y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，即服从高斯分布，其中$\\eta = \\mu$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[y|x;\\theta] \\\\ \u0026 = \\mu \\\\ \u0026= \\eta \\\\ \u0026= \\theta^{\\mathbf{T}}{x} \\end{align} $$ 第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设\n逻辑回归 逻辑回归中$y \\in {0, 1}$，自然就想到了伯努利分布，即$y|x;\\theta \\sim \\text{Bernoulli}(\\phi )$，其中$\\phi=1/1+ e^{-\\eta }$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[y|x;\\theta ] \\\\ \u0026 = \\phi \\\\ \u0026= \\frac{1}{1 + e^{-\\eta }} \\\\ \u0026= \\frac{1}{1 + e^{-\\theta^{\\mathbf{T}}x}} \\end{align} $$ 是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来\n$g(\\eta) = \\mathbb{E}[T(y);\\eta ]$被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而$g^{-1}$被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）\nsoftmax回归 构建GLM 之前逻辑回归中是只有两类，当$y \\in \\{1, 2, \\dots, k\\}$，即现在是$k$分类，分布是multinomial distribution，接下来让我们构建GLM：\n规定$\\phi_{i}$规定了输出$y_{i}$的概率，那么$\\phi_{i}, \\dots, \\phi_{k-1}$即是我们的参数，那你肯定好奇为什么$\\phi_{k}$不是，因为输出所有类的概率之和为$1$，即$\\phi_{k}$可被其他的概率表示：\n$$ \\phi_{k} = 1-\\sum_{i}^{k-1} \\phi_{i} $$\n以往的$T(y)=y$，对于多分类而言，我们采用独热编码（one-hot），即：\n$$ T(1) = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2) = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\dots ,T(k-1) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, T(k) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$\n注意$T(y) \\in \\mathbb{R}^{k-1}$，因为$T(k)$定义为全零向量，那么如何表示$T(y)$的第$i$个元素呢？ 指示函数当满足条件时为$1$，否则为$0$ $$ (T(y))_{i} = \\mathbb{1}\\{y=i\\} $$\n接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当$y$取一个具体值时，只有一个指示函数为$1$，其他为$0$，即$\\phi_{i}^{0} = 1$\n$$ \\begin{align} p(y;\\phi ) \u0026 = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{\\mathbb{1}\\{y=k\\}}_{k} \\\\ \u0026 = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{1-\\sum_{i}^{k-1}\\mathbb{1}\\{y=i\\}}_{k} \\\\ \u0026= \\phi_{1}^{(T(y))_{1}} \\phi_{2}^{(T(y)_{2})} \\dots \\phi_{k}^{1-\\sum_{i}^{k-1}(T(y))_{i}} \\\\ \\end{align} $$\n继续变形来跟定义做比较：\n$$ \\begin{align} p(y; \\phi)\u0026= \\exp \\bigg((T(y))_{1}\\log \\phi_{1} + (T(y))_{2} \\log \\phi_{2} + \\dots +(1-\\sum_{i}^{k-1}(T(y))_{i})\\log \\phi_{k} \\bigg) \\\\ \u0026= \\exp \\bigg((T(y))_{1}\\log \\frac{\\phi_{1}}{\\phi_{k}} + (T(y))_{2}\\log \\frac{\\phi_{2}}{\\phi_{k}} + \\dots+ (T(y))_{k-1}\\log \\frac{\\phi_{k-1}}{\\phi_{k}} + \\log \\phi_{k}\\bigg) \\end{align} $$\n那么：\n$$ \\eta = \\begin{bmatrix} \\log (\\phi_{1} / \\phi_{k}) \\\\ \\log (\\phi_{2} / \\phi_{k}) \\\\ \\vdots \\\\ \\log(\\phi_{k-1} / \\phi_{k}) \\end{bmatrix}, a(\\eta ) = -\\log(\\phi_{k}), b(y) =1 $$\n链接函数容易发现是：\n$$ \\eta_{i} = \\log \\frac{\\phi_{i}}{\\phi_{k}} $$\n接下来求响应函数：\n$$ \\begin{align} e^{\\eta_{i}} \u0026 = \\frac{\\phi_{i}}{\\phi_{k}} \\\\ \\phi_{k}e^{\\eta_{i}} \u0026 = \\phi_{i} \\\\ \\phi_{k}\\sum_{i}^{k} e^{\\eta_{i}} \u0026 = \\sum_{i}^{k} \\phi_{i} = 1 \\end{align} $$\n那么：\n$$ \\phi_{k} = \\frac{1}{\\sum_{i}^{k} e^{\\eta_{i}}} $$\n将$\\phi_{k}$代入上式：\n$$ \\phi_{i} = \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} $$\n这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：\n$$ \\begin{align} p(y=i|x;\\theta ) \u0026 = \\phi_{i} \\\\ \u0026= \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} \\\\ \u0026= \\frac{e^{\\theta_{i}^{\\mathbf{T}}x}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x}} \\end{align} $$\n多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：\n$$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[T(y)|x;\\theta ] \\\\ \u0026= \\mathbb{E}\\left[\\begin{array}{c|} \\mathbb{1}\\{y=1\\} \\\\ \\mathbb{1}\\{y=2\\} \\\\ \\vdots \\\\ \\mathbb{1}\\{y=k-1\\} \\end{array} x;\\theta \\right] \\end{align} $$\n又因为：\n$$ \\mathbb{E}[(T(y))_{i}] = \\phi_{i} $$\n为啥会这样呢？因为对于$(T(y))_{i}$只有两个可能，$1$或$0$，那么它的期望是不是：\n$$ \\mathbb{E}[(T(y))_{i}] = 1 \\cdot \\phi_{i} + 0 \\cdot (1-\\phi ) = \\phi_{i} $$\n$$ h_{\\theta }(x)= \\begin{bmatrix} \\phi_{1} \\\\ \\phi_{2} \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix}= \\begin{bmatrix} \\frac{\\exp ({\\theta_{1}^{\\mathbf{T}}x)}}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\frac{\\exp ({\\theta_{2}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\vdots \\\\ \\frac{\\exp ({\\theta_{k-1}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\end{bmatrix} $$\n也就是说我们的假设函数需要输出每个类的概率，尽管只有$k-1$类，$\\phi_{k} = 1- \\sum_{i}^{k-1} \\phi_{i}$得到\n接下来进行最大似然估计并对$\\ell(\\theta)$进行化简：\n$$ \\begin{align} \\ell(\\theta ) \u0026 = \\sum_{i} \\log p(y^{(i)}|x^{(i)};\\theta ) \\\\ \u0026= \\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026= \\sum_{i}^{m} \\sum_{l=1}^{k} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026= \\sum_{i}^{m} \\sum_{l=1}^{k} {\\color{red}\\mathbb{1}\\{y^{(i)}=l\\}} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left( \\log e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}} \\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left(\\theta_{l}^{\\mathbf{T}}x^{(i)}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)}-\\left( \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\underbrace{ \\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} }_{ 1 }\\right) \\\\ \u0026= \\sum_{i}^{m} \\bigg(\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)} - \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\bigg) \\end{align} $$\n上述化简主要利用了指示函数的性质以及$\\log$的运算法则，同时$\\theta \\in \\mathbb{R}^{k \\times n}$，我们利用布局法来求：\n$$ \\begin{align} \\frac{ \\partial \\ell(\\theta ) }{ \\partial \\theta_{pq} } \u0026 = \\sum_{i}^{m} \\mathbb{1}\\{y^{(i)} = p\\} x^{(i)}_{q} - \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\\\ \u0026= \\sum_{i}^{m} \\left( \\mathbb{1}\\{y^{(i)} = p\\} - \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)x^{(i)}_{q} \\end{align} $$\n因为这里是最大化$\\ell(\\theta)$，作为损失函数还应加个负号，这样才是最小，即\n$$ J(\\theta ) = -\\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} $$\n对应的微分如下：\n$$ \\frac{ \\partial J(\\theta ) }{ \\partial \\theta_{pq} } = \\sum_{i}^{m} \\left( \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} - \\mathbb{1}\\{y^{(i)} = p\\} \\right)x^{(i)}_{q} $$\n交叉熵 我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？\n联想交叉熵的定义：\n$$ H(P, Q) = - \\mathbb{E}_{x \\sim P}[\\log Q(x)] $$\n即使得模型输出的分布尽可能靠近训练集原来的分布：\n$$ \\theta^{\\ast} = \\mathop{\\arg \\min}_{\\theta} -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] $$\n我们接下来展开期望的计算：\n$$ -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] = \\frac{1}{m}\\underbrace{ -\\sum_{i}^{m} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} }_{ J(\\theta) } $$\n两者其实就差一个常数，本质是一样的\n","wordCount":"3632","inLanguage":"en","datePublished":"2023-02-17T10:52:00+08:00","dateModified":"2023-02-17T10:52:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunpengtai.top/posts/generalized-linear-models/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"https://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://yunpengtai.top/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yunpengtai.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yunpengtai.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yunpengtai.top/friends/ title=Friends><span>Friends</span></a></li><li><a href=https://yunpengtai.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=https://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Generalized Linear Models</h1><div class=post-meta><span title='2023-02-17 10:52:00 +0800 CST'>February 17, 2023</span>&nbsp;·&nbsp;3632 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%ae%9a%e4%b9%89 aria-label=定义>定义</a></li><li><a href=#%e4%be%8b%e5%ad%90 aria-label=例子>例子</a></li><li><a href=#%e6%80%a7%e8%b4%a8 aria-label=性质>性质</a></li><li><a href=#%e6%9e%84%e5%bb%baglm aria-label=构建GLM>构建GLM</a></li><li><a href=#glm%e4%be%8b%e5%ad%90 aria-label=GLM例子>GLM例子</a><ul><li><a href=#ols aria-label=OLS>OLS</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92 aria-label=逻辑回归>逻辑回归</a></li></ul></li><li><a href=#softmax%e5%9b%9e%e5%bd%92 aria-label=softmax回归>softmax回归</a><ul><li><a href=#%e6%9e%84%e5%bb%baglm-1 aria-label=构建GLM>构建GLM</a></li><li><a href=#%e4%ba%a4%e5%8f%89%e7%86%b5 aria-label=交叉熵>交叉熵</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=定义>定义<a hidden class=anchor aria-hidden=true href=#定义>#</a></h2><p>若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员</p><p><code>$$ p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) $$</code>
其中<code>$\eta$</code>被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而<code>$T(y)$</code>被称为统计充分量（sufficient statistic），通常而言：<code>$T(y) = y$</code>；<code>$a(\eta)$</code>是对数配分函数（log partition）</p><h2 id=例子>例子<a hidden class=anchor aria-hidden=true href=#例子>#</a></h2><p>接下来展示下伯努利分布和高斯分布都是指数族的一员</p><p>期望为<code>$\phi$</code>的伯努利分布且$y \in \{1, 0 \}$，那么：</p><p><code>$$ p(y=1) = \phi; p(y=0) = 1-\phi $$</code></p><p>我们将上式压缩一下：</p><p><code>$$ \begin{align} p(y;\phi ) & = \phi^{y}(1-\phi )^{1-y} \\ &= \exp \bigg(\log(\phi^{y}(1-\phi )^{1-y})\bigg) \\ &= \exp(y\log \phi + (1-y)\log(1-\phi )) \\ &= \exp\left( \left( \log \frac{\phi }{1-\phi } \right)y + \log(1-\phi) \right) \end{align} $$</code></p><p>那么可以对比着指数族的定义，易得$b(y)=1$以及
<span class=sidenote-number><small class=sidenote>取倒数可以很方便简化分式</small></span>
：</p><p><code>$$ \begin{align} \eta & = \log\left( \frac{\phi }{1-\phi } \right) \\ e^{\eta } & = \frac{\phi}{1-\phi } \\ {\color{red}e^{-\eta }} & = \frac{1-\phi}{\phi } = \frac{1}{\phi } - 1 \\ \phi & = \frac{1}{1+e^{-\eta }} \end{align} $$</code></p><p>发现很有趣的一点，$\phi(\eta)$不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：</p><p><code>$$ \begin{align} a(\eta ) & = -\log(1-\phi ) = -\log\left( 1-\frac{1}{1+e^{-\eta }} \right) \\ &= \log (1+e^{\eta }) \end{align} $$</code></p><p>接下来讨论高斯分布，因为$\sigma^{2}$对$\theta, h_{\theta }(x)$是不影响的（相当于常数）
<span class=sidenote-number><small class=sidenote>通过最大似然，发现$\sigma$对目标函数的形式无影响</small></span>
，这里为了简化表示，约定$\sigma^{2}=1$</p><p><code>$$ \begin{align} p(y;\mu ) & = \frac{1}{\sqrt{ 2\pi }}\exp\left( -\frac{(y-\mu )^{2}}{2} \right) \\ &= \underbrace{ \frac{1}{\sqrt{ 2\pi }} \exp\left( -\frac{y^{2}}{2} \right) }_{ b(y) }\exp\left( \mu y - \frac{\mu^{2}}{2} \right) \end{align} $$</code></p><p>比对定义，可以发现：</p><p><code>$$ \eta = \mu; a(\eta ) = -\frac{\eta^{2}}{2} $$</code></p><h2 id=性质>性质<a hidden class=anchor aria-hidden=true href=#性质>#</a></h2><p><code>$$ p(y;\eta ) = b(y)\exp (\eta^{\mathbf{T}}T(y) - a(\eta)) $$</code><div class="notice notice-tip"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512" fill="hsl(140, 65%, 65%)"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zM227 387l184-184c7-6 7-16 0-22l-22-23c-7-6-17-6-23 0L216 308l-70-70c-6-6-16-6-23 0l-22 23c-7 6-7 16 0 22l104 104c6 7 16 7 22 0z"/></svg></div><p>性质1：指数族分布的期望是$a(\eta)$对$\eta$的一阶微分</p></div></p><p>下面来证明上述观点：</p><p><code>$$ \begin{align} \frac{ \partial }{ \partial \eta } p(y;\eta ) & = b(y) \exp(\eta^{\mathbf{T}}y - a(\eta )) \left( y - \frac{ \partial }{ \partial \eta }a(\eta ) \right) \\ &= yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta ) \end{align} $$</code></p><p>那么：</p><p><code>$$ y p(y;\eta ) = \frac{ \partial }{ \partial \eta } p(y;\eta ) + p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta ) $$</code>
又因为：</p><p><code>$$ \begin{align} \mathbb{E}[Y;\eta ] & = \mathbb{E}[Y|X;\eta] \\ &= \int yp(y;\eta ) \,dy \\ &= \int \frac{ \partial }{ \partial \eta } p(y;\eta ) + p(y;\eta )\frac{ \partial }{ \partial \eta }a(\eta ) \, dy \\ &= \int \frac{ \partial }{ \partial \eta }p(y;\eta ) \, dy + \int p(y;\eta )\frac{ \partial }{ \partial \eta }a(\eta ) \, dy \\ &= \frac{ \partial }{ \partial \eta } \int p(y;\eta ) \, dy + \frac{ \partial }{ \partial \eta }a(\eta ) \int p(y;\eta ) \, dy \\ &= \frac{ \partial }{ \partial \eta } \cdot 1 + \frac{ \partial }{ \partial \eta } a(\eta ) \cdot 1 \\ &= 0 + \frac{ \partial }{ \partial \eta } a(\eta ) \\ &= \frac{ \partial }{ \partial \eta } a(\eta) \quad \blacksquare \end{align} $$</code></p><div class="notice notice-tip"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512" fill="hsl(140, 65%, 65%)"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zM227 387l184-184c7-6 7-16 0-22l-22-23c-7-6-17-6-23 0L216 308l-70-70c-6-6-16-6-23 0l-22 23c-7 6-7 16 0 22l104 104c6 7 16 7 22 0z"/></svg></div><p>性质2：指数族分布的方差是$a(\eta)$对$\eta$的二阶微分</p></div><p>下面来证明方差：</p><p><code>$$ \begin{align} \frac{ \partial^{2} }{ \partial \eta^{2} } p(y;\eta ) & = \frac{ \partial }{ \partial \eta } \bigg(yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta )\bigg) \\ &= y\frac{ \partial }{ \partial \eta }p(y;\eta ) - \frac{ \partial }{ \partial \eta } a(\eta )\frac{ \partial }{ \partial \eta }p(y;\eta ) - p(y;\eta )\frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) \\ &= \frac{ \partial }{ \partial \eta }p(y;\eta ) \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta ) \bigg) - p(y;\eta )\frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \bigg(yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta )\bigg)\bigg(y- \frac{ \partial }{ \partial \eta }a(\eta ) \bigg) - p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= y^{2}p(y;\eta ) - 2yp(y;\eta ) \frac{ \partial }{ \partial \eta }a(\eta ) + p(y;\eta ) (\frac{ \partial }{ \partial \eta }a(\eta ) )^{2} - p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta) \bigg)^{2} p(y;\eta ) -p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) \end{align} $$</code></p><p>那么：
<code>$$ \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta) \bigg)^{2} p(y;\eta ) = \frac{ \partial^{2} }{ \partial \eta^{2} } p(y;\eta ) +p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) $$</code></p><p>又因为：</p><p><code>$$ \begin{align} \mathbb{V}[Y;\eta ] & = \mathbb{V}[Y|X;\eta] \\ &= \int \left( y - \frac{ \partial }{ \partial \eta } a(\eta ) \right)^{2} p(y;\eta)\, dy \\ &= \int \frac{ \partial^{2} }{ \partial \eta^{2} }p(y;\eta) + p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} }a(\eta ) \, dy \\ &= \frac{ \partial^{2} }{ \partial \eta^{2} } \int p(y;\eta ) \, dy + \frac{ \partial ^{2} }{ \partial \eta^{2} } a(\eta )\int p(y;\eta ) \, dy \\ &= 0 + \frac{ \partial ^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \quad \blacksquare \end{align} $$</code><div class="notice notice-tip"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512" fill="hsl(140, 65%, 65%)"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zM227 387l184-184c7-6 7-16 0-22l-22-23c-7-6-17-6-23 0L216 308l-70-70c-6-6-16-6-23 0l-22 23c-7 6-7 16 0 22l104 104c6 7 16 7 22 0z"/></svg></div><p>性质3：指数族分布的NLL loss是concave的</p></div></p><p>接下来证明NLL Loss是凸函数，其中$a(\eta) \in \mathbb{R}^{m}, \mathbf{y} \in \mathbb{R}^{m}$：</p><p><code>$$ \begin{align} J(\eta) & = -\log \sum_{i}^{m} p(y^{(i)};\eta_{i} ) \\ &= -\log \sum_{i}^{m} b(y^{(i)})\exp(\eta_{i} T(y^{(i)}) - a(\eta_{i})) \\ &= -\log \sum_{i}^{m} b(y^{(i)}) \exp( \eta_{i} y^{(i)}- a(\eta_{i})) \\ &= a(\eta) -\sum_{i}^{m} \log b(y^{(i)}) + \eta_{i}y^{(i)} \end{align} $$</code></p><p>同时因为自身的协方差矩阵是「半正定」的：</p><p><code>$$ \begin{align} \nabla_{\eta }^{2} J(\eta ) = \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) = \mathbb{V}[\mathbf{y};\eta ] \implies PSD\quad \blacksquare \end{align} $$</code>
当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点</p><h2 id=构建glm>构建GLM<a hidden class=anchor aria-hidden=true href=#构建glm>#</a></h2><p>在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：</p><ol><li>$y|x; \theta \sim \mathrm{ExponentialFamily}(\eta )$，也就是说，给定$x, \theta$ 我们可以得到$y$的分布就是带有参数$\eta$的指数族分布</li><li>给定$x$，我们需要去预测$y$，在指数族中，$y=T(y)$，也就是说我们得去预测期望，即学得的假设$h$需要去预测期望，即：$h(x) = \mathbb{E}[y|x]$，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子：
$$
h_{\theta }(x) = p(y=1|x;\theta ) = 0 \cdot p(y=0|x;\theta) + 1 \cdot p(y=1|x;\theta) = \mathbb{E}[y|x;\theta ]
$$</li><li>自然参数$\eta$与$x$的联系是线性的，即$\eta = \theta^{\mathbf{T}}x$，若$\eta \in \mathbb{R}^{n}$，则$\eta_{i} = \theta_{i}^{\mathbf{T}}{x}$</li></ol><h2 id=glm例子>GLM例子<a hidden class=anchor aria-hidden=true href=#glm例子>#</a></h2><h3 id=ols>OLS<a hidden class=anchor aria-hidden=true href=#ols>#</a></h3><p>Ordinary Least Squares（线性回归）中的$y|x;\theta \sim \mathcal{N}(\mu, \sigma^{2})$，即服从高斯分布，其中$\eta = \mu$，那么：
<code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[y|x;\theta] \\ & = \mu \\ &= \eta \\ &= \theta^{\mathbf{T}}{x} \end{align} $$</code>
第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设</p><h3 id=逻辑回归>逻辑回归<a hidden class=anchor aria-hidden=true href=#逻辑回归>#</a></h3><p>逻辑回归中$y \in {0, 1}$，自然就想到了伯努利分布，即$y|x;\theta \sim \text{Bernoulli}(\phi )$，其中$\phi=1/1+ e^{-\eta }$，那么：
<code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[y|x;\theta ] \\ & = \phi \\ &= \frac{1}{1 + e^{-\eta }} \\ &= \frac{1}{1 + e^{-\theta^{\mathbf{T}}x}} \end{align} $$</code>
是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来</p><p>$g(\eta) = \mathbb{E}[T(y);\eta ]$被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而$g^{-1}$被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）</p><h2 id=softmax回归>softmax回归<a hidden class=anchor aria-hidden=true href=#softmax回归>#</a></h2><h3 id=构建glm-1>构建GLM<a hidden class=anchor aria-hidden=true href=#构建glm-1>#</a></h3><p>之前逻辑回归中是只有两类，当$y \in \{1, 2, \dots, k\}$，即现在是$k$分类，分布是multinomial distribution，接下来让我们构建GLM：</p><p>规定$\phi_{i}$规定了输出$y_{i}$的概率，那么$\phi_{i}, \dots, \phi_{k-1}$即是我们的参数，那你肯定好奇为什么$\phi_{k}$不是，因为输出所有类的概率之和为$1$，即$\phi_{k}$可被其他的概率表示：</p><p>$$
\phi_{k} = 1-\sum_{i}^{k-1} \phi_{i}
$$</p><p>以往的$T(y)=y$，对于多分类而言，我们采用独热编码（one-hot），即：</p><p><code>$$ T(1) = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, T(2) = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \dots ,T(k-1) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}, T(k) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} $$</code></p><p>注意$T(y) \in \mathbb{R}^{k-1}$，因为$T(k)$定义为全零向量，那么如何表示$T(y)$的第$i$个元素呢？
<span class=sidenote-number><small class=sidenote>指示函数当满足条件时为$1$，否则为$0$</small></span></p><p><code>$$ (T(y))_{i} = \mathbb{1}\{y=i\} $$</code></p><p>接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当$y$取一个具体值时，只有一个指示函数为$1$，其他为$0$，即$\phi_{i}^{0} = 1$</p><p><code>$$ \begin{align} p(y;\phi ) & = \phi^{\mathbb{1}\{y=1\}}_{1} \phi_{2}^{\mathbb{1}\{y=2\}} \dots \phi^{\mathbb{1}\{y=k\}}_{k} \\ & = \phi^{\mathbb{1}\{y=1\}}_{1} \phi_{2}^{\mathbb{1}\{y=2\}} \dots \phi^{1-\sum_{i}^{k-1}\mathbb{1}\{y=i\}}_{k} \\ &= \phi_{1}^{(T(y))_{1}} \phi_{2}^{(T(y)_{2})} \dots \phi_{k}^{1-\sum_{i}^{k-1}(T(y))_{i}} \\ \end{align} $$</code></p><p>继续变形来跟定义做比较：</p><p><code>$$ \begin{align} p(y; \phi)&= \exp \bigg((T(y))_{1}\log \phi_{1} + (T(y))_{2} \log \phi_{2} + \dots +(1-\sum_{i}^{k-1}(T(y))_{i})\log \phi_{k} \bigg) \\ &= \exp \bigg((T(y))_{1}\log \frac{\phi_{1}}{\phi_{k}} + (T(y))_{2}\log \frac{\phi_{2}}{\phi_{k}} + \dots+ (T(y))_{k-1}\log \frac{\phi_{k-1}}{\phi_{k}} + \log \phi_{k}\bigg) \end{align} $$</code></p><p>那么：</p><p><code>$$ \eta = \begin{bmatrix} \log (\phi_{1} / \phi_{k}) \\ \log (\phi_{2} / \phi_{k}) \\ \vdots \\ \log(\phi_{k-1} / \phi_{k}) \end{bmatrix}, a(\eta ) = -\log(\phi_{k}), b(y) =1 $$</code></p><p>链接函数容易发现是：</p><p><code>$$ \eta_{i} = \log \frac{\phi_{i}}{\phi_{k}} $$</code></p><p>接下来求响应函数：</p><p><code>$$ \begin{align} e^{\eta_{i}} & = \frac{\phi_{i}}{\phi_{k}} \\ \phi_{k}e^{\eta_{i}} & = \phi_{i} \\ \phi_{k}\sum_{i}^{k} e^{\eta_{i}} & = \sum_{i}^{k} \phi_{i} = 1 \end{align} $$</code></p><p>那么：</p><p><code>$$ \phi_{k} = \frac{1}{\sum_{i}^{k} e^{\eta_{i}}} $$</code></p><p>将$\phi_{k}$代入上式：</p><p><code>$$ \phi_{i} = \frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}} $$</code></p><p>这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：</p><p><code>$$ \begin{align} p(y=i|x;\theta ) & = \phi_{i} \\ &= \frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}} \\ &= \frac{e^{\theta_{i}^{\mathbf{T}}x}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x}} \end{align} $$</code></p><p>多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：</p><p><code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[T(y)|x;\theta ] \\ &= \mathbb{E}\left[\begin{array}{c|} \mathbb{1}\{y=1\} \\ \mathbb{1}\{y=2\} \\ \vdots \\ \mathbb{1}\{y=k-1\} \end{array} x;\theta \right] \end{align} $$</code></p><p>又因为：</p><p><code>$$ \mathbb{E}[(T(y))_{i}] = \phi_{i} $$</code></p><p>为啥会这样呢？因为对于$(T(y))_{i}$只有两个可能，$1$或$0$，那么它的期望是不是：</p><p><code>$$ \mathbb{E}[(T(y))_{i}] = 1 \cdot \phi_{i} + 0 \cdot (1-\phi ) = \phi_{i} $$</code></p><p><code>$$ h_{\theta }(x)= \begin{bmatrix} \phi_{1} \\ \phi_{2} \\ \vdots \\ \phi_{k-1} \end{bmatrix}= \begin{bmatrix} \frac{\exp ({\theta_{1}^{\mathbf{T}}x)}}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \\ \frac{\exp ({\theta_{2}^{\mathbf{T}}x})}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \\ \vdots \\ \frac{\exp ({\theta_{k-1}^{\mathbf{T}}x})}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \end{bmatrix} $$</code></p><p>也就是说我们的假设函数需要输出每个类的概率，尽管只有$k-1$类，$\phi_{k} = 1- \sum_{i}^{k-1} \phi_{i}$得到</p><p>接下来进行最大似然估计并对$\ell(\theta)$进行化简：</p><p><code>$$ \begin{align} \ell(\theta ) & = \sum_{i} \log p(y^{(i)}|x^{(i)};\theta ) \\ &= \sum_{i} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} \\ &= \sum_{i}^{m} \sum_{l=1}^{k} \log \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} \\ &= \sum_{i}^{m} \sum_{l=1}^{k} {\color{red}\mathbb{1}\{y^{(i)}=l\}} \log \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \left( \log e^{\theta_{l}^{\mathbf{T}}x^{(i)}}- \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}} \right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \left(\theta_{l}^{\mathbf{T}}x^{(i)}- \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \theta_{l}^{\mathbf{T}}x^{(i)}-\left( \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\underbrace{ \sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} }_{ 1 }\right) \\ &= \sum_{i}^{m} \bigg(\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \theta_{l}^{\mathbf{T}}x^{(i)} - \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\bigg) \end{align} $$</code></p><p>上述化简主要利用了指示函数的性质以及$\log$的运算法则，同时$\theta \in \mathbb{R}^{k \times n}$，我们利用布局法来求：</p><p><code>$$ \begin{align} \frac{ \partial \ell(\theta ) }{ \partial \theta_{pq} } & = \sum_{i}^{m} \mathbb{1}\{y^{(i)} = p\} x^{(i)}_{q} - \frac{1}{\sum_{j=1}^{k}e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} e^{\theta_{p}^{\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\ &= \sum_{i}^{m} \left( \mathbb{1}\{y^{(i)} = p\} - \frac{e^{\theta_{p}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)x^{(i)}_{q} \end{align} $$</code></p><p>因为这里是最大化$\ell(\theta)$，作为损失函数还应加个负号，这样才是最小，即</p><p><code>$$ J(\theta ) = -\sum_{i} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} $$</code></p><p>对应的微分如下：</p><p><code>$$ \frac{ \partial J(\theta ) }{ \partial \theta_{pq} } = \sum_{i}^{m} \left( \frac{e^{\theta_{p}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} - \mathbb{1}\{y^{(i)} = p\} \right)x^{(i)}_{q} $$</code></p><h3 id=交叉熵>交叉熵<a hidden class=anchor aria-hidden=true href=#交叉熵>#</a></h3><p>我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？</p><p>联想交叉熵的定义：</p><p><code>$$ H(P, Q) = - \mathbb{E}_{x \sim P}[\log Q(x)] $$</code></p><p>即使得模型输出的分布尽可能靠近训练集原来的分布：</p><p><code>$$ \theta^{\ast} = \mathop{\arg \min}_{\theta} -\mathbb{E}_{x \sim \mathcal{D}}[\log p_{model}(x)] $$</code></p><p>我们接下来展开期望的计算：</p><p><code>$$ -\mathbb{E}_{x \sim \mathcal{D}}[\log p_{model}(x)] = \frac{1}{m}\underbrace{ -\sum_{i}^{m} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} }_{ J(\theta) } $$</code></p><p>两者其实就差一个常数，本质是一样的</p></div><blockquote class=quote-copyright>Author: Yunpengtai<p>Link: https://yunpengtai.top/posts/generalized-linear-models/<p>License: CC BY-NC-SA 4.0. You must provide a link to the source.</blockquote><footer class=post-footer><ul class=post-tags><li><a href=https://yunpengtai.top/tags/linear-models/>Linear Models</a></li><li><a href=https://yunpengtai.top/tags/exponential-family/>Exponential Family</a></li></ul><nav class=paginav><a class=prev href=https://yunpengtai.top/posts/determinantal-point-process/><span class=title>« Prev</span><br><span>Determinantal Point Process</span></a>
<a class=next href=https://yunpengtai.top/posts/hello-world/><span class=title>Next »</span><br><span>新的主题</span></a></nav></footer><link rel=stylesheet href=https://unpkg.com/katex@0.16.7/dist/katex.min.css><script src=https://unpkg.com/katex@0.16.7/dist/katex.min.js></script>
<link href=https://cdn.jsdelivr.net/gh/sherlcok314159/artalk-blobcat@main/artalk.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/artalk/2.8.6/Artalk.js></script>
<script src=https://unpkg.com/@artalk/plugin-katex@latest/dist/artalk-plugin-katex.js></script><div id=Comments></div><script>Artalk.init({el:"#Comments",pageKey:"",pageTitle:"Generalized Linear Models",server:"https://comment.yunpengtai.top",site:"Tai's Blog"})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("//cdn.bootcss.com/pangu/4.0.7/pangu.min.js",function(){pangu.spacingPage()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>