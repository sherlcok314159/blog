<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generalized Linear Models | Tai's Blog</title><meta name=keywords content="linear models"><meta name=description content="定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ \begin{equation} p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) \end{equation} $$ 其中$\eta$被称为分布的自然参数（n"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=http://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=http://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"all",packages:{"[+]":["boldsymbol"]}},chtml:{scale:.9}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Generalized Linear Models"><meta property="og:description" content="定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ \begin{equation} p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) \end{equation} $$ 其中$\eta$被称为分布的自然参数（n"><meta property="og:type" content="article"><meta property="og:url" content="http://yunpengtai.top/posts/generalized-linear-models/"><meta property="og:image" content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-17T14:10:00+08:00"><meta property="article:modified_time" content="2023-02-17T14:10:00+08:00"><meta property="og:site_name" content="Tai's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Generalized Linear Models"><meta name=twitter:description content="定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ \begin{equation} p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) \end{equation} $$ 其中$\eta$被称为分布的自然参数（n"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"http://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Generalized Linear Models","item":"http://yunpengtai.top/posts/generalized-linear-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generalized Linear Models","name":"Generalized Linear Models","description":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ \\begin{equation} p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) \\end{equation} $$ 其中$\\eta$被称为分布的自然参数（n","keywords":["linear models"],"articleBody":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员\n$$ \\begin{equation} p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) \\end{equation} $$ 其中$\\eta$被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而$T(y)$被称为统计充分量（sufficient statistic），通常而言：$T(y) = y$；$a(\\eta)$是对数配分函数（log partition）\n例子 接下来展示下伯努利分布和高斯分布都是指数族的一员\n期望为$\\phi$的伯努利分布且$y \\in \\{1, 0\\}$，那么：\n$$ p(y=1) = \\phi; p(y=0) = 1-\\phi $$\n接着进行转换：\n$$ \\begin{align} p(y;\\phi ) \u0026 = \\phi^{y}(1-\\phi )^{1-y} \\\\ \u0026= \\exp \\bigg(\\log(\\phi^{y}(1-\\phi )^{1-y})\\bigg) \\\\ \u0026= \\exp(y\\log \\phi + (1-y)\\log(1-\\phi )) \\\\ \u0026= \\exp\\left( \\left( \\log \\frac{\\phi }{1-\\phi } \\right)y + \\log(1-\\phi) \\right) \\end{align} $$\n那么可以对比着指数族的定义，易得$b(y)=1$以及：\n$$ \\begin{align} \\eta \u0026 = \\log\\left( \\frac{\\phi }{1-\\phi } \\right) \\\\ e^{\\eta } \u0026 = \\frac{\\phi}{1-\\phi } \\\\ {\\color{red}e^{-\\eta }} \u0026 = \\frac{1-\\phi}{\\phi } = \\frac{1}{\\phi } - 1 \\\\ \\phi \u0026 = \\frac{1}{1+e^{-\\eta }} \\end{align} $$\n发现很有趣的一点，$\\phi(\\eta)$不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：\n$$ \\begin{align} a(\\eta ) \u0026 = -\\log(1-\\phi ) = -\\log\\left( 1-\\frac{1}{1+e^{-\\eta }} \\right) \\\\ \u0026= \\log (1+e^{\\eta }) \\end{align} $$\n接下来讨论高斯分布，$\\sigma^{2}$对$\\theta, h_{\\theta }(x)$是不影响的（相当于常数），为了简化表示，约定$\\sigma^{2}=1$\n$$ \\begin{align} p(y;\\mu ) \u0026 = \\frac{1}{\\sqrt{ 2\\pi }}\\exp\\left( -\\frac{(y-\\mu )^{2}}{2} \\right) \\\\ \u0026= \\underbrace{ \\frac{1}{\\sqrt{ 2\\pi }} \\exp\\left( -\\frac{y^{2}}{2} \\right) }_{ b(y) }\\exp\\left( \\mu y - \\frac{\\mu^{2}}{2} \\right) \\end{align} $$\n比对定义，可以发现：\n$$ \\eta = \\mu; a(\\eta ) = -\\frac{\\eta^{2}}{2} $$\n性质 $$ p(y;\\eta ) = b(y)\\exp (\\eta^{\\mathbf{T}}T(y) - a(\\eta)) $$\n指数族分布的期望是$a(\\eta)$对$\\eta$的一阶微分 指数族分布的方差是$a(\\eta)$对$\\eta$的二阶微分 指数族分布的NLL loss是concave的 下面来证明上述观点：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) \u0026 = b(y) \\exp(\\eta^{\\mathbf{T}}y - a(\\eta )) \\left( y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\right) \\\\ \u0026= yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\end{align} $$\n那么：\n$$ y p(y;\\eta ) = \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) $$\n又因为：\n$$ \\begin{align} \\mathbb{E}[Y;\\eta ] \u0026 = \\mathbb{E}[Y|X;\\eta] \\\\ \u0026= \\int yp(y;\\eta ) \\,dy \\\\ \u0026= \\int \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026= \\int \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\, dy + \\int p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\int p(y;\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } \\cdot 1 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\cdot 1 \\\\ \u0026= 0 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta } a(\\eta) \\quad \\blacksquare \\end{align} $$\n下面来证明方差：\n$$ \\begin{align} \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) \u0026 = \\frac{ \\partial }{ \\partial \\eta } \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg) \\\\ \u0026= y\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\\\ \u0026= \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg)\\bigg(y- \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= y^{2}p(y;\\eta ) - 2yp(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) + p(y;\\eta ) (\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) )^{2} - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) -p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\end{align} $$\n那么：\n$$ \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) +p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) $$\n又因为：\n$$ \\begin{align} \\mathbb{V}[Y;\\eta ] \u0026 = \\mathbb{V}[Y|X;\\eta] \\\\ \u0026= \\int \\left( y - \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\right)^{2} p(y;\\eta)\\, dy \\\\ \u0026= \\int \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }p(y;\\eta) + p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }a(\\eta ) \\, dy \\\\ \u0026= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta )\\int p(y;\\eta ) \\, dy \\\\ \u0026= 0 + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\quad \\blacksquare \\end{align} $$\n接下来证明NLL Loss是凸函数，其中$a(\\eta) \\in \\mathbb{R}^{m}, \\mathbf{y} \\in \\mathbb{R}^{m}$：\n$$ \\begin{align} J(\\eta) \u0026 = -\\log \\sum_{i}^{m} p(y^{(i)};\\eta_{i} ) \\\\ \u0026= -\\log \\sum_{i}^{m} b(y^{(i)})\\exp(\\eta_{i} T(y^{(i)}) - a(\\eta_{i})) \\\\ \u0026= -\\log \\sum_{i}^{m} b(y^{(i)}) \\exp( \\eta_{i} y^{(i)}- a(\\eta_{i})) \\\\ \u0026= a(\\eta) -\\sum_{i}^{m} \\log b(y^{(i)}) + \\eta_{i}y^{(i)} \\end{align} $$ 同时因为自身的协方差矩阵是「半正定」的：\n$$ \\begin{align} \\nabla_{\\eta }^{2} J(\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) = \\mathbb{V}[\\mathbf{y};\\eta ] \\implies PSD\\quad \\blacksquare \\end{align} $$\n由[[Critical Points#多变量]]可知，当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点\n构建GLM 在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：\n$y|x; \\theta \\sim \\mathrm{ExponentialFamily}(\\eta )$，也就是说，给定$x, \\theta$ 我们可以得到$y$的分布就是带有参数$\\eta$的指数族分布 给定$x$，我们需要去预测$y$，在指数族中，$y=T(y)$，也就是说我们得去预测期望，即学得的假设$h$需要去预测期望，即：$h(x) = \\mathbb{E}[y|x]$，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子： $$ h_{\\theta }(x) = p(y=1|x;\\theta ) = 0 \\cdot p(y=0|x;\\theta) + 1 \\cdot p(y=1|x;\\theta) = \\mathbb{E}[y|x;\\theta ] $$ 自然参数$\\eta$与$x$的联系是线性的，即$\\eta = \\theta^{\\mathbf{T}}x$，若$\\eta \\in \\mathbb{R}^{n}$，则$\\eta_{i} = \\theta_{i}^{\\mathbf{T}}{x}$ 根据变量的特点来选择合适的分布：\nVariable Distribution Real Numbers $\\mathbb{R}$ Gaussian Binary Classification Bernoulli Count Poisson $\\mathbb{R}^{+}$ Gamma, Exponential Distribution Beta, Dirichlet GLM例子 接下来举一些GLM的例子\nOLS Ordinary Least Squares（线性回归）中的$y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，即服从高斯分布，其中$\\eta = \\mu$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[y|x;\\theta] \\\\ \u0026 = \\mu \\\\ \u0026= \\eta \\\\ \u0026= \\theta^{\\mathbf{T}}{x} \\end{align} $$ 第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设\n逻辑回归 逻辑回归中$y \\in \\{0, 1\\}$，自然就想到了伯努利分布，即$y|x;\\theta \\sim \\text{Bernoulli}(\\phi )$，其中$\\phi=1/1+ e^{-\\eta }$，那么：\n$$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[y|x;\\theta ] \\\\ \u0026 = \\phi \\\\ \u0026= \\frac{1}{1 + e^{-\\eta }} \\\\ \u0026= \\frac{1}{1 + e^{-\\theta^{\\mathbf{T}}x}} \\end{align} $$ 是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来\n$g(\\eta) = \\mathbb{E}[T(y);\\eta ]$被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而$g^{-1}$被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）\nsoftmax回归 构建GLM 之前逻辑回归中是只有两类，当$y \\in \\{1, 2, \\dots, k\\}$，即现在是$k$分类，分布是multinomial distribution，接下来让我们构建GLM：\n规定$\\phi_{i}$规定了输出$y_{i}$的概率，那么$\\phi_{i}, \\dots, \\phi_{k-1}$即是我们的参数，那你肯定好奇为什么$\\phi_{k}$不是，因为输出所有类的概率之和为$1$，即$\\phi_{k}$可被其他的概率表示：\n$$ \\phi_{k} = 1-\\sum_{i}^{k-1} \\phi_{i} $$ 以往的$T(y)=y$，对于多分类而言，我们采用独热编码（one-hot），即：\n$$ T(1) = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2) = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\dots ,T(k-1) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, T(k) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$ 注意$T(y) \\in \\mathbb{R}^{k-1}$，因为$T(k)$定义为全零向量，那么如何表示$T(y)$的第$i$个元素呢？\n$$ (T(y))_{i} = \\mathbb{1}\\{y=i\\} $$\n接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当$y$取一个具体值时，只有一个指示函数为$1$，其他为$0$，即$\\phi_{i}^{0} = 1$\n$$ \\begin{align} p(y;\\phi ) \u0026 = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{\\mathbb{1}\\{y=k\\}}_{k} \\\\ \u0026 = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{1-\\sum_{i}^{k-1}\\mathbb{1}\\{y=i\\}}_{k} \\\\ \u0026= \\phi_{1}^{(T(y))_{1}} \\phi_{2}^{(T(y)_{2})} \\dots \\phi_{k}^{1-\\sum_{i}^{k-1}(T(y))_{i}} \\\\ \\end{align} $$ 继续变形来跟定义做比较：\n$$ \\begin{align} p(y; \\phi)\u0026= \\exp \\bigg((T(y))_{1}\\log \\phi_{1} + (T(y))_{2} \\log \\phi_{2} + \\dots +(1-\\sum_{i}^{k-1}(T(y))_{i})\\log \\phi_{k} \\bigg) \\\\ \u0026= \\exp \\bigg((T(y))_{1}\\log \\frac{\\phi_{1}}{\\phi_{k}} + (T(y))_{2}\\log \\frac{\\phi_{2}}{\\phi_{k}} + \\dots+ (T(y))_{k-1}\\log \\frac{\\phi_{k-1}}{\\phi_{k}} + \\log \\phi_{k}\\bigg) \\end{align} $$\n那么：\n$$ \\eta = \\begin{bmatrix} \\log (\\phi_{1} / \\phi_{k}) \\\\ \\log (\\phi_{2} / \\phi_{k}) \\\\ \\vdots \\\\ \\log(\\phi_{k-1} / \\phi_{k}) \\end{bmatrix}, a(\\eta ) = -\\log(\\phi_{k}), b(y) =1 $$ 链接函数容易发现是：\n$$ \\eta_{i} = \\log \\frac{\\phi_{i}}{\\phi_{k}} $$ 接下来求响应函数：\n$$ \\begin{align} e^{\\eta_{i}} \u0026 = \\frac{\\phi_{i}}{\\phi_{k}} \\\\ \\phi_{k}e^{\\eta_{i}} \u0026 = \\phi_{i} \\\\ \\phi_{k}\\sum_{i}^{k} e^{\\eta_{i}} \u0026 = \\sum_{i}^{k} \\phi_{i} = 1 \\end{align} $$ 那么：\n$$ \\phi_{k} = \\frac{1}{\\sum_{i}^{k} e^{\\eta_{i}}} $$ 将$\\phi_{k}$代入上式：\n$$ \\phi_{i} = \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} $$ 这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：\n$$ \\begin{align} p(y=i|x;\\theta ) \u0026 = \\phi_{i} \\\\ \u0026= \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} \\\\ \u0026= \\frac{e^{\\theta_{i}^{\\mathbf{T}}x}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x}} \\end{align} $$ 多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：\n$$ \\begin{align} h_{\\theta }(x) \u0026 = \\mathbb{E}[T(y)|x;\\theta ] \\\\ \u0026= \\mathbb{E}\\left[\\begin{array}{c|} \\mathbb{1}\\{y=1\\} \\\\ \\mathbb{1}\\{y=2\\} \\\\ \\vdots \\\\ \\mathbb{1}\\{y=k-1\\} \\end{array} x;\\theta \\right] \\\\ \\end{align} $$\n又因为：\n$$ \\mathbb{E}[(T(y))_{i}] = \\phi_{i} $$ 为啥会这样呢？因为对于$(T(y))_{i}$只有两个可能，$1$或$0$，那么它的期望是不是：\n$$ \\mathbb{E}[(T(y))_{i}] = 1 \\cdot \\phi_{i} + 0 \\cdot (1-\\phi ) = \\phi_{i} $$\n$$ h_{\\theta }(x)= \\begin{bmatrix} \\phi_{1} \\\\ \\phi_{2} \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix}= \\begin{bmatrix} \\frac{\\exp ({\\theta_{1}^{\\mathbf{T}}x)}}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\frac{\\exp ({\\theta_{2}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\vdots \\\\ \\frac{\\exp ({\\theta_{k-1}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\end{bmatrix} $$ 也就是说我们的假设函数需要输出每个类的概率，尽管只有$k-1$类，$\\phi_{k} = 1- \\sum_{i}^{k-1} \\phi_{i}$得到\n接下来进行最大似然估计并对$\\ell(\\theta)$进行化简：\n$$ \\begin{align} \\ell(\\theta ) \u0026 = \\sum_{i} \\log p(y^{(i)}|x^{(i)};\\theta ) \\\\ \u0026= \\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026= \\sum_{i}^{m} \\sum_{l=1}^{k} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026= \\sum_{i}^{m} \\sum_{l=1}^{k} {\\color{red}\\mathbb{1}\\{y^{(i)}=l\\}} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left( \\log e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}} \\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left(\\theta_{l}^{\\mathbf{T}}x^{(i)}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\right) \\\\ \u0026= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)}-\\left( \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\underbrace{ \\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} }_{ 1 }\\right) \\\\ \u0026= \\sum_{i}^{m} \\bigg(\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)} - \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\bigg) \\end{align} $$\n上述化简主要利用了指示函数的性质以及$\\log$的运算法则，同时$\\theta \\in \\mathbb{R}^{k \\times n}$，我们利用布局法来求：\n$$ \\begin{align} \\frac{ \\partial \\ell(\\theta ) }{ \\partial \\theta_{pq} } \u0026 = \\sum_{i}^{m} \\mathbb{1}\\{y^{(i)} = p\\} x^{(i)}_{q} - \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\\\ \u0026= \\sum_{i}^{m} \\left( \\mathbb{1}\\{y^{(i)} = p\\} - \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)x^{(i)}_{q} \\end{align} $$ 因为这里是最大化$\\ell(\\theta)$，作为损失函数还应加个负号，这样才是最小，即\n$$ J(\\theta ) = -\\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} $$\n对应的微分如下： $$ \\frac{ \\partial J(\\theta ) }{ \\partial \\theta_{pq} } = \\sum_{i}^{m} \\left( \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} - \\mathbb{1}\\{y^{(i)} = p\\} \\right)x^{(i)}_{q} $$\n交叉熵 我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？\n联想交叉熵的定义：\n$$ H(P, Q) = - \\mathbb{E}_{x \\sim P}[\\log Q(x)] $$ 即使得模型输出的分布尽可能靠近训练集原来的分布：\n$$ \\theta^{\\ast} = \\mathop{\\arg \\min}_{\\theta} -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] $$ 我们接下来展开期望的计算：\n$$ -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] = \\frac{1}{m}\\underbrace{ -\\sum_{i}^{m} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} }_{ J(\\theta) } $$ 两者其实就差一个常数，本质是一样的\n代码实现也比较轻松：\ndef CrossEntropy(y_pred, y_true): batch_size = y_pred.shape[0] y_pred = np.exp(y_pred) y_pred /= np.sum(y_pred, axis=1)[:, None] y_pred = np.take_along_axis(y_pred, y_true[:, None], axis=1) y_pred = np.log(y_pred) return -np.sum(y_pred) / batch_size ","wordCount":"3664","inLanguage":"en","datePublished":"2023-02-17T14:10:00+08:00","dateModified":"2023-02-17T14:10:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://yunpengtai.top/posts/generalized-linear-models/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"http://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://yunpengtai.top/archives/ title=归档><span>归档</span></a></li><li><a href=http://yunpengtai.top/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=http://yunpengtai.top/categories/%E6%8A%98%E8%85%BE title=折腾><span>折腾</span></a></li><li><a href=http://yunpengtai.top/tags/ title=标签><span>标签</span></a></li><li><a href=http://yunpengtai.top/friends/ title=友人><span>友人</span></a></li><li><a href=http://yunpengtai.top/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=http://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Generalized Linear Models</h1><div class=post-meta><span title='2023-02-17 14:10:00 +0800 CST'>February 17, 2023</span>&nbsp;·&nbsp;3664 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%ae%9a%e4%b9%89 aria-label=定义>定义</a></li><li><a href=#%e4%be%8b%e5%ad%90 aria-label=例子>例子</a></li><li><a href=#%e6%80%a7%e8%b4%a8 aria-label=性质>性质</a></li><li><a href=#%e6%9e%84%e5%bb%baglm aria-label=构建GLM>构建GLM</a></li><li><a href=#glm%e4%be%8b%e5%ad%90 aria-label=GLM例子>GLM例子</a><ul><li><a href=#ols aria-label=OLS>OLS</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92 aria-label=逻辑回归>逻辑回归</a></li></ul></li><li><a href=#softmax%e5%9b%9e%e5%bd%92 aria-label=softmax回归>softmax回归</a><ul><li><a href=#%e6%9e%84%e5%bb%baglm-1 aria-label=构建GLM>构建GLM</a></li><li><a href=#%e4%ba%a4%e5%8f%89%e7%86%b5 aria-label=交叉熵>交叉熵</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=定义>定义<a hidden class=anchor aria-hidden=true href=#定义>#</a></h2><p>若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员</p><p><code>$$ \begin{equation} p(y; \eta ) = b(y)\exp(\eta^{\mathbf{T}}T(y) - a(\eta )) \end{equation} $$</code>
其中<code>$\eta$</code>被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而<code>$T(y)$</code>被称为统计充分量（sufficient statistic），通常而言：<code>$T(y) = y$</code>；<code>$a(\eta)$</code>是对数配分函数（log partition）</p><h2 id=例子>例子<a hidden class=anchor aria-hidden=true href=#例子>#</a></h2><p>接下来展示下伯努利分布和高斯分布都是指数族的一员</p><p>期望为<code>$\phi$</code>的伯努利分布且<code>$y \in \{1, 0\}$</code>，那么：</p><p><code>$$ p(y=1) = \phi; p(y=0) = 1-\phi $$</code></p><p>接着进行转换：</p><p><code>$$ \begin{align} p(y;\phi ) & = \phi^{y}(1-\phi )^{1-y} \\ &= \exp \bigg(\log(\phi^{y}(1-\phi )^{1-y})\bigg) \\ &= \exp(y\log \phi + (1-y)\log(1-\phi )) \\ &= \exp\left( \left( \log \frac{\phi }{1-\phi } \right)y + \log(1-\phi) \right) \end{align} $$</code></p><p>那么可以对比着指数族的定义，易得<code>$b(y)=1$</code>以及：</p><p><code>$$ \begin{align} \eta & = \log\left( \frac{\phi }{1-\phi } \right) \\ e^{\eta } & = \frac{\phi}{1-\phi } \\ {\color{red}e^{-\eta }} & = \frac{1-\phi}{\phi } = \frac{1}{\phi } - 1 \\ \phi & = \frac{1}{1+e^{-\eta }} \end{align} $$</code></p><p>发现很有趣的一点，<code>$\phi(\eta)$</code>不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：</p><p><code>$$ \begin{align} a(\eta ) & = -\log(1-\phi ) = -\log\left( 1-\frac{1}{1+e^{-\eta }} \right) \\ &= \log (1+e^{\eta }) \end{align} $$</code></p><p>接下来讨论高斯分布，<code>$\sigma^{2}$</code>对<code>$\theta, h_{\theta }(x)$</code>是不影响的（相当于常数），为了简化表示，约定<code>$\sigma^{2}=1$</code></p><p><code>$$ \begin{align} p(y;\mu ) & = \frac{1}{\sqrt{ 2\pi }}\exp\left( -\frac{(y-\mu )^{2}}{2} \right) \\ &= \underbrace{ \frac{1}{\sqrt{ 2\pi }} \exp\left( -\frac{y^{2}}{2} \right) }_{ b(y) }\exp\left( \mu y - \frac{\mu^{2}}{2} \right) \end{align} $$</code></p><p>比对定义，可以发现：</p><p><code>$$ \eta = \mu; a(\eta ) = -\frac{\eta^{2}}{2} $$</code></p><h2 id=性质>性质<a hidden class=anchor aria-hidden=true href=#性质>#</a></h2><p><code>$$ p(y;\eta ) = b(y)\exp (\eta^{\mathbf{T}}T(y) - a(\eta)) $$</code></p><div class="notice notice-note"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zm-248 50a46 46 0 100 92 46 46 0 000-92zm-44-165 8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z"/></svg></div><ol><li>指数族分布的期望是<code>$a(\eta)$</code>对<code>$\eta$</code>的一阶微分</li><li>指数族分布的方差是<code>$a(\eta)$</code>对<code>$\eta$</code>的二阶微分</li><li>指数族分布的NLL loss是concave的</li></ol></div><p>下面来证明上述观点：</p><p><code>$$ \begin{align} \frac{ \partial }{ \partial \eta } p(y;\eta ) & = b(y) \exp(\eta^{\mathbf{T}}y - a(\eta )) \left( y - \frac{ \partial }{ \partial \eta }a(\eta ) \right) \\ &= yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta ) \end{align} $$</code></p><p>那么：</p><p><code>$$ y p(y;\eta ) = \frac{ \partial }{ \partial \eta } p(y;\eta ) + p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta ) $$</code></p><p>又因为：</p><p><code>$$ \begin{align} \mathbb{E}[Y;\eta ] & = \mathbb{E}[Y|X;\eta] \\ &= \int yp(y;\eta ) \,dy \\ &= \int \frac{ \partial }{ \partial \eta } p(y;\eta ) + p(y;\eta )\frac{ \partial }{ \partial \eta }a(\eta ) \, dy \\ &= \int \frac{ \partial }{ \partial \eta }p(y;\eta ) \, dy + \int p(y;\eta )\frac{ \partial }{ \partial \eta }a(\eta ) \, dy \\ &= \frac{ \partial }{ \partial \eta } \int p(y;\eta ) \, dy + \frac{ \partial }{ \partial \eta }a(\eta ) \int p(y;\eta ) \, dy \\ &= \frac{ \partial }{ \partial \eta } \cdot 1 + \frac{ \partial }{ \partial \eta } a(\eta ) \cdot 1 \\ &= 0 + \frac{ \partial }{ \partial \eta } a(\eta ) \\ &= \frac{ \partial }{ \partial \eta } a(\eta) \quad \blacksquare \end{align} $$</code></p><p>下面来证明方差：</p><p><code>$$ \begin{align} \frac{ \partial^{2} }{ \partial \eta^{2} } p(y;\eta ) & = \frac{ \partial }{ \partial \eta } \bigg(yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta )\bigg) \\ &= y\frac{ \partial }{ \partial \eta }p(y;\eta ) - \frac{ \partial }{ \partial \eta } a(\eta )\frac{ \partial }{ \partial \eta }p(y;\eta ) - p(y;\eta )\frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) \\ &= \frac{ \partial }{ \partial \eta }p(y;\eta ) \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta ) \bigg) - p(y;\eta )\frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \bigg(yp(y;\eta ) - p(y;\eta ) \frac{ \partial }{ \partial \eta } a(\eta )\bigg)\bigg(y- \frac{ \partial }{ \partial \eta }a(\eta ) \bigg) - p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= y^{2}p(y;\eta ) - 2yp(y;\eta ) \frac{ \partial }{ \partial \eta }a(\eta ) + p(y;\eta ) (\frac{ \partial }{ \partial \eta }a(\eta ) )^{2} - p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta) \bigg)^{2} p(y;\eta ) -p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) \end{align} $$</code></p><p>那么：</p><p><code>$$ \bigg(y - \frac{ \partial }{ \partial \eta }a(\eta) \bigg)^{2} p(y;\eta ) = \frac{ \partial^{2} }{ \partial \eta^{2} } p(y;\eta ) +p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta) $$</code></p><p>又因为：</p><p><code>$$ \begin{align} \mathbb{V}[Y;\eta ] & = \mathbb{V}[Y|X;\eta] \\ &= \int \left( y - \frac{ \partial }{ \partial \eta } a(\eta ) \right)^{2} p(y;\eta)\, dy \\ &= \int \frac{ \partial^{2} }{ \partial \eta^{2} }p(y;\eta) + p(y;\eta ) \frac{ \partial^{2} }{ \partial \eta^{2} }a(\eta ) \, dy \\ &= \frac{ \partial^{2} }{ \partial \eta^{2} } \int p(y;\eta ) \, dy + \frac{ \partial ^{2} }{ \partial \eta^{2} } a(\eta )\int p(y;\eta ) \, dy \\ &= 0 + \frac{ \partial ^{2} }{ \partial \eta^{2} } a(\eta ) \\ &= \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) \quad \blacksquare \end{align} $$</code></p><p>接下来证明NLL Loss是凸函数，其中<code>$a(\eta) \in \mathbb{R}^{m}, \mathbf{y} \in \mathbb{R}^{m}$</code>：</p><p><code>$$ \begin{align} J(\eta) & = -\log \sum_{i}^{m} p(y^{(i)};\eta_{i} ) \\ &= -\log \sum_{i}^{m} b(y^{(i)})\exp(\eta_{i} T(y^{(i)}) - a(\eta_{i})) \\ &= -\log \sum_{i}^{m} b(y^{(i)}) \exp( \eta_{i} y^{(i)}- a(\eta_{i})) \\ &= a(\eta) -\sum_{i}^{m} \log b(y^{(i)}) + \eta_{i}y^{(i)} \end{align} $$</code>
同时因为自身的协方差矩阵是「半正定」的：</p><p><code>$$ \begin{align} \nabla_{\eta }^{2} J(\eta ) = \frac{ \partial^{2} }{ \partial \eta^{2} } a(\eta ) = \mathbb{V}[\mathbf{y};\eta ] \implies PSD\quad \blacksquare \end{align} $$</code></p><p>由[[Critical Points#多变量]]可知，当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点</p><h2 id=构建glm>构建GLM<a hidden class=anchor aria-hidden=true href=#构建glm>#</a></h2><p>在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：</p><ol><li><code>$y|x; \theta \sim \mathrm{ExponentialFamily}(\eta )$</code>，也就是说，给定<code>$x, \theta$</code> 我们可以得到<code>$y$</code>的分布就是带有参数<code>$\eta$</code>的指数族分布</li><li>给定<code>$x$</code>，我们需要去预测<code>$y$</code>，在指数族中，<code>$y=T(y)$</code>，也就是说我们得去预测期望，即学得的假设<code>$h$</code>需要去预测期望，即：<code>$h(x) = \mathbb{E}[y|x]$</code>，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子：
<code>$$ h_{\theta }(x) = p(y=1|x;\theta ) = 0 \cdot p(y=0|x;\theta) + 1 \cdot p(y=1|x;\theta) = \mathbb{E}[y|x;\theta ] $$</code></li><li>自然参数<code>$\eta$</code>与<code>$x$</code>的联系是线性的，即<code>$\eta = \theta^{\mathbf{T}}x$</code>，若<code>$\eta \in \mathbb{R}^{n}$</code>，则<code>$\eta_{i} = \theta_{i}^{\mathbf{T}}{x}$</code></li></ol><p>根据变量的特点来选择合适的分布：</p><table><thead><tr><th style=text-align:center>Variable</th><th style=text-align:center>Distribution</th></tr></thead><tbody><tr><td style=text-align:center>Real Numbers <code>$\mathbb{R}$</code></td><td style=text-align:center>Gaussian</td></tr><tr><td style=text-align:center>Binary Classification</td><td style=text-align:center>Bernoulli</td></tr><tr><td style=text-align:center>Count</td><td style=text-align:center>Poisson</td></tr><tr><td style=text-align:center><code>$\mathbb{R}^{+}$</code></td><td style=text-align:center>Gamma, Exponential</td></tr><tr><td style=text-align:center>Distribution</td><td style=text-align:center>Beta, Dirichlet</td></tr></tbody></table><h2 id=glm例子>GLM例子<a hidden class=anchor aria-hidden=true href=#glm例子>#</a></h2><p>接下来举一些GLM的例子</p><h3 id=ols>OLS<a hidden class=anchor aria-hidden=true href=#ols>#</a></h3><p>Ordinary Least Squares（线性回归）中的<code>$y|x;\theta \sim \mathcal{N}(\mu, \sigma^{2})$</code>，即服从高斯分布，其中<code>$\eta = \mu$</code>，那么：
<code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[y|x;\theta] \\ & = \mu \\ &= \eta \\ &= \theta^{\mathbf{T}}{x} \end{align} $$</code>
第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设</p><h3 id=逻辑回归>逻辑回归<a hidden class=anchor aria-hidden=true href=#逻辑回归>#</a></h3><p>逻辑回归中<code>$y \in \{0, 1\}$</code>，自然就想到了伯努利分布，即<code>$y|x;\theta \sim \text{Bernoulli}(\phi )$</code>，其中<code>$\phi=1/1+ e^{-\eta }$</code>，那么：</p><p><code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[y|x;\theta ] \\ & = \phi \\ &= \frac{1}{1 + e^{-\eta }} \\ &= \frac{1}{1 + e^{-\theta^{\mathbf{T}}x}} \end{align} $$</code>
是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来</p><p><code>$g(\eta) = \mathbb{E}[T(y);\eta ]$</code>被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而<code>$g^{-1}$</code>被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）</p><h2 id=softmax回归>softmax回归<a hidden class=anchor aria-hidden=true href=#softmax回归>#</a></h2><h3 id=构建glm-1>构建GLM<a hidden class=anchor aria-hidden=true href=#构建glm-1>#</a></h3><p>之前逻辑回归中是只有两类，当<code>$y \in \{1, 2, \dots, k\}$</code>，即现在是<code>$k$</code>分类，分布是multinomial distribution，接下来让我们构建GLM：</p><p>规定<code>$\phi_{i}$</code>规定了输出<code>$y_{i}$</code>的概率，那么<code>$\phi_{i}, \dots, \phi_{k-1}$</code>即是我们的参数，那你肯定好奇为什么<code>$\phi_{k}$</code>不是，因为输出所有类的概率之和为<code>$1$</code>，即<code>$\phi_{k}$</code>可被其他的概率表示：</p><p><code>$$ \phi_{k} = 1-\sum_{i}^{k-1} \phi_{i} $$</code>
以往的<code>$T(y)=y$</code>，对于多分类而言，我们采用独热编码（one-hot），即：</p><p><code>$$ T(1) = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, T(2) = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \dots ,T(k-1) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}, T(k) = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} $$</code>
注意<code>$T(y) \in \mathbb{R}^{k-1}$</code>，因为<code>$T(k)$</code>定义为全零向量，那么如何表示<code>$T(y)$</code>的第<code>$i$</code>个元素呢？</p><p><code>$$ (T(y))_{i} = \mathbb{1}\{y=i\} $$</code></p><p>接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当<code>$y$</code>取一个具体值时，只有一个指示函数为<code>$1$</code>，其他为<code>$0$</code>，即<code>$\phi_{i}^{0} = 1$</code></p><p><code>$$ \begin{align} p(y;\phi ) & = \phi^{\mathbb{1}\{y=1\}}_{1} \phi_{2}^{\mathbb{1}\{y=2\}} \dots \phi^{\mathbb{1}\{y=k\}}_{k} \\ & = \phi^{\mathbb{1}\{y=1\}}_{1} \phi_{2}^{\mathbb{1}\{y=2\}} \dots \phi^{1-\sum_{i}^{k-1}\mathbb{1}\{y=i\}}_{k} \\ &= \phi_{1}^{(T(y))_{1}} \phi_{2}^{(T(y)_{2})} \dots \phi_{k}^{1-\sum_{i}^{k-1}(T(y))_{i}} \\ \end{align} $$</code>
继续变形来跟定义做比较：</p><p><code>$$ \begin{align} p(y; \phi)&= \exp \bigg((T(y))_{1}\log \phi_{1} + (T(y))_{2} \log \phi_{2} + \dots +(1-\sum_{i}^{k-1}(T(y))_{i})\log \phi_{k} \bigg) \\ &= \exp \bigg((T(y))_{1}\log \frac{\phi_{1}}{\phi_{k}} + (T(y))_{2}\log \frac{\phi_{2}}{\phi_{k}} + \dots+ (T(y))_{k-1}\log \frac{\phi_{k-1}}{\phi_{k}} + \log \phi_{k}\bigg) \end{align} $$</code></p><p>那么：</p><p><code>$$ \eta = \begin{bmatrix} \log (\phi_{1} / \phi_{k}) \\ \log (\phi_{2} / \phi_{k}) \\ \vdots \\ \log(\phi_{k-1} / \phi_{k}) \end{bmatrix}, a(\eta ) = -\log(\phi_{k}), b(y) =1 $$</code>
链接函数容易发现是：</p><p><code>$$ \eta_{i} = \log \frac{\phi_{i}}{\phi_{k}} $$</code>
接下来求响应函数：</p><p><code>$$ \begin{align} e^{\eta_{i}} & = \frac{\phi_{i}}{\phi_{k}} \\ \phi_{k}e^{\eta_{i}} & = \phi_{i} \\ \phi_{k}\sum_{i}^{k} e^{\eta_{i}} & = \sum_{i}^{k} \phi_{i} = 1 \end{align} $$</code>
那么：</p><p><code>$$ \phi_{k} = \frac{1}{\sum_{i}^{k} e^{\eta_{i}}} $$</code>
将<code>$\phi_{k}$</code>代入上式：</p><p><code>$$ \phi_{i} = \frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}} $$</code>
这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：</p><p><code>$$ \begin{align} p(y=i|x;\theta ) & = \phi_{i} \\ &= \frac{e^{\eta_{i}}}{\sum_{j=1}^{k} e^{\eta_{j}}} \\ &= \frac{e^{\theta_{i}^{\mathbf{T}}x}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x}} \end{align} $$</code>
多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：</p><p><code>$$ \begin{align} h_{\theta }(x) & = \mathbb{E}[T(y)|x;\theta ] \\ &= \mathbb{E}\left[\begin{array}{c|} \mathbb{1}\{y=1\} \\ \mathbb{1}\{y=2\} \\ \vdots \\ \mathbb{1}\{y=k-1\} \end{array} x;\theta \right] \\ \end{align} $$</code></p><p>又因为：</p><p><code>$$ \mathbb{E}[(T(y))_{i}] = \phi_{i} $$</code>
为啥会这样呢？因为对于<code>$(T(y))_{i}$</code>只有两个可能，<code>$1$</code>或<code>$0$</code>，那么它的期望是不是：</p><p><code>$$ \mathbb{E}[(T(y))_{i}] = 1 \cdot \phi_{i} + 0 \cdot (1-\phi ) = \phi_{i} $$</code></p><p><code>$$ h_{\theta }(x)= \begin{bmatrix} \phi_{1} \\ \phi_{2} \\ \vdots \\ \phi_{k-1} \end{bmatrix}= \begin{bmatrix} \frac{\exp ({\theta_{1}^{\mathbf{T}}x)}}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \\ \frac{\exp ({\theta_{2}^{\mathbf{T}}x})}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \\ \vdots \\ \frac{\exp ({\theta_{k-1}^{\mathbf{T}}x})}{\sum_{j=1}^{k} \exp(\theta_{j}^{\mathbf{T}}x)} \end{bmatrix} $$</code>
也就是说我们的假设函数需要输出每个类的概率，尽管只有<code>$k-1$</code>类，<code>$\phi_{k} = 1- \sum_{i}^{k-1} \phi_{i}$</code>得到</p><p>接下来进行最大似然估计并对<code>$\ell(\theta)$</code>进行化简：</p><p><code>$$ \begin{align} \ell(\theta ) & = \sum_{i} \log p(y^{(i)}|x^{(i)};\theta ) \\ &= \sum_{i} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} \\ &= \sum_{i}^{m} \sum_{l=1}^{k} \log \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} \\ &= \sum_{i}^{m} \sum_{l=1}^{k} {\color{red}\mathbb{1}\{y^{(i)}=l\}} \log \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \left( \log e^{\theta_{l}^{\mathbf{T}}x^{(i)}}- \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}} \right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \left(\theta_{l}^{\mathbf{T}}x^{(i)}- \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\right) \\ &= \sum_{i}^{m}\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \theta_{l}^{\mathbf{T}}x^{(i)}-\left( \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\underbrace{ \sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} }_{ 1 }\right) \\ &= \sum_{i}^{m} \bigg(\sum_{l=1}^{k} \mathbb{1}\{y^{(i)} = l\} \theta_{l}^{\mathbf{T}}x^{(i)} - \log \sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}\bigg) \end{align} $$</code></p><p>上述化简主要利用了指示函数的性质以及<code>$\log$</code>的运算法则，同时<code>$\theta \in \mathbb{R}^{k \times n}$</code>，我们利用布局法来求：</p><p><code>$$ \begin{align} \frac{ \partial \ell(\theta ) }{ \partial \theta_{pq} } & = \sum_{i}^{m} \mathbb{1}\{y^{(i)} = p\} x^{(i)}_{q} - \frac{1}{\sum_{j=1}^{k}e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} e^{\theta_{p}^{\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\ &= \sum_{i}^{m} \left( \mathbb{1}\{y^{(i)} = p\} - \frac{e^{\theta_{p}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)x^{(i)}_{q} \end{align} $$</code>
因为这里是最大化<code>$\ell(\theta)$</code>，作为损失函数还应加个负号，这样才是最小，即</p><p><code>$$ J(\theta ) = -\sum_{i} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} $$</code></p><p>对应的微分如下：
<code>$$ \frac{ \partial J(\theta ) }{ \partial \theta_{pq} } = \sum_{i}^{m} \left( \frac{e^{\theta_{p}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} - \mathbb{1}\{y^{(i)} = p\} \right)x^{(i)}_{q} $$</code></p><h3 id=交叉熵>交叉熵<a hidden class=anchor aria-hidden=true href=#交叉熵>#</a></h3><p>我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？</p><p>联想交叉熵的定义：</p><p><code>$$ H(P, Q) = - \mathbb{E}_{x \sim P}[\log Q(x)] $$</code>
即使得模型输出的分布尽可能靠近训练集原来的分布：</p><p><code>$$ \theta^{\ast} = \mathop{\arg \min}_{\theta} -\mathbb{E}_{x \sim \mathcal{D}}[\log p_{model}(x)] $$</code>
我们接下来展开期望的计算：</p><p><code>$$ -\mathbb{E}_{x \sim \mathcal{D}}[\log p_{model}(x)] = \frac{1}{m}\underbrace{ -\sum_{i}^{m} \log \prod_{l=1}^{k} \left( \frac{e^{\theta_{l}^{\mathbf{T}}x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{\mathbf{T}}x^{(i)}}} \right)^{\mathbb{1}\{y^{(i)}=l\}} }_{ J(\theta) } $$</code>
两者其实就差一个常数，本质是一样的</p><p>代码实现也比较轻松：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>CrossEntropy</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_true</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>y_pred</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>/=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[:,</span> <span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>take_along_axis</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_true</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>batch_size</span>
</span></span></code></pre></div></div><div style="margin-top:2em;padding:1em;border:0 solid;border-radius:10px;background-color:var(--code-bg)"><h3>如果您想要引用，请考虑如下格式：</h3><div style=padding-top:.5em>台运鹏. (Feb. 17, 2023). 《Generalized Linear Models》[Blog
post]. Retrieved from http://yunpengtai.top/posts/generalized-linear-models/</div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>@online{blog-a1d99d23743cdb5b4415e3d7166488d0,
</span></span><span class=line><span class=cl>        title={Generalized Linear Models},
</span></span><span class=line><span class=cl>        author={Yunpeng Tai},
</span></span><span class=line><span class=cl>        year={2023},
</span></span><span class=line><span class=cl>        month={Feb},
</span></span><span class=line><span class=cl>        note={http://yunpengtai.top/posts/generalized-linear-models/},
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div><div style=padding-bottom:.4em>自由转载-非商用-非衍生-保持署名（<a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-SA 4.0）</a></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://yunpengtai.top/tags/linear-models/>linear models</a></li></ul><nav class=paginav><a class=prev href=http://yunpengtai.top/posts/determinantal-point-process/><span class=title>« Prev</span><br><span>Determinantal Point Process</span></a>
<a class=next href=http://yunpengtai.top/posts/dive-in-distributed-training/><span class=title>Next »</span><br><span>Diving in distributed training in PyTorch</span></a></nav></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/artalk@2.8.6/dist/Artalk.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/@artalk/plugin-katex@0.2.4/dist/artalk-plugin-katex.min.js></script><div id=Comments></div><script>const savedTheme=localStorage.getItem("pref-theme");let darkMode="auto";savedTheme!==null&&(darkMode=savedTheme==="dark");const artalk=Artalk.init({el:"#Comments",pageKey:"",pageTitle:"Generalized Linear Models",server:"https://comment.yunpengtai.top",site:"Tai's Blog",darkMode,versionCheck:!1});document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?artalk.setDarkMode(!1):artalk.setDarkMode(!0)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=http://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.min.js",function(){pangu.spacingPage()})</script><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>