<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Tips for Training Neural Networks | Tai's Blog</title><meta name=keywords content="训练,神经网络"><meta name=description content="Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog&rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=http://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=http://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"all",packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Tips for Training Neural Networks"><meta property="og:description" content="Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog&rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,"><meta property="og:type" content="article"><meta property="og:url" content="http://yunpengtai.top/posts/tips-for-training-nn/"><meta property="og:image" content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-30T11:10:00+08:00"><meta property="article:modified_time" content="2022-07-30T11:10:00+08:00"><meta property="og:site_name" content="Tai's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Tips for Training Neural Networks"><meta name=twitter:description content="Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog&rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"http://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Tips for Training Neural Networks","item":"http://yunpengtai.top/posts/tips-for-training-nn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Tips for Training Neural Networks","name":"Tips for Training Neural Networks","description":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog\u0026rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,","keywords":["训练","神经网络"],"articleBody":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog’s interesting part.\nNowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others’ work (e.g. reproducing a BERT) because everything is there for you. However, when designing a NN or facing a new task, you are most probably trapped somewhere.\nAnd this blog is meant to guide you to handle new problems.\nLet’s first begin with some basic rules. Hope you guys enjoy it!\nRush into training neural networks leads to suffering. Training NN is not like writing the common code. For instance, if you plug in a int while it needs a string, errors just come out. However, writing the code about NN can not be so easy for it won’t show you bugs automatically (only if you make big mistakes). Sweating the details always pays off. Someone may say the details are infinite and can stop you from marching. Note that the details mentioned here are all necessary instead of some trivialities. And sweating the details can reduce your pain. Observation leads to intuition. Sadly, if you just keep thinking about something, inspiration will never come to you. For instance, if you want to upgrade the algorithm, you had better observe the data where the algorithm fails instead of just thinking about the algorithm. Trusting your intuition instead of your implementation. Sometimes when you come up with a new idea, the implementation of it may go wrong to some degree. When the result is opposite to your assumption, always check your code first before doubting your idea. Quicker, then better. When you are trying to test your hypothesis, use the most efficient way to verify it as fast as possible. Then let us go through concrete parts.\nFamiliar with Data At this stage, you need to do some basic analysis and data mining. Assume we have a classification dataset in NLP. There are several aspects to think about.\nThe Distribution. To begin with, you need to know the label distribution especially and visualize it. For instance, if you observe long-tailed distribution (e.g. the number of the instances for good emotion is 900 while for the bad emotion is 10), then some methods such as data augmentation or cost-sensitive loss functions can play their part. For your interest, you can refer to this up-to-date survey. Similarly, you may also need to know the distribution of the length of sequence. Thus you can set the appropriate maximum sequence length for your task. Moreover, you can also pass the original data through the feature extractor (such as BERT) to gain their representation. Then you can cluster them. Greed is good. I strongly suggest that you look through the whole dataset ambitiously just like the greedy algorithm. And I promise you are bound to find surprise. You can have a whole understanding of the domain of this dataset. And you can choose appropriate pre-trained models according to the domain. Also, remember to understand the labels. Once you understand the labels, you can see if the annotation is contradictory. And you can select certain samples to see the annotation and estimate how noisy the whole dataset is. You may also need to think for the machine. Are the sequences easy to understand? If they are easy to understand, then we do not need to apply very complicated models to tackle this problem. Is the local information more important than the global? Your understanding about the dataset can help you figure out some basic modeling problems and offer you intuition about rule-based methods. Simple quantification. You may need to know the size of the dataset. If the size is small, we can use the simple models such as textCNN or FastText instead of BERT-based models for the complicated models need more data to model the inductive bias. Also, you can write simple code for detecting the duplicate instances and instances which are corrupted (e.g. lack of the label). Stand on the shoulder of the model. When your model is trained on the dataset, you can see how it performs on the dev/eval set. You need to pay attention to those mis-predicted instances (i.e. bad cases) and think about why the prediction is wrong. Is the label wrong or the way of modeling weak to capture these information. Filter / Sort / Clean. You can decide whether to filter or clean the dataset based on your thorough observation. End-to-End Pipeline When you finish observing the dataset, you need to build the simple pipeline to ensure everything goes well.\nFix the random seed. When carrying out the experiments, you had better fix the seed to reduce the influences of randomness on the experiments. As simple as possible. When building the pipeline, you do not have to use very complicated modeling methods, etc. We are just testing. Thus make everything as simple as possible. For instance, you can use the simple classifier such as SVM and MLP (Multi-Layer Perceptron). Record the accuracy and loss. Training accuracy and loss are very useful for you to figure out which difference is beneficial. Also, we do not need complicated tools (e.g. Tensorboard and Wandb) to do so. You can use a list to store things you want and visualize it by matplotlib or write it down in a txt (Sometimes, the data on the terminal can disappear for certain reasons). Track the progress. In python, you can simply use the tqdm to track the progress. And you can also add the immediate accuracy and loss on the progress bar. Believe me, this can reduce your anxiety. Verify the init loss. For the multi-label classification problem, its loss should equal $-\\log (1/ \\text{num classes})$ (with a softmax). For instance, if you need to make the true prediction among 4 labels, the init loss should equal $1.386$. Good Initialization. For regression problems, if the average of your data is 6, then you can initialize the bias as 6 which can lead to fast and stable convergence. One more example, if you want to initialize the weights and you do not want the weights to be influenced by the output shape, then you may prefer Lecun Normal to Glorot Normal (all initialize with $\\text{N(0, scale)}$). Also, normal initialization is better than uniform initialization by experience. Last but not least, when facing an imblanced dataset with ratio 1:10 (positive cases V.S. negative cases), set the bias on the logits so that the model can learn the bias with the first few iterations. # fan_in, fan_out represent the input and output shape scale = 1. # lecun normal scale_lecun /= max(1., fan_in) scale_lecun = sqrt(scale_lecun) # glorot normal scale_glorot /= max(1., fan_in + fan_out) scale_glorot = sqrt(scale_glorot) Human Baseline. If the dataset is very particular and there are few related evaluation methods, you had better set the human baseline in sampled instances. Compared to the human baseline, you can have an idea that where your model has gone. Input-Independent Baseline. You can set the input all zeros and see the performance. And it should be worse than the performance of plugging in your data. Overfit a small batch. The model should overfit a batch of few instances (e.g. 10 or 20). Theoretically speaking, you should achieve the least loss. If the model fails to do so, then you can go and find the foxy bug. Visualize the input before going into the NN. Take Google’s code as example, it shows the input tensors when performing classification problems by BERT. This habit has saved me many times when coming up with a brand-new task because the preparation of data can be hard to some degree. Visualize the predictions dynamically over the course of training. By doing so, we can have a direct picture about where the model has gone and how it performs. Try Back-Propogation yourself. Gradients can give you information about what the model depends on to make such predictions. Generalize a special case. You should not write the general functions at the beginning because your thoughts can be easy to change, thus these general functions are fragile. You can generalize a special case when you are sure that it won’t change a lot. Overfitting Since we have built a pipeline and tested it, it’s time for us to make the model overfit the whole dataset.\nPicking the right model. The selection of models is related to the size of the dataset and complexity of the task. If your dataset is small, you can choose relatively big models to overfit. Borrow experience from the giants. Sometimes you are unfamiliar with the task, you may have no idea about which hyper-parameter to choose (e.g. learning rate). Then you can search some related papers and see the appendix for training details. Carry out many controlled experiments. Deep Learning is a experimental subject. Sometimes observation fails to give you idea about what exactly it will perform. For instance, if you want to know which learning rate is most suitable for this task, try more options to select the best. Remember change a variable once a time to reduce the influence of mixture. Turn to tricks. Tricks are infinite. For instance, you can apply adversarial training like FGM or PGD to improve the model’s performance. Also, if permitted, you can use random searching for the best parameters. Regularize More data is better. The most effective way to regularize the model is collecting more real-world data for training. After all, we are using the small set of data to approximate the distribution of the real-world. Data Augmentation. If you lack data, you can apply data augmentation to increase your data. Although this method seems very easy, it does demand your thorough understanding of your data and task. And creative methods can always pay off. For instance, in NLP, you can use back-translation to augment. Cross Validation. You can split the data several times. And use separate data to train some models. Then we can ensemble them to gain the final prediction. Others Always remember to record your results in a good order. For instance, you must record all the parameters and the model’s performances at the dev/eval set. You had better record the motivation for you trying out this experiment. Always back up your code and data. When you are trying some new methods, do not just try it on the original code. The same for the data. You need to back up the original pipeline and data for bad things happening. ","wordCount":"1798","inLanguage":"en","datePublished":"2022-07-30T11:10:00+08:00","dateModified":"2022-07-30T11:10:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://yunpengtai.top/posts/tips-for-training-nn/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"http://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://yunpengtai.top/archives/ title=归档><span>归档</span></a></li><li><a href=http://yunpengtai.top/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=http://yunpengtai.top/categories/%E6%8A%98%E8%85%BE title=折腾><span>折腾</span></a></li><li><a href=http://yunpengtai.top/tags/ title=标签><span>标签</span></a></li><li><a href=http://yunpengtai.top/friends/ title=友人><span>友人</span></a></li><li><a href=http://yunpengtai.top/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=http://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Tips for Training Neural Networks</h1><div class=post-meta><span title='2022-07-30 11:10:00 +0800 CST'>July 30, 2022</span>&nbsp;·&nbsp;1798 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#familiar-with-data aria-label="Familiar with Data">Familiar with Data</a></li><li><a href=#end-to-end-pipeline aria-label="End-to-End Pipeline">End-to-End Pipeline</a></li><li><a href=#overfitting aria-label=Overfitting>Overfitting</a></li><li><a href=#regularize aria-label=Regularize>Regularize</a></li><li><a href=#others aria-label=Others>Others</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Recently, I have read a <a href=http://karpathy.github.io/2019/04/25/recipe/>blog</a> about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own <code>experience</code> in this post along with <code>summarizing</code> that blog&rsquo;s interesting part.</p><p>Nowadays, it seems like that training NN is extremely easy for there are plenty of free <code>frameworks</code> which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are <code>copying</code> others&rsquo; work (e.g. reproducing a BERT) because everything is there for you. However, when designing a NN or facing a new task, you are most probably trapped somewhere.</p><p>And this blog is meant to guide you to handle new problems.</p><p>Let&rsquo;s first begin with some basic rules. Hope you guys enjoy it!</p><ol><li><b>Rush into training neural networks leads to suffering.</b> Training NN is not like writing the common code. For instance, if you plug in a int while it needs a string, errors just come out. However, writing the code about NN can not be so easy for it won&rsquo;t show you bugs <code>automatically</code> (only if you make big mistakes).</li><li><b>Sweating the details always pays off.</b> Someone may say the details are <code>infinite</code> and can stop you from marching. Note that the details mentioned here are all necessary instead of some <code>trivialities</code>. And sweating the details can reduce your pain.</li><li><b>Observation leads to intuition.</b> Sadly, if you just keep thinking about something, inspiration will never come to you. For instance, if you want to upgrade the algorithm, you had better observe the data where the algorithm fails instead of just thinking about the algorithm.</li><li><b>Trusting your intuition instead of your implementation.</b> Sometimes when you come up with a new idea, the implementation of it may go wrong to some degree. When the result is <code>opposite</code> to your <code>assumption</code>, always check your code first before doubting your idea.</li><li><b>Quicker, then better.</b> When you are trying to test your hypothesis, use the most <code>efficient</code> way to verify it as <code>fast</code> as possible.</li></ol><p>Then let us go through concrete parts.</p><h3 id=familiar-with-data>Familiar with Data<a hidden class=anchor aria-hidden=true href=#familiar-with-data>#</a></h3><p>At this stage, you need to do some basic analysis and data mining. Assume we have a <code>classification</code> dataset in NLP. There are several aspects to think about.</p><ul><li><b>The Distribution.</b> To begin with, you need to know the <code>label</code> distribution especially and <code>visualize</code> it. For instance, if you observe <code>long-tailed distribution</code> (e.g. the number of the instances for good emotion is 900 while for the bad emotion is 10), then some methods such as data augmentation or cost-sensitive loss functions can play their part. For your interest, you can refer to this up-to-date <a href=https://arxiv.org/pdf/2110.04596.pdf>survey</a>. Similarly, you may also need to know the distribution of the <code>length</code> of sequence. Thus you can set the appropriate maximum sequence length for your task. Moreover, you can also pass the original data through the <code>feature extractor</code> (such as BERT) to gain their representation. Then you can <code>cluster</code> them.</li><li><b>Greed is good.</b> I strongly suggest that you <code>look through</code> the whole dataset ambitiously just like the <code>greedy algorithm</code>. And I promise you are bound to find surprise. You can have a whole understanding of the <code>domain</code> of this dataset. And you can choose appropriate <code>pre-trained</code> models according to the domain. Also, remember to understand the <code>labels</code>. Once you understand the labels, you can see if the annotation is <code>contradictory</code>. And you can select certain samples to see the annotation and estimate how noisy the whole dataset is. You may also need to think for the <code>machine</code>. Are the sequences easy to understand? If they are easy to understand, then we do not need to apply very complicated models to tackle this problem. Is the <code>local</code> information more important than the <code>global</code>? Your understanding about the dataset can help you figure out some basic modeling problems and offer you intuition about <code>rule-based</code> methods.</li><li><b>Simple quantification.</b> You may need to know the <code>size</code> of the dataset. If the size is <code>small</code>, we can use the simple models such as textCNN or FastText instead of BERT-based models for the complicated models need more data to model the inductive bias. Also, you can write simple code for detecting the <code>duplicate</code> instances and instances which are <code>corrupted</code> (e.g. lack of the label).</li><li><b>Stand on the shoulder of the model.</b> When your model is trained on the dataset, you can see how it performs on the <code>dev/eval</code> set. You need to pay attention to those <code>mis-predicted</code> instances (i.e. bad cases) and think about why the prediction is wrong. Is the label wrong or the way of modeling weak to capture these information.</li><li><b>Filter / Sort / Clean</b>. You can decide whether to filter or clean the dataset based on your thorough observation.</li></ul><h3 id=end-to-end-pipeline>End-to-End Pipeline<a hidden class=anchor aria-hidden=true href=#end-to-end-pipeline>#</a></h3><p>When you finish observing the dataset, you need to build the simple pipeline to ensure everything goes well.</p><ul><li><b>Fix the random seed.</b> When carrying out the experiments, you had better fix the seed to reduce the influences of <code>randomness</code> on the experiments.</li><li><b>As simple as possible.</b> When building the pipeline, you do not have to use very complicated modeling methods, etc. We are just testing. Thus make everything as simple as possible. For instance, you can use the simple classifier such as SVM and MLP (Multi-Layer Perceptron).</li><li><b>Record the accuracy and loss.</b> Training accuracy and loss are very useful for you to figure out which <code>difference</code> is beneficial. Also, we do not need complicated tools (e.g. Tensorboard and Wandb) to do so. You can use a list to store things you want and visualize it by matplotlib or write it down in a <code>txt</code> (Sometimes, the data on the terminal can disappear for certain reasons).</li><li><b>Track the progress.</b> In python, you can simply use the <code>tqdm</code> to track the progress. And you can also add the immediate accuracy and loss on the progress bar. Believe me, this can reduce your anxiety.</li><li><b>Verify the init loss.</b> For the multi-label classification problem, its loss should equal <code>$-\log (1/ \text{num classes})$</code> (with a softmax). For instance, if you need to make the true prediction among 4 labels, the init loss should equal $1.386$.</li><li><b>Good Initialization.</b> For regression problems, if the <code>average</code> of your data is 6, then you can initialize the bias as 6 which can lead to <code>fast</code> and stable convergence. One more example, if you want to initialize the weights and you do not want the weights to be influenced by the <code>output shape</code>, then you may prefer Lecun Normal to Glorot Normal (all initialize with <code>$\text{N(0, scale)}$</code>). Also, <code>normal</code> initialization is better than uniform initialization by experience. Last but not least, when facing an <code>imblanced</code> dataset with ratio 1:10 (positive cases V.S. negative cases), set the bias on the <code>logits</code> so that the model can learn the bias with the first few iterations.<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># fan_in, fan_out represent the input and output shape</span>
</span></span><span class=line><span class=cl><span class=n>scale</span> <span class=o>=</span> <span class=mf>1.</span>
</span></span><span class=line><span class=cl><span class=c1># lecun normal</span>
</span></span><span class=line><span class=cl><span class=n>scale_lecun</span> <span class=o>/=</span> <span class=nb>max</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>fan_in</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>scale_lecun</span> <span class=o>=</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>scale_lecun</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># glorot normal</span>
</span></span><span class=line><span class=cl><span class=n>scale_glorot</span> <span class=o>/=</span> <span class=nb>max</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>fan_in</span> <span class=o>+</span> <span class=n>fan_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>scale_glorot</span> <span class=o>=</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>scale_glorot</span><span class=p>)</span>
</span></span></code></pre></div></li><li><b>Human Baseline.</b> If the dataset is very particular and there are few related evaluation methods, you had better set the <code>human baseline</code> in sampled instances. Compared to the human baseline, you can have an idea that where your model has gone.</li><li><b>Input-Independent Baseline.</b> You can set the input all <code>zeros</code> and see the performance. And it should be <code>worse</code> than the performance of plugging in your data.</li><li><b>Overfit a small batch.</b> The model should <code>overfit</code> a batch of few instances (e.g. 10 or 20). Theoretically speaking, you should achieve the least loss. If the model fails to do so, then you can go and find the foxy bug.</li><li><b>Visualize the input before going into the NN.</b> Take Google&rsquo;s <a href=https://github.com/google-research/bert>code</a> as example, it shows the input tensors when performing classification problems by BERT. This habit has saved me many times when coming up with a brand-new task because the preparation of data can be hard to some degree.</li><li><b>Visualize the predictions dynamically over the course of training.</b> By doing so, we can have a direct picture about where the model has gone and how it performs.</li><li><b>Try Back-Propogation yourself.</b> <code>Gradients</code> can give you information about what the model depends on to make such predictions.</li><li><b>Generalize a special case.</b> You should not write the general functions at the beginning because your thoughts can be easy to change, thus these general functions are fragile. You can generalize a special case when you are sure that it won&rsquo;t change a lot.</li></ul><h3 id=overfitting>Overfitting<a hidden class=anchor aria-hidden=true href=#overfitting>#</a></h3><p>Since we have built a pipeline and tested it, it&rsquo;s time for us to make the model overfit the whole dataset.</p><ul><li><b>Picking the right model.</b> The selection of models is related to the size of the dataset and complexity of the task. If your dataset is small, you can choose relatively big models to overfit.</li><li><b>Borrow experience from the giants.</b> Sometimes you are unfamiliar with the task, you may have no idea about which <code>hyper-parameter</code> to choose (e.g. learning rate). Then you can search some related papers and see the <code>appendix</code> for training details.</li><li><b>Carry out many controlled experiments.</b> Deep Learning is a <code>experimental subject</code>. Sometimes observation fails to give you idea about what exactly it will perform. For instance, if you want to know which learning rate is most suitable for this task, try more options to select the best. Remember change a variable <code>once a time</code> to reduce the influence of mixture.</li><li><b>Turn to tricks.</b> Tricks are infinite. For instance, you can apply adversarial training like FGM or PGD to improve the model&rsquo;s performance. Also, if permitted, you can use <code>random</code> searching for the best parameters.</li></ul><h3 id=regularize>Regularize<a hidden class=anchor aria-hidden=true href=#regularize>#</a></h3><ul><li><b>More data is better.</b> The most effective way to regularize the model is collecting more real-world data for training. After all, we are using the <code>small</code> set of data to <code>approximate</code> the distribution of the real-world.</li><li><b>Data Augmentation.</b> If you <code>lack</code> data, you can apply data augmentation to increase your data. Although this method seems very easy, it does <code>demand</code> your thorough understanding of your data and task. And <code>creative</code> methods can always pay off. For instance, in NLP, you can use back-translation to augment.</li><li><b>Cross Validation.</b> You can split the data several times. And use separate data to train some models. Then we can ensemble them to gain the final prediction.</li></ul><h3 id=others>Others<a hidden class=anchor aria-hidden=true href=#others>#</a></h3><ul><li><b>Always remember to record your results in a good order.</b> For instance, you must record all the parameters and the model&rsquo;s performances at the dev/eval set. You had better record the <code>motivation</code> for you trying out this experiment.</li><li><b>Always back up your code and data.</b> When you are trying some new methods, do not just try it on the original code. The same for the data. You need to back up the original pipeline and data for bad things happening.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://yunpengtai.top/tags/%E8%AE%AD%E7%BB%83/>训练</a></li><li><a href=http://yunpengtai.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li></ul><nav class=paginav><a class=prev href=http://yunpengtai.top/posts/deep-back-propagation/><span class=title>« Prev</span><br><span>Going Deeper into Back-Propagation</span></a>
<a class=next href=http://yunpengtai.top/posts/quotes/><span class=title>Next »</span><br><span>Quotes of Mathematicians</span></a></nav></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/artalk@2.8.6/dist/Artalk.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/@artalk/plugin-katex@0.2.4/dist/artalk-plugin-katex.min.js></script><div id=Comments></div><script>const savedTheme=localStorage.getItem("pref-theme");let darkMode="auto";savedTheme!==null&&(darkMode=savedTheme==="dark");const artalk=Artalk.init({el:"#Comments",pageKey:"",pageTitle:"Tips for Training Neural Networks",server:"https://comment.yunpengtai.top",site:"Tai's Blog",darkMode,versionCheck:!1});document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?artalk.setDarkMode(!1):artalk.setDarkMode(!0)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=http://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.min.js",function(){pangu.spacingPage()})</script><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>