<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>NCE的朋友们 | Tai's Blog</title><meta name=keywords content="contrastive learning"><meta name=description content="在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\boldsymbo"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=https://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{tags:"all",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="NCE的朋友们"><meta property="og:description" content="在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\boldsymbo"><meta property="og:type" content="article"><meta property="og:url" content="https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-08T21:52:00+08:00"><meta property="article:modified_time" content="2023-07-08T21:52:00+08:00"><meta property="og:site_name" content="Tai's Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"NCE的朋友们","item":"https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NCE的朋友们","name":"NCE的朋友们","description":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbo","keywords":["contrastive learning"],"articleBody":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbol{c}$，要去词表$\\mathcal{V}$挑选$\\boldsymbol{w}$来生成：\nNegative Sampling 其实要说 NCE 的缺点，就是需要花时间调参数，比如说$k$和噪声分布$p_{n}$的选择，而负采样则固定下来这两个参数，不同的负采样对于这两个参数的选择也不尽相同，这里介绍比较简单的一种\n我们规定，当$\\mathcal{D}=1$时代表从训练集采样，而$\\mathcal{D}=0$则代表从噪声中采样，为了表达简便，对$\\boldsymbol{c}$进行省略，即$p(\\mathcal{D}=1|\\boldsymbol{c}, \\boldsymbol{w}) = p(\\mathcal{D}=1|\\boldsymbol{w})$\nNCE 会这样求样本来自哪个采样的概率：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026 = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026 = \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n而负采样说，如果词表大小是$|\\mathcal{V}|$，不妨就采样$|\\mathcal{V}|$个噪声，而且每个噪声是等概率出现的，那么：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026 = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026 = \\frac{1}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\end{align} $$\n综合上面的式子来看，其实负采样是 NCE 的一种\nImportance Sampling 我们记需要计算的分母（配分函数）为$Z(\\boldsymbol{\\theta })$，注意：$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})=\\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}))$，$s_{\\boldsymbol{\\theta}}$为打分函数\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\underbrace{ \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) }_{ Z(\\boldsymbol{\\theta }) }} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} $$\n那么：\n$$ \\begin{align} Z(\\boldsymbol{\\theta }) \u0026 = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) {\\color{blue}\\frac{p_{n}(\\boldsymbol{w}')}{p_{n}(\\boldsymbol{w}')}} \\\\ \u0026= \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} p_{n}(\\boldsymbol{w}') \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\\\ \u0026= \\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\end{align} $$\n跟 NCE 不同的是，重要性采样并没有把配分函数当成参数，而是利用噪声数据去拟合它：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})/p_{n}(\\boldsymbol{w}')} $$\n同样地操作，我们可以采样$k$个噪声样本来去近似期望，即：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) \\approx \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}p_{n}(\\boldsymbol{w}_{i})u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{i}, \\boldsymbol{c})/p_{n}(\\boldsymbol{w}_{i})} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w_{i}}, \\boldsymbol{c})} $$\n仔细看，其实重要性采样是利用蒙特卡洛采样将分母从一开始的$|\\mathcal{V}|$项降低到$k$项\n可能读者还比较熟悉 InfoNCE，但是它的思想不是为了去「近似配分」，这里就不放在一起展开了\n","wordCount":"866","inLanguage":"en","datePublished":"2023-07-08T21:52:00+08:00","dateModified":"2023-07-08T21:52:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"https://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://yunpengtai.top/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yunpengtai.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yunpengtai.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yunpengtai.top/friends/ title=Friends><span>Friends</span></a></li><li><a href=https://yunpengtai.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=https://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>NCE的朋友们</h1><div class=post-meta><span title='2023-07-08 21:52:00 +0800 CST'>July 8, 2023</span>&nbsp;·&nbsp;866 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#negative-sampling aria-label="Negative Sampling">Negative Sampling</a></li><li><a href=#importance-sampling aria-label="Importance Sampling">Importance Sampling</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>在<a href=https://yunpengtai.top/posts/noise-contrastive-estimation/>Noise Contrastive Estimation</a>中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文<code>$\boldsymbol{c}$</code>，要去词表<code>$\mathcal{V}$</code>挑选<code>$\boldsymbol{w}$</code>来生成：</p><h3 id=negative-sampling>Negative Sampling<a hidden class=anchor aria-hidden=true href=#negative-sampling>#</a></h3><p>其实要说 NCE 的缺点，就是需要花时间调参数，比如说<code>$k$</code>和噪声分布<code>$p_{n}$</code>的选择，而负采样则固定下来这两个参数，不同的负采样对于这两个参数的选择也不尽相同，这里介绍比较简单的一种</p><p>我们规定，当<code>$\mathcal{D}=1$</code>时代表从训练集采样，而<code>$\mathcal{D}=0$</code>则代表从噪声中采样，为了表达简便，对<code>$\boldsymbol{c}$</code>进行省略，即<code>$p(\mathcal{D}=1|\boldsymbol{c}, \boldsymbol{w}) = p(\mathcal{D}=1|\boldsymbol{w})$</code></p><p>NCE 会这样求样本来自哪个采样的概率：</p><p><code>$$ \begin{align} p(\mathcal{D}=1|\boldsymbol{w}) & = \frac{p_{\boldsymbol{\theta }}(\boldsymbol{w})}{p_{\boldsymbol{\theta }}(\boldsymbol{w})+kp_{n}(\boldsymbol{w})} \\ p(\mathcal{D}=0|\boldsymbol{w}) & = \frac{kp_{n}(\boldsymbol{w})}{p_{\boldsymbol{\theta }}(\boldsymbol{w})+kp_{n}(\boldsymbol{w})} \end{align} $$</code></p><p>而负采样说，如果词表大小是<code>$|\mathcal{V}|$</code>，不妨就采样<code>$|\mathcal{V}|$</code>个噪声，而且每个噪声是等概率出现的，那么：</p><p><code>$$ \begin{align} p(\mathcal{D}=1|\boldsymbol{w}) & = \frac{p_{\boldsymbol{\theta }}(\boldsymbol{w})}{p_{\boldsymbol{\theta }}(\boldsymbol{w})+1} \\ p(\mathcal{D}=0|\boldsymbol{w}) & = \frac{1}{p_{\boldsymbol{\theta }}(\boldsymbol{w})+1} \end{align} $$</code></p><p>综合上面的式子来看，其实负采样是 NCE 的一种</p><h3 id=importance-sampling>Importance Sampling<a hidden class=anchor aria-hidden=true href=#importance-sampling>#</a></h3><p>我们记需要计算的分母（配分函数）为<code>$Z(\boldsymbol{\theta })$</code>，注意：<code>$u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})=\exp(s_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c}))$</code>，<code>$s_{\boldsymbol{\theta}}$</code>为打分函数</p><p><code>$$ p_{\boldsymbol{\theta }}(\boldsymbol{w}|\boldsymbol{c}) = \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{\underbrace{ \sum_{\boldsymbol{w}' \in \mathcal{V}}u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c}) }_{ Z(\boldsymbol{\theta }) }} = \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{Z(\boldsymbol{\theta })} $$</code></p><p>那么：</p><p><code>$$ \begin{align} Z(\boldsymbol{\theta }) & = \sum_{\boldsymbol{w}' \in \mathcal{V}} u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c}) = \sum_{\boldsymbol{w}' \in \mathcal{V}} u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c}) {\color{blue}\frac{p_{n}(\boldsymbol{w}')}{p_{n}(\boldsymbol{w}')}} \\ &= \sum_{\boldsymbol{w}' \in \mathcal{V}} p_{n}(\boldsymbol{w}') \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c})}{p_{n}(\boldsymbol{w}')} \\ &= \mathbb{E}_{\boldsymbol{w}' \sim p_{n}} \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c})}{p_{n}(\boldsymbol{w}')} \end{align} $$</code></p><p>跟 NCE 不同的是，重要性采样并没有把配分函数当成参数，而是利用噪声数据去拟合它：</p><p><code>$$ p_{\boldsymbol{\theta }}(\boldsymbol{w}|\boldsymbol{c}) = \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{Z(\boldsymbol{\theta })} = \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{\mathbb{E}_{\boldsymbol{w}' \sim p_{n}} u_{\boldsymbol{\theta }}(\boldsymbol{w}', \boldsymbol{c})/p_{n}(\boldsymbol{w}')} $$</code></p><p>同样地操作，我们可以采样<code>$k$</code>个噪声样本来去近似期望，即：</p><p><code>$$ p_{\boldsymbol{\theta }}(\boldsymbol{w}|\boldsymbol{c}) \approx \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{\sum_{i=1}^{k}p_{n}(\boldsymbol{w}_{i})u_{\boldsymbol{\theta }}(\boldsymbol{w}_{i}, \boldsymbol{c})/p_{n}(\boldsymbol{w}_{i})} = \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{\sum_{i=1}^{k}u_{\boldsymbol{\theta }}(\boldsymbol{w_{i}}, \boldsymbol{c})} $$</code></p><p>仔细看，其实重要性采样是利用蒙特卡洛采样将分母从一开始的<code>$|\mathcal{V}|$</code>项降低到<code>$k$</code>项</p><p>可能读者还比较熟悉 InfoNCE，但是它的思想不是为了去「近似配分」，这里就不放在一起展开了</p></div><blockquote class=quote-copyright>Author: Yunpengtai<p>Link: https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/<p>License: CC BY-NC-SA 4.0. You must provide a link to the source.</blockquote><footer class=post-footer><ul class=post-tags><li><a href=https://yunpengtai.top/tags/contrastive-learning/>contrastive learning</a></li></ul><nav class=paginav><a class=prev href=https://yunpengtai.top/posts/infonce/><span class=title>« Prev</span><br><span>放大镜下的 InfoNCE</span></a>
<a class=next href=https://yunpengtai.top/posts/numerical-stability/><span class=title>Next »</span><br><span>Numerical Stability</span></a></nav></footer><link rel=stylesheet href=https://unpkg.com/katex@0.15.3/dist/katex.min.css><script defer src=https://unpkg.com/katex@0.15.3/dist/katex.min.js></script>
<link href=https://cdnjs.cloudflare.com/ajax/libs/artalk/2.8.6/Artalk.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/artalk/2.8.6/Artalk.js></script>
<script src=https://unpkg.com/@artalk/plugin-katex/dist/artalk-plugin-katex.js></script><div id=Comments></div><script>Artalk.init({el:"#Comments",pageKey:"",pageTitle:"NCE的朋友们",server:"https://comment.yunpengtai.top",site:"Tai's Blog"})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("//cdn.bootcss.com/pangu/4.0.7/pangu.min.js",function(){pangu.spacingPage()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>