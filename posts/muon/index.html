<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Muon: 控制谱范数下的最速下降 | Tai's Blog</title><meta name=keywords content="llms,training,optimizer,muon,adam"><meta name=description content="本文将主要涵盖以下内容： 从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=http://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=http://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"all",packages:{"[+]":["boldsymbol"]}},chtml:{scale:.9}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Muon: 控制谱范数下的最速下降"><meta property="og:description" content="本文将主要涵盖以下内容： 从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导"><meta property="og:type" content="article"><meta property="og:url" content="http://yunpengtai.top/posts/muon/"><meta property="og:image" content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-03T22:30:00+08:00"><meta property="article:modified_time" content="2025-06-03T22:30:00+08:00"><meta property="og:site_name" content="Tai's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Muon: 控制谱范数下的最速下降"><meta name=twitter:description content="本文将主要涵盖以下内容： 从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"http://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Muon: 控制谱范数下的最速下降","item":"http://yunpengtai.top/posts/muon/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Muon: 控制谱范数下的最速下降","name":"Muon: 控制谱范数下的最速下降","description":"本文将主要涵盖以下内容： 从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导","keywords":["llms","training","optimizer","muon","adam"],"articleBody":"本文将主要涵盖以下内容：\n从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导的过程跟真正实现上有差异，比如实际是对动量进行正交化，而不是对梯度，但读者无须担心，本文最后还是会回归到具体的实现 介绍 Kimi 团队 https://github.com/MoonshotAI/Moonlight 在 Muon 基础上的改进和代码实现，主要是 weight decay 以及对齐更新量的 RMSNorm 两个方面 逐一实现上面提及的 Muon，然后与原始的 Adam 做对照实验进行验证 最后对 Muon 进行 FLOPS 分析 推导 Muon 度量线性层 给定输入 $\\boldsymbol{x} \\in\\mathbb{R}^{n}$，权重矩阵 $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$，过一层「线性层」（Linear Layers），即 $\\boldsymbol{y} = \\mathbf{W}\\boldsymbol{x}$（这里对 bias 进行忽略）。那么有个有趣的问题，$\\mathbf{W}$ 究竟对输入做了什么？或者如何度量这种线性运算呢？\n此时可以联系一下「算子范数」（Operator Norm）的定义：给定任意两种 norm 方式 $\\| \\cdot\\|_{\\text{F}}$ 和 $\\|\\cdot\\|_{\\text{E}}$，对于任意的 $\\boldsymbol{x}$，算子范数是 $\\mathbf{W}$ 能对 $\\boldsymbol{x}$ 进行的最大拉伸量：\n$$ \\|\\mathbf{W}\\|_{\\text{op}} := \\max_{\\boldsymbol{x}\\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{E}}} $$\n接着让我们看看两种 norm 方式 $\\|\\cdot\\|_{\\text{F}}, \\|\\cdot\\|_{\\text{E}}$ 均为 RMSNorm 时会发生什么，先回顾下 RMSNorm 的定义：\n$$ \\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\sqrt{ \\frac{1}{n} \\sum_{i} \\boldsymbol{x}_{i}^{2}} = \\sqrt{ \\frac{1}{n} } \\|\\boldsymbol{x}\\|_{2} $$\n那么：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} := \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}}}{ \\|\\boldsymbol{x}\\|_{\\text{RMS}}} = \\sqrt{ \\frac{n}{m} }\\underbrace{ {\\color{#08F} \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}}} }_{\\text{L2 operator norm} } \\tag{\\#1} $$\n可以发现 RMSNorm 算子范数是一种归一化后的 L2 算子范数，那么 L2 算子范数究竟是什么呢？我们接着推导：\n首先对 $\\mathbf{W}$ 进行「奇异值分解」（SVD），即 $\\mathbf{W}=\\mathbf{U\\Sigma V^{\\top}}$，其中 $\\mathbf{U}, \\mathbf{V}$ 都是正交矩阵，而 $\\mathbf{\\Sigma}$ 是对角矩阵，对角线的元素为奇异值，不妨设 $\\sigma_{1}\\geq\\sigma_{2}\\geq \\dots \\geq\\sigma_{r}, \\, r=\\min(m,n)$\n先说明一个重要的性质，即「正交变换之后不改变 L2 范数的大小」，证明如下：因为 $\\mathbf{V}$ 是正交矩阵，所以 $\\mathbf{V}^{\\top}\\mathbf{V} = \\mathbf{I}_{n}$\n$$ \\|\\mathbf{V}\\boldsymbol{x}\\|_{2}^{2} = (\\mathbf{V}\\boldsymbol{x})^{\\top}\\mathbf{V}\\boldsymbol{x} = \\boldsymbol{x}^{\\top}\\underbrace{ \\mathbf{V}^{\\top}\\mathbf{V} }_{ \\mathbf{I}_{n} }\\boldsymbol{x} = \\boldsymbol{x}^{\\top}\\boldsymbol{x} = \\|\\boldsymbol{x}\\|_{2}^{2} $$\n接着开始正式推导 L2 算子范数，不妨记 $\\mathbf{V}^{\\top}\\boldsymbol{x} = \\boldsymbol{y}$，蓝色部分的变换都用到了刚刚提及的「正交变换之后不改变 L2 范数的大小」的性质\n$$ \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{U\\Sigma V^{\\top}}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{U\\Sigma}\\boldsymbol{y}\\|_{2}}{{\\color{#08F}\\|\\boldsymbol{y}\\|_{2}}} = \\frac{{\\color{#08F}\\|\\Sigma \\boldsymbol{y}\\|_{2}}}{\\|\\boldsymbol{y}\\|_{2}} $$\n那么：\n$$ \\max_{\\boldsymbol{x}\\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} \\implies \\max \\frac{\\|\\Sigma \\boldsymbol{y}\\|_{2}^{2}}{\\|\\boldsymbol{y}\\|_{2}^{2}}= \\frac{\\sum_{i}\\sigma_{i}^{2}y_{i}^{2}}{\\sum_{i}y_{i}^{2}}\\leq \\frac{\\sigma_{1}^{2}\\sum_{i}y_{i}^{2}}{\\sum_{i}y_{i}^{2}} = \\sigma_{1}^{2} $$\n即：\n$$ \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\sigma_{1} = \\sigma_{\\text{max}} = \\underbrace{ \\|\\mathbf{W}\\|_{2} }_{ \\text{Spectral Norm} } $$\n「谱范数」（Spectral Norm）指的是矩阵的最大奇异值，那么联系式 $\\#1$ 可得：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} = \\sqrt{ \\frac{n}{m} }{ \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}}} = \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2} \\tag{\\#2} $$\nRMSNorm 的算子范数是一种归一化的谱范数\n输出变化量 在训练神经网络时，我们想知道，当权重矩阵更新后，输出会多大程度随之变化（当然这里 $\\boldsymbol{x}$ 也会发生变化，会在后文论述），即：\n$$ \\Delta \\boldsymbol{y} = (\\mathbf{W}+\\Delta \\mathbf{W})\\boldsymbol{x} - \\mathbf{W}\\boldsymbol{x} = \\Delta \\mathbf{W}\\boldsymbol{x} $$\n联系式 $\\#1$ 的定义，可知：\n$$ \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} = \\|\\Delta \\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} \\leq \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\cdot\\|\\boldsymbol{x}\\|_{\\text{RMS}} \\tag{\\#3} $$\n换言之，当权重矩阵更新后，我们找到了输出变化量 RMSNorm 的最大值，这里利用算子范数，巧妙地将矩阵的更新和输出的更新联系了起来\n对偶化梯度 通过「泰勒展开式」（Taylor Expansion），可知：\n$$ \\mathcal{L}(\\mathbf{W} + \\Delta \\mathbf{W}) = \\mathcal{L}(\\mathbf{W})+ \\langle \\nabla_{\\mathbf{W}} \\mathcal{L}, \\Delta \\mathbf{W} \\rangle + \\text{High Order Terms} $$\n由于变化量比较小，不妨对高次项进行省略，即「线性近似」：\n$$ \\Delta \\mathcal{L} = \\mathcal{L}(\\mathbf{W}+\\Delta \\mathbf{W}) - \\mathcal{L}(\\mathbf{W}) \\approx \\langle\\nabla_{\\mathbf{W}}\\mathcal{L}, \\Delta \\mathbf{W}\\rangle $$\n我们肯定想要 loss 变化量越小越好，同时变化的过程中，我们并不希望输出变化量太大，否则就会破坏前面泰勒展开的前提；同时如果输出变化量太大，会让训练过程不稳定。那么我们就对输出变化量加个 bound，即：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} =\\mathcal{O}(1) $$\n笔者以为「对输出变化量进行约束」即为 Muon 的核心 motivation，将式 $\\#3$ 代入，则：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\, \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\cdot\\|\\boldsymbol{x}\\|_{\\text{RMS}} =\\mathcal{O}(1) $$\n假设 $\\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\mathcal{O}(1)$，则：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\, \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} =\\mathcal{O}(1) \\tag{\\#4} $$\n式 $\\#4$ 即被称为「对偶化梯度」（Dualizing the Gradients），那么这个如何解？\n可以先展开一下约束，使其与谱范数联系起来：\n$$ \\begin{align} \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} = \\sqrt{ \\frac{n}{m} } \\|\\Delta\\mathbf{W}\\|_{2} =\\mathcal{O}(1) \\\\ \\implies \\|\\Delta\\mathbf{W}\\|_{2} =\\mathcal{O}\\left(\\sqrt{\\frac{m}{n}}\\right) \\end{align} $$\n求解约束 接着我们对梯度进行奇异值分解： $\\nabla_{\\mathbf{W}}\\mathcal{L}=\\mathbf{U\\Sigma V^{\\top}}$，这里是「经济型分解」，即 $\\mathbf{U}\\in\\mathbb{R}^{m\\times r}, \\mathbf{V}\\in\\mathbb{R}^{n\\times r}, \\mathbf{\\Sigma} \\in\\mathbb{R}^{r \\times r}$，其中 $r=\\text{rank}(\\nabla_{\\mathbf{W}}\\mathcal{L})\\leq \\min(m, n)$。假设没有约束的情况下，我们想要去 $\\min \\langle \\nabla_{\\mathbf{W}}\\mathcal{L}, \\Delta \\mathbf{W}\\rangle$，我们会直接按照梯度的反方向，即\n$$ \\Delta \\mathbf{W} = - \\nabla_{\\mathbf{W}}\\mathcal{L} $$\n但目前有个约束条件，我们可以对 $\\Delta \\mathbf{W}$ 进行「正交化」（Orthogonalization）， $c \\in\\mathbb{R}$ 是一个系数：\n$$ \\Delta \\mathbf{W} = -c \\cdot\\mathbf{UV}^{\\top} $$\n之所以写成上述形式，主要是因为约束是有关谱范数的，而 $\\|\\mathbf{UV^{\\top}}\\|_{2}=1$，下面来证明这一性质。首先，对于一个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 来说，要求其谱范数，按照如下流程：\n$$ \\|\\mathbf{A}\\|_{2} = \\sqrt{ \\lambda_{\\max}(\\mathbf{A^{\\ast}}\\mathbf{A}) } = \\sigma_{\\max}(\\mathbf{A}) $$\n其中 $\\mathbf{A}^{*}$ 是矩阵 $\\mathbf{A}$ 的「共轭转置」（Conjugate Transpose），对于「实数域」的矩阵来说，共轭等于其自身，故而 $\\mathbf{A}^{*}= \\mathbf{A}^{\\top}$，所以就可以求 $\\|\\mathbf{UV}^{\\top}\\|_{2}$\n$$ (\\mathbf{UV}^{\\top})^{\\top}\\mathbf{UV^{\\top}} = \\mathbf{VU}^{\\top}\\mathbf{UV^{\\top}} = \\mathbf{VV^{\\top}} = \\mathbf{I}_{r}\\implies \\|\\mathbf{UV}^{\\top}\\|_{2} = \\sqrt{ \\lambda_{\\max} } = 1 $$\n接着来计算一下权重更新量的谱范数：\n$$ \\|\\Delta\\mathbf{W}\\|_{2} = \\|-c\\cdot \\mathbf{UV^{\\top}}\\|_{2} = |c| \\cdot \\|\\mathbf{UV^{\\top}}\\|_{2} = |c| $$\n那么：\n$$ \\|\\Delta \\mathbf{W\\|_{2}}=\\mathcal{O} \\left(\\sqrt{ \\frac{m}{n} } \\right)\\implies c =\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n可得：\n$$ \\Delta \\mathbf{W} = - \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} } \\right)\\mathbf{UV^{\\top}} \\tag{\\#5} $$\n即式 $\\#5$ 就是对偶化梯度的解，小结一下，我们将对输出变化量的约束转为对权重变化量的谱范数要求，最后通过「正交化」来求解\n求正交化 那么如何求正交化呢？正交化即让「奇异值全变为 $1$」，而奇异值是正数，就相当于对其使用了 sign 函数\n$$ \\nabla_{\\mathbf{W}}\\mathcal{L} = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{UV^{\\top}} $$\n但是要分解完 SVD，再对 $\\Sigma$ 使用 sign，这个代价有点大，有没有直接在 $\\nabla_{\\mathbf{W}}\\mathcal{L}$ 上作用的办法呢？有一条推论可以借助：「奇次矩阵多项式（Odd Matrix Polynomials）和 SVD 存在可交换性」，即我们对 $\\nabla_{\\mathbf{W}}\\mathcal{L}$ 的操作等于对 $\\Sigma$ 进行操作：\n$$ p(\\nabla_{\\mathbf{W}}\\mathcal{L}) = p(\\mathbf{U\\Sigma V^{\\top}}) = \\mathbf{U}p(\\Sigma)\\mathbf{V^{\\top}} $$\n但问题是 sign 并非是矩阵多项式的形式，奇次矩阵多项式一般长这样：\n$$ p(\\mathbf{X}):= a\\cdot \\mathbf{X} + b \\cdot \\mathbf{XX^{\\top}X} + c\\cdot \\mathbf{XX^{\\top}XX^{\\top}X} + \\dots $$\n此时，就可以用一种 sign 近似 https://epubs.siam.org/doi/10.1137/0707031 来完成，即：\n$$ p(\\Sigma) := \\frac{3}{2}\\Sigma - \\frac{1}{2}\\Sigma\\Sigma^{\\top}\\Sigma $$\n可以用单变量来可视化看一下：\n$$ p(x) = \\frac{3}{2} x - \\frac{1}{2} x^{3}; \\quad p_{n}(x)= \\underbrace{ p \\circ p \\circ \\dots \\circ p }_{ n }(x) $$\n当 $n=14$ 时，可以发现，在 $[0, \\sqrt{ 3 }]$ 范围内可以做到很好地近似 sign，即结果为 $1$\n而上述过程被称为「Newton-Schulz Iteration」，因为我们的目的是想让 $\\mathbf{\\Sigma}$ 变成单位矩阵，所以也可称为矩阵的「零次幂」（Zero Power）求解过程\n让我们简要总结一下推导出的 Muon 公式，其中 $\\eta$ 是学习率\n$$ \\mathbf{W}_{} \\gets \\mathbf{W} - \\eta \\cdot\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right)\\text{NewtonSchulz}(\\nabla_{\\mathbf{W}}\\mathcal{L}), \\mathbf{W} \\in\\mathbb{R}^{m \\times n} $$\n更新规则 实际上 Muon 的更新规则如下：我们并非是对梯度做正交化，而是对 Nesterov Style 的动量做，然后系数是 $1, \\sqrt{ m/n }$ 的最大值\n$$ \\begin{align} \\mathbf{M}_{t} \u0026 = \\beta \\, \\mathbf{M}_{t-1} + (1-\\beta) \\nabla_{\\mathbf{W}}\\mathcal{L}\\\\ \\mathbf{O}_{t} \u0026 = \\text{NewtonSchulz}(\\beta \\,\\mathbf{M}_{t} + (1-\\beta)\\nabla_{\\mathbf{W}}\\mathcal{L})\\\\ \\mathbf{W}_{t} \u0026 = \\mathbf{W}_{t-1} - \\eta \\cdot {\\color{#08F}\\max \\left( 1, \\sqrt{ \\frac{m}{n} } \\right)} \\mathbf{O}_{t} \\end{align} $$\n与谱条件的联系 先联系上述式 $\\#1$ 和式 $\\#2$：\n$$ \\begin{align} \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} \u0026 := \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}}}{ \\|\\boldsymbol{x}\\|_{\\text{RMS}}} \\\\ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} \u0026 = \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2} \\end{align} $$\n那么可以导出：\n$$ \\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} \\leq \\|\\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\|\\boldsymbol{x}\\|_{\\text{RMS}}=\\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2}\\|\\boldsymbol{x}\\|_{\\text{RMS}} \\tag{\\#6} $$\n如果想要控制输出的 RMSNorm，即 $\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} = \\mathcal{O}(1)$，假设 $\\|\\boldsymbol{x}\\|_{\\text{RMS}}=\\mathcal{O}(1)$，则：\n$$ \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2}\\|\\boldsymbol{x}\\|_{\\text{RMS}} =\\mathcal{O}(1) \\implies \\|\\mathbf{W}\\|_{2} =\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n上面推导 Muon 是约束输出变化量来导出对权重的谱范数进行约束，但严格来说，关于输出变化量的推导是不严谨的，这也是 Muon 和谱条件 https://arxiv.org/abs/2310.17813 （Spectral Condition）的不同之处，因为当权重变了之后，输入也会随之改变，比如第二层的输入其实是第一层的输出，这里推导借鉴了苏老师关于谱条件的介绍 https://kexue.fm/archives/10795 ，记 $\\boldsymbol{x}_{k}$ 为第 $k$ 层的输出\n$$ \\begin{align} \\Delta \\boldsymbol{x}_{k} \u0026 = (\\boldsymbol{x}_{k-1}+\\Delta \\boldsymbol{x}_{k-1})(\\mathbf{W}_{k}+\\Delta \\mathbf{W}_{k}) - \\boldsymbol{x}_{k-1}\\mathbf{W}_{k} \\\\[5pt] \u0026= \\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k}) + (\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k} + (\\Delta \\boldsymbol{x}_{k-1})(\\Delta\\mathbf{W}_{k}) \\end{align} $$\n那么：\n$$ \\begin{align} \\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} \u0026= \\|\\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k}) + (\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k} + (\\Delta \\boldsymbol{x}_{k-1})(\\Delta\\mathbf{W}_{k})\\|_{\\text{RMS}} \\\\[5pt] \u0026\\leq \\|\\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k})\\|_{\\text{RMS}} + \\|(\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k}\\|_{\\text{RMS}} + \\|(\\Delta \\boldsymbol{x}_{k-1})(\\Delta \\mathbf{W}_{k})\\|_{\\text{RMS}} \\\\ \\end{align} $$\n联系式 $\\#6$，将三项分开来看，同时沿用假设：$\\|\\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}=\\mathcal{O}(1), \\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}=\\mathcal{O}(1)$\n$$ \\begin{align} \\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} \u0026 \\leq \\sqrt{ \\frac{n}{m}}\\bigg(\\|\\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\Delta \\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) \\\\[5pt] \u0026\\leq \\sqrt{ \\frac{n}{m} }\\bigg(\\|\\Delta\\mathbf{W}_{k}\\|_{2}+\\|\\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) \\end{align} $$\n若要求 $\\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} =\\mathcal{O}(1)$，则：\n$$ \\sqrt{ \\frac{n}{m}}\\bigg(\\|\\Delta\\mathbf{W}_{k}\\|_{2}+\\underbrace{ \\|\\mathbf{W}_{k}\\|_{2} }_{\\mathcal{O}(\\sqrt{m/n}) }+\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) =\\mathcal{O}(1) $$\n最后就导出了：\n$$ \\|\\Delta \\mathbf{W}_{k}\\|_{2} = \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n从这个角度来看，其实 Muon 是谱条件的子集，因为谱条件不仅要求控制权重的变化量，还要求控制权重本身\n直觉解释 为什么要正交化 这里给出两个原因：\nJordan（也就是 Muon 的作者）的博客 https://kellerjordan.github.io/posts/muon/ 中是这样说的：\n因为在 Transformer 模型的训练中，梯度矩阵的「条件数」（condition number）通常是非常大的，条件数的一个定义是 $\\sigma_{\\max} / \\sigma_{\\min}$，这个值越大，说明梯度矩阵是由少数主要方向主导的，即 low-rank 的结构，然后正交化可以使得那些本来很弱势的方向被重新关注\n个人认为「高条件数 -\u003e low-rank」是比较牵强的，比如正常满秩的矩阵，最小的奇异值很小，也会让整体的条件数很大。但梯度矩阵是 low-rank 多半是正确的，那么正交化的确会有让弱势方向的比重增大的优势\n然而，如果按照上面的推导就知道，一开始我们是想 bound 住输出变化量的 RMSNorm，进而推导出需要在最速下降的同时控制谱范数\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} =\\mathcal{O}(1) {\\color{#08F}\\implies} \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}}=\\mathcal{O}(1) {\\color{#08F}\\implies} \\|\\Delta \\mathbf{W}\\|_{2}= \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n换句话说，Muon 的本质即是「控制谱范数下的最速下降」\n为什么控制谱范数 那么问题是，为什么要控制谱范数呢？或者为什么控制谱范数下的最速下降收敛更快，泛化更好呢？直观来说，我们是通过 bound 住输出的变化量 $\\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}}$ 来导出需要控制谱范数，如果更新量过大，会使得训练整体就不太稳定，同时控制住之后还可以使得反传的梯度更加健康，总结就是会使得训练过程中前传和反传更加稳定，这也是为什么有些时候 Muon 可以比 Adam 使用更大学习率的原因\n花开两朵各表一枝，控制谱范数这个推论还可以由谱条件推出来。谱条件其实为了小模型上的最优参数（比如学习率）可以迁移到更大的模型上，即参数迁移不受模型尺度影响。那么如何做到呢？即让不同尺度的模型具有相同的 training dynamics，通过控制模型每一层的输出和输出变化量的 RMSNorm 来做到。换言之，控制谱范数还可以让不同尺度模型的 training dynamics 相同，从而达到迁移参数的目的\nMoonlight 接着来介绍 Kimi 团队在 Muon 上的改进，主要是 weight decay 以及对齐 RMSNorm\nWeight Decay Kimi 团队在进行 scaling 实验时，发现在原始 Muon 的不断更新下，权重的 RMSNorm 会不断变大，可能会超出 bfloat16 的精度，进而有损性能。为了弥补这种情况，加上了权重衰减，即为下式蓝色部分\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta(\\mathbf{O}_{t} + {\\color{#08F}\\lambda \\mathbf{W}_{t-1}}) $$\n加了 weight decay 虽然一开始收敛会慢，但后面就会超过不加 weight decay 的情况；同时，如果不加 weight decay，长时间训练后就会跟 AdamW 很接近\nRMSNorm 对齐 将梯度进行奇异值分解： $\\nabla_{\\mathbf{W}}\\mathcal{L}=\\mathbf{U\\Sigma V^{\\top}}$，即 $\\mathbf{U}\\in\\mathbb{R}^{m\\times r}, \\mathbf{V}\\in\\mathbb{R}^{n\\times r}, \\mathbf{\\Sigma} \\in\\mathbb{R}^{r \\times r}$，其中 $r=\\text{rank}(\\nabla_{\\mathbf{W}}\\mathcal{L})\\leq \\min(m, n)$\n向量的 RMSNorm 计算如下：\n$$ \\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\sqrt{\\frac{1}{n}\\sum_{i} \\boldsymbol{x}_{i}^{2}} $$\n那么同理可得矩阵的 RMSNorm 计算：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS}} = \\sqrt{ \\frac{1}{mn} \\sum_{i}\\sum_{j} w_{ij}^{2} } $$\n下面推导依然按照先前对「梯度正交化」的角度，首先：\n$$ \\|\\mathbf{O}_{t}\\|_{\\text{RMS}} = \\|\\mathbf{UV}^{\\top}\\|_\\text{RMS} = \\sqrt{ \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n}\\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2}} $$\n由于 $\\mathbf{U}, \\mathbf{V}$ 都是正交矩阵，所以其行向量和列向量都是「单位向量」，则：\n$$ mn \\|\\mathbf{O}_{t}\\|_{\\text{RMS}}^{2} = \\sum_{i=1}^{m}\\sum_{j=1}^{n}\\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2} = \\sum_{k=1}^{r} \\left(\\sum_{i=1}^{m} u_{ik}^{2}\\right)\\left(\\sum_{j=1}^{n}v_{kj}^{2}\\right) = \\sum_{k=1}^{r} 1 = r $$\n同时因为实际情况下「严格低秩」的概率比较小，所以不妨按满秩来算，即 $r=\\min(m,n)$\n$$ \\|\\mathbf{O}_{t}\\|_{\\text{RMS}} = \\sqrt{ \\frac{r}{mn} } = \\sqrt{ \\frac{1}{\\max(m, n)} } $$\n而实际 LLMs 训练时，不同的权重矩阵 $m,n$ 不同，则导致更新量的 RMSNorm 不均衡，所以可以用一个系数来「归一化更新量的 RMSNorm」，即：\n$$ \\left\\|\\sqrt{ \\max(m,n) } \\cdot \\mathbf{O}_{t}\\right\\|_{\\text{RMS}} = 1 $$\n同时观察到对于 Adam 来说，其 $\\|\\mathbf{O}_{t}\\|_{\\text{RMS}} \\in[0.2, 0.4]$，这里为了可以直接迁移之前 Adam 的学习率，就进一步对齐 Adam 更新量的 RMSNorm，即将 Muon 实际上更新量的 RMSNorm 控制在 $0.2$ 左右：\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta\\left({\\color{#08F}0.2\\sqrt{ \\max(m,n) }}\\mathbf{O}_{t}+\\lambda \\mathbf{W}_{t-1}\\right) $$\n对比一下 Muon 的系数：\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta\\left( {\\color{#08F}\\max\\left( 1,\\sqrt{ \\frac{m}{n} }\\right)} \\mathbf{O}_{t} + \\lambda \\mathbf{W}_{t-1}\\right ) $$\n实现 接下来到了喜闻乐见的上代码环节，具体会实现原始的 Muon 以及 Kimi 在 Muon 上的改进\nNewton Schulz 迭代 Newton Schulz 的实现主要参考官方的源代码 https://github.com/KellerJordan/Muon/blob/master/muon.py ，记 Frobenius Norm $\\|\\mathbf{W}\\|_{\\text{F}}$，其定义如下：\n$$ \\|\\mathbf{W}\\|_{\\text{F}} = \\sqrt{ \\sum_{i}\\sum_{j} w_{ij}^{2}} $$\n接着我们来证明「Frobenius 归一化可以使得矩阵的奇异值缩放到 $[0, 1]$ 之间」，Frobenius Norm 满足下式：\n$$ \\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}} \\leq \\|\\mathbf{W}\\|_{\\text{F}}\\|\\boldsymbol{x}\\|_{\\text{F}} $$\n联想一下，对于向量来说的 L2 norm，不就是 Frobenius Norm 的一种形式嘛：\n$$ \\forall \\boldsymbol{x}, \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{F}}} \\leq \\frac{\\|\\mathbf{W}\\|_{\\text{F}}\\|\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{F}}} = \\|\\mathbf{W}\\|_{\\text{F}} $$\n联系上面我们推导的结果：\n$$ \\max_{\\boldsymbol{x}\\neq\\boldsymbol{0}} \\frac{\\|\\mathbf{A}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\|\\mathbf{A}\\|_{2} \\leq \\|\\mathbf{A}\\|_{\\text{F}} $$\n也就是说「Frobenius Norm 大于或等于 Spectral Norm」，那么 Spectral Norm 代表的是最大的奇异值，这就可以导出 Frobenius Norm 归一化后的矩阵的奇异值在 $[0,1]$ 之间\nNewton Schulz 实现 def newton_schulz5( gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): \"\"\" 1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出 2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \\Sigma 变成 I, 从而完成对梯度的正交化, 即 UV^T, 但实际上是 US'V^T, S'_ii \\in [1-e, 1+e], 并且不影响实际效果 \"\"\" assert gradient.ndim \u003e= 2 dim1, dim2 = gradient.size(-2), gradient.size(-1) gradient = gradient.bfloat16() # 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1) # 但如果 dim1 过大，运算效率和内存开销都不友好 # 所以可以转置，最后返回前转回来即可 if dim1 \u003e dim2: gradient = gradient.mT # 进行 Frobenius Norm 以确保奇异值在 [0, 1] x = gradient / (gradient.norm() + eps) a, b, c = coefficients for _ in range(steps): xx_T = x @ x.mT xx_Tx = xx_T @ x x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) if dim1 \u003e dim2: x = x.mT return x 下图是 $y=ax + bx^{3}+cx^{5}$ 以及 $y=1$（红线），观察发现等于 $1$ 的情况不多\n换句话说，似乎假设并不成立了\n$$ \\begin{align} \\text{Theory: } \\nabla_{\\mathbf{W}}\\mathcal{L} \u0026 = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{UV^{\\top}} \\\\ \\text{Empricial: } \\nabla_{\\mathbf{W}} \\mathcal{L} \u0026 = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{U{\\color{#08F}\\Sigma'}V^{\\top}}, \\Sigma_{ii}' \\in [0.6, 1.2] \\end{align} $$\n真实实验结果是我们需要确保在 $[0,1]$ 内收敛到 $[1-\\epsilon, 1+\\epsilon]$ 即可，按照 Jordan 的博客， $\\epsilon$ 可以大到 $0.3$ 而不影响性能，并不需要严格等于 $1$\n还有一个问题，为什么使用 Newton-Schulz 来进行正交化呢？因为正交化还可以用其他方法：\n不使用 SVD 的原因是因为太慢了 不使用 Coupled Newton Iteration 是因为在 bfloat16 上不稳定，至少得需要 float32 才行 再简单说说如何来微调这些系数，下图是两种系数的对比（取自 Jordan 的博客）：\n主要就是 $\\phi'(0)$ 的值，或者说 $0$ 处以及附近点的斜率，这个决定了初始的收敛速度，大些是更好的\n更新 接着来实现 Muon 的更新规则，默认加上了 weight decay，以及通过判断 rms_match 是否为 None 来决定使用 Kimi 的 rms match 还是官方默认的 scale 系数\nMuon 更新代码 class Muon(torch.optim.Optimizer): ... # 先省略其他部分 @torch.no_grad() def step_muon(self, param_groups): def update_momentum(): momentum.lerp_(grad, 1 - beta) momentum_ns = beta * momentum + (1 - beta) * grad if nesterov else momentum return momentum_ns def update(): p.mul_(1 - lr * weight_decay) if rms_match is not None: # 使用 Kimi 提出的 scale scale = rms_match * math.sqrt(max(p.shape[-2:])) else: # original scale by KellerJordan scale = max(1, p.size(-2) / p.size(-1))**0.5 p.add_(delta, alpha=-lr * scale) for group in param_groups: params = group['params'] lr = group['lr'] weight_decay = group['weight_decay'] beta = group['beta'] nesterov = group['nesterov'] ns_steps = group['ns_steps'] rms_match = group['rms_match'] for p in params: grad = p.grad state = self.state[p] if 'momentum' not in state: state['momentum'] = torch.zeros_like( grad, memory_format=torch.preserve_format ) momentum = state['momentum'] momentum_ns = update_momentum() if momentum_ns.ndim == 4: # 拉伸 conv filter 参数 momentum_ns = momentum_ns.view(momentum_ns.size(0), -1) delta = self.newton_schulz5(momentum_ns, ns_steps) update() def newton_schulz5( self, gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): \"\"\" 1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出 2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \\Sigma 变成 I, 从而完成对梯度的正交化, 即 UV^T, 但实际上是 US'V^T, S'_ii \\in [1-e, 1+e], 并且不影响实际效果 \"\"\" assert gradient.ndim \u003e= 2 dim1, dim2 = gradient.size(-2), gradient.size(-1) gradient = gradient.bfloat16() # 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1) # 但如果 dim1 过大，运算效率和内存开销都不友好 # 所以可以转置，最后返回前转回来即可 if dim1 \u003e dim2: gradient = gradient.mT # 进行 Frobenius Norm 以确保奇异值在 [0, 1] x = gradient / (gradient.norm() + eps) a, b, c = coefficients for _ in range(steps): xx_T = x @ x.mT xx_Tx = xx_T @ x x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) if dim1 \u003e dim2: x = x.mT return x 与 Adam 一道 使用时，对于非矩阵参数（比如 Layer Norm 的 gamma 系数）以及 embed, lm_head 的部分需要使用 Adam 来进行优化，所以还需要加入拆分参数以及 Adam 的使用，这里也列出 Adam 的更新规则来便于读者对照阅读代码：\n$$ \\begin{align} \\boldsymbol{m}_{t} \u0026 = \\beta_{1}\\boldsymbol{m}_{t-1} + (1-\\beta_{1})\\nabla_{\\mathbf{W}}\\mathcal{L} \\\\ \\boldsymbol{v}_{t} \u0026 = \\beta_{2}\\boldsymbol{v}_{t-1} + (1-\\beta_{2})\\nabla_{\\mathbf{W}}^{2}\\mathcal{L} \\\\ \\boldsymbol{\\hat{m}}_{t} \u0026 = \\frac{\\boldsymbol{m}_{t}}{1- \\beta_{1}^{t}}, \\boldsymbol{\\hat{v}}_{t} = \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{2}^{t}} \\\\ \\mathbf{W}_{t} \u0026 = \\mathbf{W}_{t-1} - \\eta\\left( \\frac{\\boldsymbol{\\hat{m}}_{t}}{\\sqrt{ \\boldsymbol{\\hat{v}}_{t} }+\\epsilon} + \\lambda \\mathbf{W}_{t-1} \\right) \\end{align} $$\n还有一条实践的 tip，如果是用在 Transformer 中的 Q，K，V 上，最好是单独的权重，而不是整体一个大的权重，然后再 split 出 Q，K，V\nMuon 整体实现 class Muon(torch.optim.Optimizer): def __init__( self, lr: float, # 仅占位符 params: Iterator[Tuple[str, Parameter]], weight_decay: float = 0.1, beta: float = 0.95, nesterov: bool = True, ns_steps: int = 5, rms_match: float = 0.2, adam_betas: list[float] = [0.9, 0.999], adam_eps: float = 1e-8, ): param_groups = self.split_params(params) other_defaults = dict(betas=adam_betas, eps=adam_eps) param_groups = [ dict( params=param_groups['hidden_matrix_params'], # Muon 去优化的参数 weight_decay=weight_decay, beta=beta, nesterov=nesterov, ns_steps=ns_steps, rms_match=rms_match ), dict( params=param_groups['non_matrix_params'], weight_decay=0., # 默认非矩阵参数不加 weight decay **other_defaults ), dict( params=param_groups['embed_lm_head_matrix_params'], weight_decay=weight_decay, **other_defaults, ) ] super().__init__(param_groups, {}) def step(self): muon_param_groups = [group for group in self.param_groups if 'ns_steps' in group] other_param_groups = [ group for group in self.param_groups if 'ns_steps' not in group ] self.step_muon(muon_param_groups) self.step_adamw(other_param_groups) @torch.no_grad() def step_muon(self, param_groups): ... @torch.no_grad() def step_adamw(self, param_groups): def update_momentum(): momentum1.lerp_(grad, 1 - beta1) momentum2.lerp_(grad.square(), 1 - beta2) def update(): bias_correction1 = 1 - beta1**step bias_correction2 = 1 - beta2**step scale = bias_correction1 / bias_correction2**0.5 delta = momentum1 / (momentum2.sqrt() + eps) p.mul_(1 - lr * weight_decay) p.add_(delta, alpha=-lr / scale) for group in param_groups: params = group['params'] lr = group['lr'] weight_decay = group['weight_decay'] beta1, beta2 = group['betas'] eps = group['eps'] for p in params: grad = p.grad state = self.state[p] if 'step' not in state: state['step'] = 0 state['momentum1'] = torch.zeros_like( grad, memory_format=torch.preserve_format ) state['momentum2'] = torch.zeros_like( grad, memory_format=torch.preserve_format ) momentum1 = state['momentum1'] momentum2 = state['momentum2'] state['step'] += 1 step = state['step'] update_momentum() update() def split_params(self, params: Iterator[Tuple[str, Parameter]]): # params: model.named_parameters() # Muon 只负责优化除了 embed, lm_head 之外的「矩阵」参数 param_dict = {pn: p for pn, p in params if p.requires_grad} non_matrix_params = [p for p in param_dict.values() if p.ndim \u003c 2] embed_lm_head_matrix_params = [ p for pn, p in param_dict.items() if p.dim() \u003e= 2 and ('embed' in pn or 'lm_head' in pn) ] hidden_matrix_params = [ p for pn, p in param_dict.items() if p.ndim \u003e= 2 and 'layers' in pn ] return dict( non_matrix_params=non_matrix_params, embed_lm_head_matrix_params=embed_lm_head_matrix_params, hidden_matrix_params=hidden_matrix_params ) def newton_schulz5( self, gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): ... 实验 setting 设定 数值 $\\#$params 0.6B $\\#$ train tokens 58B $\\#$ eval tokens 0.2B lr 8e-4 weight decay 0.1 seq length 8192 global batch 384 LR schedule linear warm (0.01) cosine decay (1.) 不同的 scale 系数 首先是对照原始 Muon 和 Kimi Muon 的系数，这里偷懒就不单独为原始的 Muon 像 speedrun 那样为 Adam 和 Muon 调制不同的学习率，统一用一个学习率，可以发现无论是训练还是 eval，Kimi 的系数都会更好\nKimi Muon VS AdamW 接着我们来与 AdamW 对比，可以发现是 Kimi Muon 收敛更快\nFLOPs 分析 我们来分析一下 FLOPs（浮点数运行次数），对于一个 $\\mathbf{A} \\in\\mathbb{R}^{m\\times n},\\mathbf{B}\\in\\mathbb{R}^{n\\times p}$，两者相乘的 FLOPs 是 $mp(2n-1)=2mnp-mp$，$mp$ 是新矩阵的元素个数，然后每个新矩阵的元素需要经过 $n$ 次乘法和 $n-1$ 次加法\n介绍完基本概念，来先分析一下 NS 迭代的 FLOPs，主要的计算量在如下代码处：\nfor _ in range(steps): xx_T = x @ x.mT # 1. m x n, n x m =\u003e m x m xx_Tx = xx_T @ x # 2. m x m, m x n =\u003e m x n # 3. xx_T @ xx_Tx: m x m, m x n =\u003e m x n x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) # 4 对于 1 的操作，FLOPs 为 $m^{2}(2n-1)=2m^{2}n-m^{2}$；对于 2 的操作，FLOPs 为 $mn(2m-1)=2m^{2}n-mn$；对于 3 的操作，FLOPS 为 $mn(2m-1) = 2m^{2}n-mn$，然后需要将其进行相加：\n$$ \\begin{align} \\underbrace{ 2m^{2}n-m^{2} }_{ \\mathbf{XX^{\\top}} }+\\underbrace{ 2m^{2}n-mn }_{ \\mathbf{XX^{\\top}X} } + \\underbrace{ mn }_{ a\\mathbf{X} } +\\underbrace{ mn }_{ b\\mathbf{XX^{\\top}X} } + \\underbrace{ mn }_{ c \\times \\dots } + \\underbrace{ 2m^{2}n-mn }_{ \\mathbf{XX^{\\top}XX^{\\top}X} } + \\underbrace{ 2mn }_{ \\text{ 两次加法} } \\\\ = 6m^{2}n-m^{2}+3mn \\approx 6m^{2}n \\end{align} $$\n然后我们运行 $T$ 步 NS 迭代，即为 $6Tm^{2}n$\n对于一个线性层来说，我们对其进行前向和反向的计算的 FLOPs 为多少？这里省略对于偏置的计算，因为不是主要计算量，记输入矩阵 $\\mathbf{X} \\in \\mathbb{R}^{B\\times m}$\n$$ \\begin{align} \u0026\\text{Forward: } \\mathbf{Y} = \\mathbf{XW}: B\\times m, m\\times n= B\\times n\\implies Bn(2m-1) \\approx 2Bmn \\\\ \u0026\\text{Backward 1: } \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{X}} = \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{Y}}\\mathbf{W^{\\top}}: B\\times n, n \\times m \\implies Bm(2n-1)\\approx 2Bmn \\\\ \u0026\\text{Backward 2: } \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{W}} = \\mathbf{X}^{\\top}\\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{Y}}: m \\times B, B\\times n\\implies mn(2B-1)\\approx 2Bmn \\end{align} $$\n所以整个加起来即为 $6Bmn$，这里计算输入的梯度是因为在网络中，当前的输入其实就是前一层的输出，计算当前输入的梯度是为了 back-propogation 的时候便于计算\n那么使用 Muon 时额外带来的开销是：\n$$ \\frac{6Tm^{2}n}{6Bmn} = \\frac{Tm}{B} $$\n当 $T=5$ 时，对于 nanoGPT 以及 Llama 405B 而言，额外的开销并不算很大：\n$$ \\begin{align} \u0026\\text{nanoGPT: } 5 \\times \\frac{768}{524288} = 0.7\\% \\\\ \u0026\\text{Llama 405B: } 5 \\times \\frac{16384}{16\\times10^{6}} = 0.5\\% \\end{align} $$\n","wordCount":"6883","inLanguage":"en","datePublished":"2025-06-03T22:30:00+08:00","dateModified":"2025-06-03T22:30:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://yunpengtai.top/posts/muon/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"http://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://yunpengtai.top/archives/ title=归档><span>归档</span></a></li><li><a href=http://yunpengtai.top/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=http://yunpengtai.top/categories/%E6%8A%98%E8%85%BE title=折腾><span>折腾</span></a></li><li><a href=http://yunpengtai.top/tags/ title=标签><span>标签</span></a></li><li><a href=http://yunpengtai.top/friends/ title=友人><span>友人</span></a></li><li><a href=http://yunpengtai.top/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=http://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Muon: 控制谱范数下的最速下降</h1><div class=post-meta><span title='2025-06-03 22:30:00 +0800 CST'>June 3, 2025</span>&nbsp;·&nbsp;6883 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%8e%a8%e5%af%bc-muon aria-label="推导 Muon">推导 Muon</a><ul><li><a href=#%e5%ba%a6%e9%87%8f%e7%ba%bf%e6%80%a7%e5%b1%82 aria-label=度量线性层>度量线性层</a></li><li><a href=#%e8%be%93%e5%87%ba%e5%8f%98%e5%8c%96%e9%87%8f aria-label=输出变化量>输出变化量</a></li><li><a href=#%e5%af%b9%e5%81%b6%e5%8c%96%e6%a2%af%e5%ba%a6 aria-label=对偶化梯度>对偶化梯度</a></li><li><a href=#%e6%b1%82%e8%a7%a3%e7%ba%a6%e6%9d%9f aria-label=求解约束>求解约束</a></li><li><a href=#%e6%b1%82%e6%ad%a3%e4%ba%a4%e5%8c%96 aria-label=求正交化>求正交化</a></li><li><a href=#%e6%9b%b4%e6%96%b0%e8%a7%84%e5%88%99 aria-label=更新规则>更新规则</a></li></ul></li><li><a href=#%e4%b8%8e%e8%b0%b1%e6%9d%a1%e4%bb%b6%e7%9a%84%e8%81%94%e7%b3%bb aria-label=与谱条件的联系>与谱条件的联系</a></li><li><a href=#%e7%9b%b4%e8%a7%89%e8%a7%a3%e9%87%8a aria-label=直觉解释>直觉解释</a><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e6%ad%a3%e4%ba%a4%e5%8c%96 aria-label=为什么要正交化>为什么要正交化</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8e%a7%e5%88%b6%e8%b0%b1%e8%8c%83%e6%95%b0 aria-label=为什么控制谱范数>为什么控制谱范数</a></li></ul></li><li><a href=#moonlight aria-label=Moonlight>Moonlight</a><ul><li><a href=#weight-decay aria-label="Weight Decay">Weight Decay</a></li><li><a href=#rmsnorm-%e5%af%b9%e9%bd%90 aria-label="RMSNorm 对齐">RMSNorm 对齐</a></li></ul></li><li><a href=#%e5%ae%9e%e7%8e%b0 aria-label=实现>实现</a><ul><li><a href=#newton-schulz-%e8%bf%ad%e4%bb%a3 aria-label="Newton Schulz 迭代">Newton Schulz 迭代</a></li><li><a href=#%e6%9b%b4%e6%96%b0 aria-label=更新>更新</a></li><li><a href=#%e4%b8%8e-adam-%e4%b8%80%e9%81%93 aria-label="与 Adam 一道">与 Adam 一道</a></li></ul></li><li><a href=#%e5%ae%9e%e9%aa%8c aria-label=实验>实验</a><ul><li><a href=#setting aria-label=setting>setting</a></li><li><a href=#%e4%b8%8d%e5%90%8c%e7%9a%84-scale-%e7%b3%bb%e6%95%b0 aria-label="不同的 scale 系数">不同的 scale 系数</a></li><li><a href=#kimi-muon-vs-adamw aria-label="Kimi Muon VS AdamW">Kimi Muon VS AdamW</a></li></ul></li><li><a href=#flops-%e5%88%86%e6%9e%90 aria-label="FLOPs 分析">FLOPs 分析</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>本文将主要涵盖以下内容：</p><ol><li>从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客
<span class=sidenote-number><small class=sidenote><a href=https://jeremybernste.in/writing/deriving-muon>https://jeremybernste.in/writing/deriving-muon</a></small></span>
的基础上进行延伸。值得注意的是，推导的过程跟真正实现上有差异，比如实际是对动量进行正交化，而不是对梯度，但读者无须担心，本文最后还是会回归到具体的实现</li><li>介绍 Kimi 团队
<span class=sidenote-number><small class=sidenote><a href=https://github.com/MoonshotAI/Moonlight>https://github.com/MoonshotAI/Moonlight</a></small></span>
在 Muon 基础上的改进和代码实现，主要是 weight decay 以及对齐更新量的 RMSNorm 两个方面</li><li>逐一实现上面提及的 Muon，然后与原始的 Adam 做对照实验进行验证</li><li>最后对 Muon 进行 FLOPS 分析</li></ol><h2 id=推导-muon>推导 Muon<a hidden class=anchor aria-hidden=true href=#推导-muon>#</a></h2><h3 id=度量线性层>度量线性层<a hidden class=anchor aria-hidden=true href=#度量线性层>#</a></h3><p>给定输入 <code>$\boldsymbol{x} \in\mathbb{R}^{n}$</code>，权重矩阵 <code>$\mathbf{W}\in\mathbb{R}^{m \times n}$</code>，过一层「线性层」（Linear Layers），即 <code>$\boldsymbol{y} = \mathbf{W}\boldsymbol{x}$</code>（这里对 bias 进行忽略）。那么有个有趣的问题，<code>$\mathbf{W}$</code> 究竟对输入做了什么？或者如何度量这种线性运算呢？</p><p>此时可以联系一下「算子范数」（Operator Norm）的定义：给定任意两种 norm 方式 <code>$\| \cdot\|_{\text{F}}$</code> 和 <code>$\|\cdot\|_{\text{E}}$</code>，对于任意的 <code>$\boldsymbol{x}$</code>，算子范数是 <code>$\mathbf{W}$</code> 能对 <code>$\boldsymbol{x}$</code> 进行的最大拉伸量：</p><p><code>$$ \|\mathbf{W}\|_{\text{op}} := \max_{\boldsymbol{x}\neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{\text{F}}}{\|\boldsymbol{x}\|_{\text{E}}} $$</code></p><p>接着让我们看看两种 norm 方式 <code>$\|\cdot\|_{\text{F}}, \|\cdot\|_{\text{E}}$</code> 均为 RMSNorm 时会发生什么，先回顾下 RMSNorm 的定义：</p><p><code>$$ \|\boldsymbol{x}\|_{\text{RMS}} = \sqrt{ \frac{1}{n} \sum_{i} \boldsymbol{x}_{i}^{2}} = \sqrt{ \frac{1}{n} } \|\boldsymbol{x}\|_{2} $$</code></p><p>那么：</p><p><code>$$ \|\mathbf{W}\|_{\text{RMS} \to \text{RMS}} := \max_{\boldsymbol{x} \neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{\text{RMS}}}{ \|\boldsymbol{x}\|_{\text{RMS}}} = \sqrt{ \frac{n}{m} }\underbrace{ {\color{#08F} \max_{\boldsymbol{x} \neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}}} }_{\text{L2 operator norm} } \tag{\#1} $$</code></p><p>可以发现 RMSNorm 算子范数是一种归一化后的 L2 算子范数，那么 L2 算子范数究竟是什么呢？我们接着推导：</p><p>首先对 <code>$\mathbf{W}$</code> 进行「奇异值分解」（SVD），即 <code>$\mathbf{W}=\mathbf{U\Sigma V^{\top}}$</code>，其中 <code>$\mathbf{U}, \mathbf{V}$</code> 都是正交矩阵，而 <code>$\mathbf{\Sigma}$</code> 是对角矩阵，对角线的元素为奇异值，不妨设 <code>$\sigma_{1}\geq\sigma_{2}\geq \dots \geq\sigma_{r}, \, r=\min(m,n)$</code></p><p>先说明一个重要的性质，即「正交变换之后不改变 L2 范数的大小」，证明如下：因为 <code>$\mathbf{V}$</code> 是正交矩阵，所以 <code>$\mathbf{V}^{\top}\mathbf{V} = \mathbf{I}_{n}$</code></p><p><code>$$ \|\mathbf{V}\boldsymbol{x}\|_{2}^{2} = (\mathbf{V}\boldsymbol{x})^{\top}\mathbf{V}\boldsymbol{x} = \boldsymbol{x}^{\top}\underbrace{ \mathbf{V}^{\top}\mathbf{V} }_{ \mathbf{I}_{n} }\boldsymbol{x} = \boldsymbol{x}^{\top}\boldsymbol{x} = \|\boldsymbol{x}\|_{2}^{2} $$</code></p><p>接着开始正式推导 L2 算子范数，不妨记 <code>$\mathbf{V}^{\top}\boldsymbol{x} = \boldsymbol{y}$</code>，蓝色部分的变换都用到了刚刚提及的「正交变换之后不改变 L2 范数的大小」的性质</p><p><code>$$ \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} = \frac{\|\mathbf{U\Sigma V^{\top}}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} = \frac{\|\mathbf{U\Sigma}\boldsymbol{y}\|_{2}}{{\color{#08F}\|\boldsymbol{y}\|_{2}}} = \frac{{\color{#08F}\|\Sigma \boldsymbol{y}\|_{2}}}{\|\boldsymbol{y}\|_{2}} $$</code></p><p>那么：</p><p><code>$$ \max_{\boldsymbol{x}\neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} \implies \max \frac{\|\Sigma \boldsymbol{y}\|_{2}^{2}}{\|\boldsymbol{y}\|_{2}^{2}}= \frac{\sum_{i}\sigma_{i}^{2}y_{i}^{2}}{\sum_{i}y_{i}^{2}}\leq \frac{\sigma_{1}^{2}\sum_{i}y_{i}^{2}}{\sum_{i}y_{i}^{2}} = \sigma_{1}^{2} $$</code></p><p>即：</p><p><code>$$ \max_{\boldsymbol{x} \neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} = \sigma_{1} = \sigma_{\text{max}} = \underbrace{ \|\mathbf{W}\|_{2} }_{ \text{Spectral Norm} } $$</code></p><p>「谱范数」（Spectral Norm）指的是矩阵的最大奇异值，那么联系式 <code>$\#1$</code> 可得：</p><p><code>$$ \|\mathbf{W}\|_{\text{RMS} \to \text{RMS}} = \sqrt{ \frac{n}{m} }{ \max_{\boldsymbol{x} \neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}}} = \sqrt{ \frac{n}{m} }\|\mathbf{W}\|_{2} \tag{\#2} $$</code></p><blockquote class=quote><p>RMSNorm 的算子范数是一种归一化的谱范数</p></blockquote><h3 id=输出变化量>输出变化量<a hidden class=anchor aria-hidden=true href=#输出变化量>#</a></h3><p>在训练神经网络时，我们想知道，当权重矩阵更新后，输出会多大程度随之变化（当然这里 <code>$\boldsymbol{x}$</code> 也会发生变化，会在后文论述），即：</p><p><code>$$ \Delta \boldsymbol{y} = (\mathbf{W}+\Delta \mathbf{W})\boldsymbol{x} - \mathbf{W}\boldsymbol{x} = \Delta \mathbf{W}\boldsymbol{x} $$</code></p><p>联系式 <code>$\#1$</code> 的定义，可知：</p><p><code>$$ \|\Delta \boldsymbol{y}\|_{\text{RMS}} = \|\Delta \mathbf{W}\boldsymbol{x}\|_{\text{RMS}} \leq \|\Delta \mathbf{W}\|_{\text{RMS}\to \text{RMS}} \cdot\|\boldsymbol{x}\|_{\text{RMS}} \tag{\#3} $$</code></p><p>换言之，当权重矩阵更新后，我们找到了输出变化量 RMSNorm 的最大值，这里利用算子范数，巧妙地将矩阵的更新和输出的更新联系了起来</p><h3 id=对偶化梯度>对偶化梯度<a hidden class=anchor aria-hidden=true href=#对偶化梯度>#</a></h3><p>通过「泰勒展开式」（Taylor Expansion），可知：</p><p><code>$$ \mathcal{L}(\mathbf{W} + \Delta \mathbf{W}) = \mathcal{L}(\mathbf{W})+ \langle \nabla_{\mathbf{W}} \mathcal{L}, \Delta \mathbf{W} \rangle + \text{High Order Terms} $$</code></p><p>由于变化量比较小，不妨对高次项进行省略，即「线性近似」：</p><p><code>$$ \Delta \mathcal{L} = \mathcal{L}(\mathbf{W}+\Delta \mathbf{W}) - \mathcal{L}(\mathbf{W}) \approx \langle\nabla_{\mathbf{W}}\mathcal{L}, \Delta \mathbf{W}\rangle $$</code></p><p>我们肯定想要 loss 变化量越小越好，同时变化的过程中，我们并不希望输出变化量太大，否则就会破坏前面泰勒展开的前提；同时如果输出变化量太大，会让训练过程不稳定。那么我们就对输出变化量加个 bound，即：</p><p><code>$$ \min \, \langle \nabla_{\mathbf{W}}\mathcal{L},\Delta \mathbf{W}\rangle, \|\Delta \boldsymbol{y}\|_{\text{RMS}} =\mathcal{O}(1) $$</code></p><p>笔者以为「对输出变化量进行约束」即为 Muon 的核心 motivation，将式 <code>$\#3$</code> 代入，则：</p><p><code>$$ \min \, \langle \nabla_{\mathbf{W}}\mathcal{L},\Delta \mathbf{W}\rangle, \, \|\Delta \mathbf{W}\|_{\text{RMS}\to \text{RMS}} \cdot\|\boldsymbol{x}\|_{\text{RMS}} =\mathcal{O}(1) $$</code></p><p>假设 <code>$\|\boldsymbol{x}\|_{\text{RMS}} = \mathcal{O}(1)$</code>，则：</p><p><code>$$ \min \, \langle \nabla_{\mathbf{W}}\mathcal{L},\Delta \mathbf{W}\rangle, \, \|\Delta \mathbf{W}\|_{\text{RMS}\to \text{RMS}} =\mathcal{O}(1) \tag{\#4} $$</code></p><p>式 <code>$\#4$</code> 即被称为「对偶化梯度」（Dualizing the Gradients），那么这个如何解？</p><p>可以先展开一下约束，使其与谱范数联系起来：</p><p><code>$$ \begin{align} \|\Delta \mathbf{W}\|_{\text{RMS}\to \text{RMS}} = \sqrt{ \frac{n}{m} } \|\Delta\mathbf{W}\|_{2} =\mathcal{O}(1) \\ \implies \|\Delta\mathbf{W}\|_{2} =\mathcal{O}\left(\sqrt{\frac{m}{n}}\right) \end{align} $$</code></p><h3 id=求解约束>求解约束<a hidden class=anchor aria-hidden=true href=#求解约束>#</a></h3><p>接着我们对梯度进行奇异值分解： <code>$\nabla_{\mathbf{W}}\mathcal{L}=\mathbf{U\Sigma V^{\top}}$</code>，这里是「经济型分解」，即 <code>$\mathbf{U}\in\mathbb{R}^{m\times r}, \mathbf{V}\in\mathbb{R}^{n\times r}, \mathbf{\Sigma} \in\mathbb{R}^{r \times r}$</code>，其中 <code>$r=\text{rank}(\nabla_{\mathbf{W}}\mathcal{L})\leq \min(m, n)$</code>。假设没有约束的情况下，我们想要去 <code>$\min \langle \nabla_{\mathbf{W}}\mathcal{L}, \Delta \mathbf{W}\rangle$</code>，我们会直接按照梯度的反方向，即</p><p><code>$$ \Delta \mathbf{W} = - \nabla_{\mathbf{W}}\mathcal{L} $$</code></p><p>但目前有个约束条件，我们可以对 <code>$\Delta \mathbf{W}$</code> 进行「正交化」（Orthogonalization）， <code>$c \in\mathbb{R}$</code> 是一个系数：</p><p><code>$$ \Delta \mathbf{W} = -c \cdot\mathbf{UV}^{\top} $$</code></p><p>之所以写成上述形式，主要是因为约束是有关谱范数的，而 <code>$\|\mathbf{UV^{\top}}\|_{2}=1$</code>，下面来证明这一性质。首先，对于一个矩阵 <code>$\mathbf{A} \in \mathbb{R}^{m \times n}$</code> 来说，要求其谱范数，按照如下流程：</p><p><code>$$ \|\mathbf{A}\|_{2} = \sqrt{ \lambda_{\max}(\mathbf{A^{\ast}}\mathbf{A}) } = \sigma_{\max}(\mathbf{A}) $$</code></p><p>其中 <code>$\mathbf{A}^{*}$</code> 是矩阵 <code>$\mathbf{A}$</code> 的「共轭转置」（Conjugate Transpose），对于「实数域」的矩阵来说，共轭等于其自身，故而 <code>$\mathbf{A}^{*}= \mathbf{A}^{\top}$</code>，所以就可以求 <code>$\|\mathbf{UV}^{\top}\|_{2}$</code></p><p><code>$$ (\mathbf{UV}^{\top})^{\top}\mathbf{UV^{\top}} = \mathbf{VU}^{\top}\mathbf{UV^{\top}} = \mathbf{VV^{\top}} = \mathbf{I}_{r}\implies \|\mathbf{UV}^{\top}\|_{2} = \sqrt{ \lambda_{\max} } = 1 $$</code></p><p>接着来计算一下权重更新量的谱范数：</p><p><code>$$ \|\Delta\mathbf{W}\|_{2} = \|-c\cdot \mathbf{UV^{\top}}\|_{2} = |c| \cdot \|\mathbf{UV^{\top}}\|_{2} = |c| $$</code></p><p>那么：</p><p><code>$$ \|\Delta \mathbf{W\|_{2}}=\mathcal{O} \left(\sqrt{ \frac{m}{n} } \right)\implies c =\mathcal{O}\left(\sqrt{ \frac{m}{n} }\right) $$</code></p><p>可得：</p><p><code>$$ \Delta \mathbf{W} = - \mathcal{O}\left(\sqrt{ \frac{m}{n} } \right)\mathbf{UV^{\top}} \tag{\#5} $$</code></p><p>即式 <code>$\#5$</code> 就是对偶化梯度的解，小结一下，我们将对输出变化量的约束转为对权重变化量的谱范数要求，最后通过「正交化」来求解</p><h3 id=求正交化>求正交化<a hidden class=anchor aria-hidden=true href=#求正交化>#</a></h3><p>那么如何求正交化呢？正交化即让「奇异值全变为 <code>$1$</code>」，而奇异值是正数，就相当于对其使用了 sign 函数</p><p><code>$$ \nabla_{\mathbf{W}}\mathcal{L} = \mathbf{U\Sigma V^{\top}} \mapsto \mathbf{UV^{\top}} $$</code></p><p>但是要分解完 SVD，再对 <code>$\Sigma$</code> 使用 sign，这个代价有点大，有没有直接在 <code>$\nabla_{\mathbf{W}}\mathcal{L}$</code> 上作用的办法呢？有一条推论可以借助：「奇次矩阵多项式（Odd Matrix Polynomials）和 SVD 存在可交换性」，即我们对 <code>$\nabla_{\mathbf{W}}\mathcal{L}$</code> 的操作等于对 <code>$\Sigma$</code> 进行操作：</p><p><code>$$ p(\nabla_{\mathbf{W}}\mathcal{L}) = p(\mathbf{U\Sigma V^{\top}}) = \mathbf{U}p(\Sigma)\mathbf{V^{\top}} $$</code></p><p>但问题是 sign 并非是矩阵多项式的形式，奇次矩阵多项式一般长这样：</p><p><code>$$ p(\mathbf{X}):= a\cdot \mathbf{X} + b \cdot \mathbf{XX^{\top}X} + c\cdot \mathbf{XX^{\top}XX^{\top}X} + \dots $$</code></p><p>此时，就可以用一种 sign 近似
<span class=sidenote-number><small class=sidenote><a href=https://epubs.siam.org/doi/10.1137/0707031>https://epubs.siam.org/doi/10.1137/0707031</a></small></span>
来完成，即：</p><p><code>$$ p(\Sigma) := \frac{3}{2}\Sigma - \frac{1}{2}\Sigma\Sigma^{\top}\Sigma $$</code></p><p>可以用单变量来可视化看一下：</p><p><code>$$ p(x) = \frac{3}{2} x - \frac{1}{2} x^{3}; \quad p_{n}(x)= \underbrace{ p \circ p \circ \dots \circ p }_{ n }(x) $$</code></p><p>当 <code>$n=14$</code> 时，可以发现，在 <code>$[0, \sqrt{ 3 }]$</code> 范围内可以做到很好地近似 sign，即结果为 <code>$1$</code></p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/fb2ea1d5c563fcff52aeb78a352d5ff3.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/fb2ea1d5c563fcff52aeb78a352d5ff3.png#center width=400px height=350px></figure></a><p>而上述过程被称为「Newton-Schulz Iteration」，因为我们的目的是想让 <code>$\mathbf{\Sigma}$</code> 变成单位矩阵，所以也可称为矩阵的「零次幂」（Zero Power）求解过程</p><p>让我们简要总结一下推导出的 Muon 公式，其中 <code>$\eta$</code> 是学习率</p><p><code>$$ \mathbf{W}_{} \gets \mathbf{W} - \eta \cdot\mathcal{O}\left(\sqrt{ \frac{m}{n} }\right)\text{NewtonSchulz}(\nabla_{\mathbf{W}}\mathcal{L}), \mathbf{W} \in\mathbb{R}^{m \times n} $$</code></p><h3 id=更新规则>更新规则<a hidden class=anchor aria-hidden=true href=#更新规则>#</a></h3><p>实际上 Muon 的更新规则如下：我们并非是对梯度做正交化，而是对 Nesterov Style 的动量做，然后系数是 <code>$1, \sqrt{ m/n }$</code> 的最大值</p><p><code>$$ \begin{align} \mathbf{M}_{t} & = \beta \, \mathbf{M}_{t-1} + (1-\beta) \nabla_{\mathbf{W}}\mathcal{L}\\ \mathbf{O}_{t} & = \text{NewtonSchulz}(\beta \,\mathbf{M}_{t} + (1-\beta)\nabla_{\mathbf{W}}\mathcal{L})\\ \mathbf{W}_{t} & = \mathbf{W}_{t-1} - \eta \cdot {\color{#08F}\max \left( 1, \sqrt{ \frac{m}{n} } \right)} \mathbf{O}_{t} \end{align} $$</code></p><h2 id=与谱条件的联系>与谱条件的联系<a hidden class=anchor aria-hidden=true href=#与谱条件的联系>#</a></h2><p>先联系上述式 <code>$\#1$</code> 和式 <code>$\#2$</code>：</p><p><code>$$ \begin{align} \|\mathbf{W}\|_{\text{RMS} \to \text{RMS}} & := \max_{\boldsymbol{x} \neq \boldsymbol{0}} \frac{\|\mathbf{W}\boldsymbol{x}\|_{\text{RMS}}}{ \|\boldsymbol{x}\|_{\text{RMS}}} \\ \|\mathbf{W}\|_{\text{RMS} \to \text{RMS}} & = \sqrt{ \frac{n}{m} }\|\mathbf{W}\|_{2} \end{align} $$</code></p><p>那么可以导出：</p><p><code>$$ \|\mathbf{W}\boldsymbol{x}\|_{\text{RMS}} \leq \|\mathbf{W}\|_{\text{RMS}\to \text{RMS}} \|\boldsymbol{x}\|_{\text{RMS}}=\sqrt{ \frac{n}{m} }\|\mathbf{W}\|_{2}\|\boldsymbol{x}\|_{\text{RMS}} \tag{\#6} $$</code></p><p>如果想要控制输出的 RMSNorm，即 <code>$\|\mathbf{W}\boldsymbol{x}\|_{\text{RMS}} = \mathcal{O}(1)$</code>，假设 <code>$\|\boldsymbol{x}\|_{\text{RMS}}=\mathcal{O}(1)$</code>，则：</p><p><code>$$ \sqrt{ \frac{n}{m} }\|\mathbf{W}\|_{2}\|\boldsymbol{x}\|_{\text{RMS}} =\mathcal{O}(1) \implies \|\mathbf{W}\|_{2} =\mathcal{O}\left(\sqrt{ \frac{m}{n} }\right) $$</code></p><p>上面推导 Muon 是约束输出变化量来导出对权重的谱范数进行约束，但严格来说，关于输出变化量的推导是不严谨的，这也是 Muon 和谱条件
<span class=sidenote-number><small class=sidenote><a href=https://arxiv.org/abs/2310.17813>https://arxiv.org/abs/2310.17813</a></small></span>
（Spectral Condition）的不同之处，因为当权重变了之后，输入也会随之改变，比如第二层的输入其实是第一层的输出，这里推导借鉴了苏老师关于谱条件的介绍
<span class=sidenote-number><small class=sidenote><a href=https://kexue.fm/archives/10795>https://kexue.fm/archives/10795</a></small></span>
，记 <code>$\boldsymbol{x}_{k}$</code> 为第 <code>$k$</code> 层的输出</p><p><code>$$ \begin{align} \Delta \boldsymbol{x}_{k} & = (\boldsymbol{x}_{k-1}+\Delta \boldsymbol{x}_{k-1})(\mathbf{W}_{k}+\Delta \mathbf{W}_{k}) - \boldsymbol{x}_{k-1}\mathbf{W}_{k} \\[5pt] &= \boldsymbol{x}_{k-1}(\Delta \mathbf{W}_{k}) + (\Delta \boldsymbol{x}_{k-1})\mathbf{W}_{k} + (\Delta \boldsymbol{x}_{k-1})(\Delta\mathbf{W}_{k}) \end{align} $$</code></p><p>那么：</p><p><code>$$ \begin{align} \|\Delta \boldsymbol{x}_{k}\|_{\text{RMS}} &= \|\boldsymbol{x}_{k-1}(\Delta \mathbf{W}_{k}) + (\Delta \boldsymbol{x}_{k-1})\mathbf{W}_{k} + (\Delta \boldsymbol{x}_{k-1})(\Delta\mathbf{W}_{k})\|_{\text{RMS}} \\[5pt] &\leq \|\boldsymbol{x}_{k-1}(\Delta \mathbf{W}_{k})\|_{\text{RMS}} + \|(\Delta \boldsymbol{x}_{k-1})\mathbf{W}_{k}\|_{\text{RMS}} + \|(\Delta \boldsymbol{x}_{k-1})(\Delta \mathbf{W}_{k})\|_{\text{RMS}} \\ \end{align} $$</code></p><p>联系式 <code>$\#6$</code>，将三项分开来看，同时沿用假设：<code>$\|\boldsymbol{x}_{k-1}\|_{\text{RMS}}=\mathcal{O}(1), \|\Delta \boldsymbol{x}_{k-1}\|_{\text{RMS}}=\mathcal{O}(1)$</code></p><p><code>$$ \begin{align} \|\Delta \boldsymbol{x}_{k}\|_{\text{RMS}} & \leq \sqrt{ \frac{n}{m}}\bigg(\|\boldsymbol{x}_{k-1}\|_{\text{RMS}}\|\Delta \mathbf{W}_{k}\|_{2}+\|\Delta \boldsymbol{x}_{k-1}\|_{\text{RMS}}\|\mathbf{W}_{k}\|_{2}+\|\Delta \boldsymbol{x}_{k-1}\|_{\text{RMS}}\|\Delta \mathbf{W}_{k}\|_{2}\bigg) \\[5pt] &\leq \sqrt{ \frac{n}{m} }\bigg(\|\Delta\mathbf{W}_{k}\|_{2}+\|\mathbf{W}_{k}\|_{2}+\|\Delta \mathbf{W}_{k}\|_{2}\bigg) \end{align} $$</code></p><p>若要求 <code>$\|\Delta \boldsymbol{x}_{k}\|_{\text{RMS}} =\mathcal{O}(1)$</code>，则：</p><p><code>$$ \sqrt{ \frac{n}{m}}\bigg(\|\Delta\mathbf{W}_{k}\|_{2}+\underbrace{ \|\mathbf{W}_{k}\|_{2} }_{\mathcal{O}(\sqrt{m/n}) }+\|\Delta \mathbf{W}_{k}\|_{2}\bigg) =\mathcal{O}(1) $$</code></p><p>最后就导出了：</p><p><code>$$ \|\Delta \mathbf{W}_{k}\|_{2} = \mathcal{O}\left(\sqrt{ \frac{m}{n} }\right) $$</code></p><p>从这个角度来看，其实 Muon 是谱条件的子集，因为谱条件不仅要求控制权重的变化量，还要求控制权重本身</p><h2 id=直觉解释>直觉解释<a hidden class=anchor aria-hidden=true href=#直觉解释>#</a></h2><h3 id=为什么要正交化>为什么要正交化<a hidden class=anchor aria-hidden=true href=#为什么要正交化>#</a></h3><p>这里给出两个原因：</p><p>Jordan（也就是 Muon 的作者）的博客
<span class=sidenote-number><small class=sidenote><a href=https://kellerjordan.github.io/posts/muon/>https://kellerjordan.github.io/posts/muon/</a></small></span>
中是这样说的：</p><blockquote class=quote><p>因为在 Transformer 模型的训练中，梯度矩阵的「条件数」（condition number）通常是非常大的，条件数的一个定义是 <code>$\sigma_{\max} / \sigma_{\min}$</code>，这个值越大，说明梯度矩阵是由少数主要方向主导的，即 low-rank 的结构，然后正交化可以使得那些本来很弱势的方向被重新关注</p></blockquote><p>个人认为「高条件数 -> low-rank」是比较牵强的，比如正常满秩的矩阵，最小的奇异值很小，也会让整体的条件数很大。但梯度矩阵是 low-rank 多半是正确的，那么正交化的确会有让弱势方向的比重增大的优势</p><p>然而，如果按照上面的推导就知道，一开始我们是想 bound 住输出变化量的 RMSNorm，进而推导出需要在最速下降的同时控制谱范数</p><p><code>$$ \min \, \langle \nabla_{\mathbf{W}}\mathcal{L},\Delta \mathbf{W}\rangle, \|\Delta \boldsymbol{y}\|_{\text{RMS}} =\mathcal{O}(1) {\color{#08F}\implies} \|\Delta \mathbf{W}\|_{\text{RMS}\to \text{RMS}}=\mathcal{O}(1) {\color{#08F}\implies} \|\Delta \mathbf{W}\|_{2}= \mathcal{O}\left(\sqrt{ \frac{m}{n} }\right) $$</code></p><p>换句话说，Muon 的本质即是「控制谱范数下的最速下降」</p><h3 id=为什么控制谱范数>为什么控制谱范数<a hidden class=anchor aria-hidden=true href=#为什么控制谱范数>#</a></h3><p>那么问题是，为什么要控制谱范数呢？或者为什么控制谱范数下的最速下降收敛更快，泛化更好呢？直观来说，我们是通过 bound 住输出的变化量 <code>$\|\Delta \boldsymbol{y}\|_{\text{RMS}}$</code> 来导出需要控制谱范数，如果更新量过大，会使得训练整体就不太稳定，同时控制住之后还可以使得反传的梯度更加健康，总结就是会使得训练过程中前传和反传更加稳定，这也是为什么有些时候 Muon 可以比 Adam 使用更大学习率的原因</p><p>花开两朵各表一枝，控制谱范数这个推论还可以由谱条件推出来。谱条件其实为了小模型上的最优参数（比如学习率）可以迁移到更大的模型上，即参数迁移不受模型尺度影响。那么如何做到呢？即让不同尺度的模型具有相同的 training dynamics，通过控制模型每一层的输出和输出变化量的 RMSNorm 来做到。换言之，控制谱范数还可以让不同尺度模型的 training dynamics 相同，从而达到迁移参数的目的</p><h2 id=moonlight>Moonlight<a hidden class=anchor aria-hidden=true href=#moonlight>#</a></h2><p>接着来介绍 Kimi 团队在 Muon 上的改进，主要是 weight decay 以及对齐 RMSNorm</p><h3 id=weight-decay>Weight Decay<a hidden class=anchor aria-hidden=true href=#weight-decay>#</a></h3><p>Kimi 团队在进行 scaling 实验时，发现在原始 Muon 的不断更新下，权重的 RMSNorm 会不断变大，可能会超出 bfloat16 的精度，进而有损性能。为了弥补这种情况，加上了权重衰减，即为下式蓝色部分</p><p><code>$$ \mathbf{W}_{t} = \mathbf{W}_{t-1} - \eta(\mathbf{O}_{t} + {\color{#08F}\lambda \mathbf{W}_{t-1}}) $$</code></p><p>加了 weight decay 虽然一开始收敛会慢，但后面就会超过不加 weight decay 的情况；同时，如果不加 weight decay，长时间训练后就会跟 AdamW 很接近</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/1a970b80a98081dbc8c5d5cd4e486462.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/1a970b80a98081dbc8c5d5cd4e486462.png#center></figure></a><h3 id=rmsnorm-对齐>RMSNorm 对齐<a hidden class=anchor aria-hidden=true href=#rmsnorm-对齐>#</a></h3><p>将梯度进行奇异值分解： <code>$\nabla_{\mathbf{W}}\mathcal{L}=\mathbf{U\Sigma V^{\top}}$</code>，即 <code>$\mathbf{U}\in\mathbb{R}^{m\times r}, \mathbf{V}\in\mathbb{R}^{n\times r}, \mathbf{\Sigma} \in\mathbb{R}^{r \times r}$</code>，其中 <code>$r=\text{rank}(\nabla_{\mathbf{W}}\mathcal{L})\leq \min(m, n)$</code></p><p>向量的 RMSNorm 计算如下：</p><p><code>$$ \|\boldsymbol{x}\|_{\text{RMS}} = \sqrt{\frac{1}{n}\sum_{i} \boldsymbol{x}_{i}^{2}} $$</code></p><p>那么同理可得矩阵的 RMSNorm 计算：</p><p><code>$$ \|\mathbf{W}\|_{\text{RMS}} = \sqrt{ \frac{1}{mn} \sum_{i}\sum_{j} w_{ij}^{2} } $$</code></p><p>下面推导依然按照先前对「梯度正交化」的角度，首先：</p><p><code>$$ \|\mathbf{O}_{t}\|_{\text{RMS}} = \|\mathbf{UV}^{\top}\|_\text{RMS} = \sqrt{ \frac{1}{mn} \sum_{i=1}^{m} \sum_{j=1}^{n}\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2}} $$</code></p><p>由于 <code>$\mathbf{U}, \mathbf{V}$</code> 都是正交矩阵，所以其行向量和列向量都是「单位向量」，则：</p><p><code>$$ mn \|\mathbf{O}_{t}\|_{\text{RMS}}^{2} = \sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2} = \sum_{k=1}^{r} \left(\sum_{i=1}^{m} u_{ik}^{2}\right)\left(\sum_{j=1}^{n}v_{kj}^{2}\right) = \sum_{k=1}^{r} 1 = r $$</code></p><p>同时因为实际情况下「严格低秩」的概率比较小，所以不妨按满秩来算，即 <code>$r=\min(m,n)$</code></p><p><code>$$ \|\mathbf{O}_{t}\|_{\text{RMS}} = \sqrt{ \frac{r}{mn} } = \sqrt{ \frac{1}{\max(m, n)} } $$</code></p><p>而实际 LLMs 训练时，不同的权重矩阵 <code>$m,n$</code> 不同，则导致更新量的 RMSNorm 不均衡，所以可以用一个系数来「归一化更新量的 RMSNorm」，即：</p><p><code>$$ \left\|\sqrt{ \max(m,n) } \cdot \mathbf{O}_{t}\right\|_{\text{RMS}} = 1 $$</code></p><p>同时观察到对于 Adam 来说，其 <code>$\|\mathbf{O}_{t}\|_{\text{RMS}} \in[0.2, 0.4]$</code>，这里为了可以直接迁移之前 Adam 的学习率，就进一步对齐 Adam 更新量的 RMSNorm，即将 Muon 实际上更新量的 RMSNorm 控制在 <code>$0.2$</code> 左右：</p><p><code>$$ \mathbf{W}_{t} = \mathbf{W}_{t-1} - \eta\left({\color{#08F}0.2\sqrt{ \max(m,n) }}\mathbf{O}_{t}+\lambda \mathbf{W}_{t-1}\right) $$</code></p><p>对比一下 Muon 的系数：</p><p><code>$$ \mathbf{W}_{t} = \mathbf{W}_{t-1} - \eta\left( {\color{#08F}\max\left( 1,\sqrt{ \frac{m}{n} }\right)} \mathbf{O}_{t} + \lambda \mathbf{W}_{t-1}\right ) $$</code></p><h2 id=实现>实现<a hidden class=anchor aria-hidden=true href=#实现>#</a></h2><p>接下来到了喜闻乐见的上代码环节，具体会实现原始的 Muon 以及 Kimi 在 Muon 上的改进</p><h3 id=newton-schulz-迭代>Newton Schulz 迭代<a hidden class=anchor aria-hidden=true href=#newton-schulz-迭代>#</a></h3><p>Newton Schulz 的实现主要参考官方的源代码
<span class=sidenote-number><small class=sidenote><a href=https://github.com/KellerJordan/Muon/blob/master/muon.py>https://github.com/KellerJordan/Muon/blob/master/muon.py</a></small></span>
，记 Frobenius Norm <code>$\|\mathbf{W}\|_{\text{F}}$</code>，其定义如下：</p><p><code>$$ \|\mathbf{W}\|_{\text{F}} = \sqrt{ \sum_{i}\sum_{j} w_{ij}^{2}} $$</code></p><p>接着我们来证明「Frobenius 归一化可以使得矩阵的奇异值缩放到 <code>$[0, 1]$</code> 之间」，Frobenius Norm 满足下式：</p><p><code>$$ \|\mathbf{W}\boldsymbol{x}\|_{\text{F}} \leq \|\mathbf{W}\|_{\text{F}}\|\boldsymbol{x}\|_{\text{F}} $$</code></p><p>联想一下，对于向量来说的 L2 norm，不就是 Frobenius Norm 的一种形式嘛：</p><p><code>$$ \forall \boldsymbol{x}, \frac{\|\mathbf{W}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} = \frac{\|\mathbf{W}\boldsymbol{x}\|_{\text{F}}}{\|\boldsymbol{x}\|_{\text{F}}} \leq \frac{\|\mathbf{W}\|_{\text{F}}\|\boldsymbol{x}\|_{\text{F}}}{\|\boldsymbol{x}\|_{\text{F}}} = \|\mathbf{W}\|_{\text{F}} $$</code></p><p>联系上面我们推导的结果：</p><p><code>$$ \max_{\boldsymbol{x}\neq\boldsymbol{0}} \frac{\|\mathbf{A}\boldsymbol{x}\|_{2}}{\|\boldsymbol{x}\|_{2}} = \|\mathbf{A}\|_{2} \leq \|\mathbf{A}\|_{\text{F}} $$</code></p><p>也就是说「Frobenius Norm 大于或等于 Spectral Norm」，那么 Spectral Norm 代表的是最大的奇异值，这就可以导出 Frobenius Norm 归一化后的矩阵的奇异值在 <code>$[0,1]$</code> 之间</p><p><details><summary markdown=span>Newton Schulz 实现</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>newton_schulz5</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>coefficients</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mf>3.4445</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.7750</span><span class=p>,</span> <span class=mf>2.0315</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>eps</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-7</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出
</span></span></span><span class=line><span class=cl><span class=s2>    2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \Sigma 变成 I,
</span></span></span><span class=line><span class=cl><span class=s2>    从而完成对梯度的正交化, 即 UV^T, 但实际上是 US&#39;V^T, S&#39;_ii \in [1-e, 1+e],
</span></span></span><span class=line><span class=cl><span class=s2>    并且不影响实际效果
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>gradient</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&gt;=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>dim1</span><span class=p>,</span> <span class=n>dim2</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>),</span> <span class=n>gradient</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 但如果 dim1 过大，运算效率和内存开销都不友好</span>
</span></span><span class=line><span class=cl>    <span class=c1># 所以可以转置，最后返回前转回来即可</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dim1</span> <span class=o>&gt;</span> <span class=n>dim2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>    <span class=c1># 进行 Frobenius Norm 以确保奇异值在 [0, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>gradient</span> <span class=o>/</span> <span class=p>(</span><span class=n>gradient</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span> <span class=o>=</span> <span class=n>coefficients</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>xx_T</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>x</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>        <span class=n>xx_Tx</span> <span class=o>=</span> <span class=n>xx_T</span> <span class=o>@</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>b</span> <span class=o>*</span> <span class=n>xx_Tx</span> <span class=o>+</span> <span class=n>c</span> <span class=o>*</span> <span class=p>(</span><span class=n>xx_T</span> <span class=o>@</span> <span class=n>xx_Tx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dim1</span> <span class=o>&gt;</span> <span class=n>dim2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div></details></p><p>下图是 <code>$y=ax + bx^{3}+cx^{5}$</code> 以及 <code>$y=1$</code>（红线），观察发现等于 <code>$1$</code> 的情况不多</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/63389e6ae7fa900842a13cde93ee5555.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/63389e6ae7fa900842a13cde93ee5555.png#center width=350px height=350px></figure></a><p>换句话说，似乎假设并不成立了</p><p><code>$$ \begin{align} \text{Theory: } \nabla_{\mathbf{W}}\mathcal{L} & = \mathbf{U\Sigma V^{\top}} \mapsto \mathbf{UV^{\top}} \\ \text{Empricial: } \nabla_{\mathbf{W}} \mathcal{L} & = \mathbf{U\Sigma V^{\top}} \mapsto \mathbf{U{\color{#08F}\Sigma'}V^{\top}}, \Sigma_{ii}' \in [0.6, 1.2] \end{align} $$</code></p><p>真实实验结果是我们需要确保在 <code>$[0,1]$</code> 内收敛到 <code>$[1-\epsilon, 1+\epsilon]$</code> 即可，按照 Jordan 的博客， <code>$\epsilon$</code> 可以大到 <code>$0.3$</code> 而不影响性能，并不需要严格等于 <code>$1$</code></p><p>还有一个问题，为什么使用 Newton-Schulz 来进行正交化呢？因为正交化还可以用其他方法：</p><ol><li>不使用 SVD 的原因是因为太慢了</li><li>不使用 Coupled Newton Iteration 是因为在 bfloat16 上不稳定，至少得需要 float32 才行</li></ol><p>再简单说说如何来微调这些系数，下图是两种系数的对比（取自 Jordan 的博客）：</p><p>主要就是 <code>$\phi'(0)$</code> 的值，或者说 <code>$0$</code> 处以及附近点的斜率，这个决定了初始的收敛速度，大些是更好的</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/a737c3a70e9c0cc2c853e1cd1afeeaa7.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/a737c3a70e9c0cc2c853e1cd1afeeaa7.png#center width=700px height=350px></figure></a><h3 id=更新>更新<a hidden class=anchor aria-hidden=true href=#更新>#</a></h3><p>接着来实现 Muon 的更新规则，默认加上了 weight decay，以及通过判断 rms_match 是否为 None 来决定使用 Kimi 的 rms match 还是官方默认的 scale 系数</p><p><details><summary markdown=span>Muon 更新代码</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Muon</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span> <span class=c1># 先省略其他部分</span>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step_muon</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>param_groups</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>update_momentum</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>momentum</span><span class=o>.</span><span class=n>lerp_</span><span class=p>(</span><span class=n>grad</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>beta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>momentum_ns</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>momentum</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta</span><span class=p>)</span> <span class=o>*</span> <span class=n>grad</span> <span class=k>if</span> <span class=n>nesterov</span> <span class=k>else</span> <span class=n>momentum</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>momentum_ns</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>update</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>weight_decay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>rms_match</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>  <span class=c1># 使用 Kimi 提出的 scale</span>
</span></span><span class=line><span class=cl>                <span class=n>scale</span> <span class=o>=</span> <span class=n>rms_match</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=nb>max</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]))</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>  <span class=c1># original scale by KellerJordan</span>
</span></span><span class=line><span class=cl>                <span class=n>scale</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>p</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>p</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span><span class=o>**</span><span class=mf>0.5</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=n>delta</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>params</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>lr</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>weight_decay</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;weight_decay&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>beta</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;beta&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>nesterov</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;nesterov&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>ns_steps</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;ns_steps&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>rms_match</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;rms_match&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>params</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>grad</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>p</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=s1>&#39;momentum&#39;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>state</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>grad</span><span class=p>,</span> <span class=n>memory_format</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>preserve_format</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>momentum</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>momentum_ns</span> <span class=o>=</span> <span class=n>update_momentum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>momentum_ns</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>4</span><span class=p>:</span>  <span class=c1># 拉伸 conv filter 参数</span>
</span></span><span class=line><span class=cl>                    <span class=n>momentum_ns</span> <span class=o>=</span> <span class=n>momentum_ns</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>momentum_ns</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>delta</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>newton_schulz5</span><span class=p>(</span><span class=n>momentum_ns</span><span class=p>,</span> <span class=n>ns_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>update</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>newton_schulz5</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>coefficients</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mf>3.4445</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.7750</span><span class=p>,</span> <span class=mf>2.0315</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>eps</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出
</span></span></span><span class=line><span class=cl><span class=s2>        2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \Sigma 变成 I,
</span></span></span><span class=line><span class=cl><span class=s2>        从而完成对梯度的正交化, 即 UV^T, 但实际上是 US&#39;V^T, S&#39;_ii \in [1-e, 1+e],
</span></span></span><span class=line><span class=cl><span class=s2>        并且不影响实际效果
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>gradient</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&gt;=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=n>dim1</span><span class=p>,</span> <span class=n>dim2</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>),</span> <span class=n>gradient</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 但如果 dim1 过大，运算效率和内存开销都不友好</span>
</span></span><span class=line><span class=cl>        <span class=c1># 所以可以转置，最后返回前转回来即可</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>dim1</span> <span class=o>&gt;</span> <span class=n>dim2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>        <span class=c1># 进行 Frobenius Norm 以确保奇异值在 [0, 1]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>gradient</span> <span class=o>/</span> <span class=p>(</span><span class=n>gradient</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>c</span> <span class=o>=</span> <span class=n>coefficients</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>xx_T</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>x</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>            <span class=n>xx_Tx</span> <span class=o>=</span> <span class=n>xx_T</span> <span class=o>@</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>b</span> <span class=o>*</span> <span class=n>xx_Tx</span> <span class=o>+</span> <span class=n>c</span> <span class=o>*</span> <span class=p>(</span><span class=n>xx_T</span> <span class=o>@</span> <span class=n>xx_Tx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>dim1</span> <span class=o>&gt;</span> <span class=n>dim2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mT</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div></details></p><h3 id=与-adam-一道>与 Adam 一道<a hidden class=anchor aria-hidden=true href=#与-adam-一道>#</a></h3><p>使用时，对于非矩阵参数（比如 Layer Norm 的 gamma 系数）以及 embed, lm_head 的部分需要使用 Adam 来进行优化，所以还需要加入拆分参数以及 Adam 的使用，这里也列出 Adam 的更新规则来便于读者对照阅读代码：</p><p><code>$$ \begin{align} \boldsymbol{m}_{t} & = \beta_{1}\boldsymbol{m}_{t-1} + (1-\beta_{1})\nabla_{\mathbf{W}}\mathcal{L} \\ \boldsymbol{v}_{t} & = \beta_{2}\boldsymbol{v}_{t-1} + (1-\beta_{2})\nabla_{\mathbf{W}}^{2}\mathcal{L} \\ \boldsymbol{\hat{m}}_{t} & = \frac{\boldsymbol{m}_{t}}{1- \beta_{1}^{t}}, \boldsymbol{\hat{v}}_{t} = \frac{\boldsymbol{v}_{t}}{1-\beta_{2}^{t}} \\ \mathbf{W}_{t} & = \mathbf{W}_{t-1} - \eta\left( \frac{\boldsymbol{\hat{m}}_{t}}{\sqrt{ \boldsymbol{\hat{v}}_{t} }+\epsilon} + \lambda \mathbf{W}_{t-1} \right) \end{align} $$</code></p><p>还有一条实践的 tip，如果是用在 Transformer 中的 Q，K，V 上，最好是单独的权重，而不是整体一个大的权重，然后再 split 出 Q，K，V</p><p><details><summary markdown=span>Muon 整体实现</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Muon</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>  <span class=c1># 仅占位符</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span><span class=p>:</span> <span class=n>Iterator</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Parameter</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>        <span class=n>weight_decay</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>beta</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.95</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>nesterov</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ns_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>rms_match</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>adam_betas</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>adam_eps</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>param_groups</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_params</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>other_defaults</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>betas</span><span class=o>=</span><span class=n>adam_betas</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=n>adam_eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>param_groups</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>params</span><span class=o>=</span><span class=n>param_groups</span><span class=p>[</span><span class=s1>&#39;hidden_matrix_params&#39;</span><span class=p>],</span>  <span class=c1># Muon 去优化的参数</span>
</span></span><span class=line><span class=cl>                <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>beta</span><span class=o>=</span><span class=n>beta</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>nesterov</span><span class=o>=</span><span class=n>nesterov</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ns_steps</span><span class=o>=</span><span class=n>ns_steps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>rms_match</span><span class=o>=</span><span class=n>rms_match</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>params</span><span class=o>=</span><span class=n>param_groups</span><span class=p>[</span><span class=s1>&#39;non_matrix_params&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.</span><span class=p>,</span>  <span class=c1># 默认非矩阵参数不加 weight decay</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=n>other_defaults</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>params</span><span class=o>=</span><span class=n>param_groups</span><span class=p>[</span><span class=s1>&#39;embed_lm_head_matrix_params&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=n>other_defaults</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>param_groups</span><span class=p>,</span> <span class=p>{})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>muon_param_groups</span> <span class=o>=</span> <span class=p>[</span><span class=n>group</span> <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_groups</span> <span class=k>if</span> <span class=s1>&#39;ns_steps&#39;</span> <span class=ow>in</span> <span class=n>group</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>other_param_groups</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>group</span> <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_groups</span> <span class=k>if</span> <span class=s1>&#39;ns_steps&#39;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>group</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>step_muon</span><span class=p>(</span><span class=n>muon_param_groups</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>step_adamw</span><span class=p>(</span><span class=n>other_param_groups</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step_muon</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>param_groups</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step_adamw</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>param_groups</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>update_momentum</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>momentum1</span><span class=o>.</span><span class=n>lerp_</span><span class=p>(</span><span class=n>grad</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>momentum2</span><span class=o>.</span><span class=n>lerp_</span><span class=p>(</span><span class=n>grad</span><span class=o>.</span><span class=n>square</span><span class=p>(),</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>update</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>bias_correction1</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=o>**</span><span class=n>step</span>
</span></span><span class=line><span class=cl>            <span class=n>bias_correction2</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=o>**</span><span class=n>step</span>
</span></span><span class=line><span class=cl>            <span class=n>scale</span> <span class=o>=</span> <span class=n>bias_correction1</span> <span class=o>/</span> <span class=n>bias_correction2</span><span class=o>**</span><span class=mf>0.5</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=n>momentum1</span> <span class=o>/</span> <span class=p>(</span><span class=n>momentum2</span><span class=o>.</span><span class=n>sqrt</span><span class=p>()</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>weight_decay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=n>delta</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=-</span><span class=n>lr</span> <span class=o>/</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>params</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>lr</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>weight_decay</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;weight_decay&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>beta1</span><span class=p>,</span> <span class=n>beta2</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;betas&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>eps</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;eps&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>params</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>grad</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>p</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=s1>&#39;step&#39;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>state</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>grad</span><span class=p>,</span> <span class=n>memory_format</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>preserve_format</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum2&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>grad</span><span class=p>,</span> <span class=n>memory_format</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>preserve_format</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>momentum1</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum1&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>momentum2</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;momentum2&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=n>step</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>update_momentum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>update</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>split_params</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>:</span> <span class=n>Iterator</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Parameter</span><span class=p>]]):</span>
</span></span><span class=line><span class=cl>        <span class=c1># params: model.named_parameters()</span>
</span></span><span class=line><span class=cl>        <span class=c1># Muon 只负责优化除了 embed, lm_head 之外的「矩阵」参数</span>
</span></span><span class=line><span class=cl>        <span class=n>param_dict</span> <span class=o>=</span> <span class=p>{</span><span class=n>pn</span><span class=p>:</span> <span class=n>p</span> <span class=k>for</span> <span class=n>pn</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>params</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>non_matrix_params</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>param_dict</span><span class=o>.</span><span class=n>values</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&lt;</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>embed_lm_head_matrix_params</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span> <span class=k>for</span> <span class=n>pn</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>param_dict</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>dim</span><span class=p>()</span> <span class=o>&gt;=</span> <span class=mi>2</span> <span class=ow>and</span> <span class=p>(</span><span class=s1>&#39;embed&#39;</span> <span class=ow>in</span> <span class=n>pn</span> <span class=ow>or</span> <span class=s1>&#39;lm_head&#39;</span> <span class=ow>in</span> <span class=n>pn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_matrix_params</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span> <span class=k>for</span> <span class=n>pn</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>param_dict</span><span class=o>.</span><span class=n>items</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>ndim</span> <span class=o>&gt;=</span> <span class=mi>2</span> <span class=ow>and</span> <span class=s1>&#39;layers&#39;</span> <span class=ow>in</span> <span class=n>pn</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>non_matrix_params</span><span class=o>=</span><span class=n>non_matrix_params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>embed_lm_head_matrix_params</span><span class=o>=</span><span class=n>embed_lm_head_matrix_params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_matrix_params</span><span class=o>=</span><span class=n>hidden_matrix_params</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>newton_schulz5</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>coefficients</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mf>3.4445</span><span class=p>,</span> <span class=o>-</span><span class=mf>4.7750</span><span class=p>,</span> <span class=mf>2.0315</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>eps</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span>
</span></span></code></pre></div></details></p><h2 id=实验>实验<a hidden class=anchor aria-hidden=true href=#实验>#</a></h2><h3 id=setting>setting<a hidden class=anchor aria-hidden=true href=#setting>#</a></h3><table><thead><tr><th style=text-align:center>设定</th><th style=text-align:center>数值</th></tr></thead><tbody><tr><td style=text-align:center><code>$\#$</code>params</td><td style=text-align:center>0.6B</td></tr><tr><td style=text-align:center><code>$\#$</code> train tokens</td><td style=text-align:center>58B</td></tr><tr><td style=text-align:center><code>$\#$</code> eval tokens</td><td style=text-align:center>0.2B</td></tr><tr><td style=text-align:center>lr</td><td style=text-align:center>8e-4</td></tr><tr><td style=text-align:center>weight decay</td><td style=text-align:center>0.1</td></tr><tr><td style=text-align:center>seq length</td><td style=text-align:center>8192</td></tr><tr><td style=text-align:center>global batch</td><td style=text-align:center>384</td></tr><tr><td style=text-align:center>LR schedule</td><td style=text-align:center>linear warm (0.01) cosine decay (1.)</td></tr></tbody></table><h3 id=不同的-scale-系数>不同的 scale 系数<a hidden class=anchor aria-hidden=true href=#不同的-scale-系数>#</a></h3><p>首先是对照原始 Muon 和 Kimi Muon 的系数，这里偷懒就不单独为原始的 Muon 像 speedrun 那样为 Adam 和 Muon 调制不同的学习率，统一用一个学习率，可以发现无论是训练还是 eval，Kimi 的系数都会更好</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/a5d0a747d859159f4361f929ddc41b0a.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/a5d0a747d859159f4361f929ddc41b0a.png#center></figure></a><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/da4f2c00d3d01585b8a6161551e6264c.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/da4f2c00d3d01585b8a6161551e6264c.png#center></figure></a><h3 id=kimi-muon-vs-adamw>Kimi Muon VS AdamW<a hidden class=anchor aria-hidden=true href=#kimi-muon-vs-adamw>#</a></h3><p>接着我们来与 AdamW 对比，可以发现是 Kimi Muon 收敛更快</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/a18addd05ad24e0d0e000c502e1b3831.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/a18addd05ad24e0d0e000c502e1b3831.png#center></figure></a><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://png.yunpengtai.top/2025/06/35eb10f3e6868ad03e7b308aec9a5e88.png><figure class=align-center><img loading=lazy src=https://png.yunpengtai.top/2025/06/35eb10f3e6868ad03e7b308aec9a5e88.png#center></figure></a><h2 id=flops-分析>FLOPs 分析<a hidden class=anchor aria-hidden=true href=#flops-分析>#</a></h2><p>我们来分析一下 FLOPs（浮点数运行次数），对于一个 <code>$\mathbf{A} \in\mathbb{R}^{m\times n},\mathbf{B}\in\mathbb{R}^{n\times p}$</code>，两者相乘的 FLOPs 是 <code>$mp(2n-1)=2mnp-mp$</code>，<code>$mp$</code> 是新矩阵的元素个数，然后每个新矩阵的元素需要经过 <code>$n$</code> 次乘法和 <code>$n-1$</code> 次加法</p><p>介绍完基本概念，来先分析一下 NS 迭代的 FLOPs，主要的计算量在如下代码处：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>xx_T</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>x</span><span class=o>.</span><span class=n>mT</span> <span class=c1># 1. m x n, n x m =&gt; m x m</span>
</span></span><span class=line><span class=cl>    <span class=n>xx_Tx</span> <span class=o>=</span> <span class=n>xx_T</span> <span class=o>@</span> <span class=n>x</span> <span class=c1># 2. m x m, m x n =&gt; m x n</span>
</span></span><span class=line><span class=cl>    <span class=c1># 3. xx_T @ xx_Tx: m x m, m x n =&gt; m x n</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>b</span> <span class=o>*</span> <span class=n>xx_Tx</span> <span class=o>+</span> <span class=n>c</span> <span class=o>*</span> <span class=p>(</span><span class=n>xx_T</span> <span class=o>@</span> <span class=n>xx_Tx</span><span class=p>)</span> <span class=c1># 4</span>
</span></span></code></pre></div><p>对于 1 的操作，FLOPs 为 <code>$m^{2}(2n-1)=2m^{2}n-m^{2}$</code>；对于 2 的操作，FLOPs 为 <code>$mn(2m-1)=2m^{2}n-mn$</code>；对于 3 的操作，FLOPS 为 <code>$mn(2m-1) = 2m^{2}n-mn$</code>，然后需要将其进行相加：</p><p><code>$$ \begin{align} \underbrace{ 2m^{2}n-m^{2} }_{ \mathbf{XX^{\top}} }+\underbrace{ 2m^{2}n-mn }_{ \mathbf{XX^{\top}X} } + \underbrace{ mn }_{ a\mathbf{X} } +\underbrace{ mn }_{ b\mathbf{XX^{\top}X} } + \underbrace{ mn }_{ c \times \dots } + \underbrace{ 2m^{2}n-mn }_{ \mathbf{XX^{\top}XX^{\top}X} } + \underbrace{ 2mn }_{ \text{ 两次加法} } \\ = 6m^{2}n-m^{2}+3mn \approx 6m^{2}n \end{align} $$</code></p><p>然后我们运行 <code>$T$</code> 步 NS 迭代，即为 <code>$6Tm^{2}n$</code></p><p>对于一个线性层来说，我们对其进行前向和反向的计算的 FLOPs 为多少？这里省略对于偏置的计算，因为不是主要计算量，记输入矩阵 <code>$\mathbf{X} \in \mathbb{R}^{B\times m}$</code></p><p><code>$$ \begin{align} &\text{Forward: } \mathbf{Y} = \mathbf{XW}: B\times m, m\times n= B\times n\implies Bn(2m-1) \approx 2Bmn \\ &\text{Backward 1: } \frac{\text{d}\mathcal{L}}{\text{d}\mathbf{X}} = \frac{\text{d}\mathcal{L}}{\text{d}\mathbf{Y}}\mathbf{W^{\top}}: B\times n, n \times m \implies Bm(2n-1)\approx 2Bmn \\ &\text{Backward 2: } \frac{\text{d}\mathcal{L}}{\text{d}\mathbf{W}} = \mathbf{X}^{\top}\frac{\text{d}\mathcal{L}}{\text{d}\mathbf{Y}}: m \times B, B\times n\implies mn(2B-1)\approx 2Bmn \end{align} $$</code></p><p>所以整个加起来即为 <code>$6Bmn$</code>，这里计算输入的梯度是因为在网络中，当前的输入其实就是前一层的输出，计算当前输入的梯度是为了 back-propogation 的时候便于计算</p><p>那么使用 Muon 时额外带来的开销是：</p><p><code>$$ \frac{6Tm^{2}n}{6Bmn} = \frac{Tm}{B} $$</code></p><p>当 <code>$T=5$</code> 时，对于 nanoGPT 以及 Llama 405B 而言，额外的开销并不算很大：</p><p><code>$$ \begin{align} &\text{nanoGPT: } 5 \times \frac{768}{524288} = 0.7\% \\ &\text{Llama 405B: } 5 \times \frac{16384}{16\times10^{6}} = 0.5\% \end{align} $$</code></p></div><div style="margin-top:2em;padding:1em;border:0 solid;border-radius:10px;background-color:var(--code-bg)"><h3>如果您想要引用，请考虑如下格式：</h3><div style=padding-top:.5em>台运鹏. (Jun. 3, 2025). 《Muon: 控制谱范数下的最速下降》[Blog
post]. Retrieved from http://yunpengtai.top/posts/muon/</div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>@online{blog-b7d41de06fb8ec87754d2e577274744d,
</span></span><span class=line><span class=cl>        title={Muon: 控制谱范数下的最速下降},
</span></span><span class=line><span class=cl>        author={Yunpeng Tai},
</span></span><span class=line><span class=cl>        year={2025},
</span></span><span class=line><span class=cl>        month={Jun},
</span></span><span class=line><span class=cl>        note={http://yunpengtai.top/posts/muon/},
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div><div style=padding-bottom:.4em>自由转载-非商用-非衍生-保持署名（<a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-SA 4.0）</a></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://yunpengtai.top/tags/llms/>llms</a></li><li><a href=http://yunpengtai.top/tags/training/>training</a></li><li><a href=http://yunpengtai.top/tags/optimizer/>optimizer</a></li><li><a href=http://yunpengtai.top/tags/muon/>muon</a></li><li><a href=http://yunpengtai.top/tags/adam/>adam</a></li></ul><nav class=paginav><a class=prev href=http://yunpengtai.top/posts/agentic-coding/><span class=title>« Prev</span><br><span>Agentic Coding: 当编程被按下加速键</span></a>
<a class=next href=http://yunpengtai.top/posts/know-mihomo/><span class=title>Next »</span><br><span>「网络代理」这件小事</span></a></nav></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/artalk@2.8.6/dist/Artalk.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/@artalk/plugin-katex@0.2.4/dist/artalk-plugin-katex.min.js></script><div id=Comments></div><script>const savedTheme=localStorage.getItem("pref-theme");let darkMode="auto";savedTheme!==null&&(darkMode=savedTheme==="dark");const artalk=Artalk.init({el:"#Comments",pageKey:"",pageTitle:"Muon: 控制谱范数下的最速下降",server:"https://comment.yunpengtai.top",site:"Tai's Blog",darkMode,versionCheck:!1});document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?artalk.setDarkMode(!1):artalk.setDarkMode(!0)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=http://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.min.js",function(){pangu.spacingPage()})</script><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>