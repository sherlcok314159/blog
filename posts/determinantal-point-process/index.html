<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Determinantal Point Process | Tai's Blog</title><meta name=keywords content="ML"><meta name=description content="在机器学习中，我们通常会面临一个问题：给定一个集合$\mathbf{S}$，从中寻找$k$个样本构成子集$\mathbf{V}$，尽量使得子"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=http://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=http://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"all",packages:{"[+]":["boldsymbol"]}},chtml:{scale:.9}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="Determinantal Point Process"><meta property="og:description" content="在机器学习中，我们通常会面临一个问题：给定一个集合$\mathbf{S}$，从中寻找$k$个样本构成子集$\mathbf{V}$，尽量使得子"><meta property="og:type" content="article"><meta property="og:url" content="http://yunpengtai.top/posts/determinantal-point-process/"><meta property="og:image" content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-21T15:20:00+08:00"><meta property="article:modified_time" content="2023-04-21T15:20:00+08:00"><meta property="og:site_name" content="Tai's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yunpengtai.top/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Determinantal Point Process"><meta name=twitter:description content="在机器学习中，我们通常会面临一个问题：给定一个集合$\mathbf{S}$，从中寻找$k$个样本构成子集$\mathbf{V}$，尽量使得子"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"http://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"Determinantal Point Process","item":"http://yunpengtai.top/posts/determinantal-point-process/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Determinantal Point Process","name":"Determinantal Point Process","description":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子","keywords":["ML"],"articleBody":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子集的质量高同时多样性好。比如在推荐系统中，我们就希望给用户推荐的东西尽可能的有质量，同时具有差异性。\n而使得采样的子集尽可能具备多样性便是行列式点过程（Determinantal Point Process）大展身手的地方，俗称 DPP\n边缘分布 首先引入 DPP 的边缘分布定义，当我们某次采样出子集$\\mathbf{A}$，「包括」$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$的概率：\n$$ P(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K_{V}}) $$\n$\\mathbf{K}$是核矩阵（Kernel Matrix），即：\n$$ \\mathbf{K}_{ij} = k(\\boldsymbol{v_{i}}, \\boldsymbol{v_{j}}) $$\n$\\mathbf{K_{V}}$是由$\\mathbf{V}$中元素构成的子矩阵，举个例子，假如$\\mathbf{V}=\\{ 1,2 \\}$，那么：\n$$ P(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K_{V}}) = \\left|\\begin{array}{cc} \\mathbf{K}_{11} \u0026 \\mathbf{K}_{12} \\\\ \\mathbf{K}_{21} \u0026 \\mathbf{K}_{22} \\end{array}\\right| = \\mathbf{K}_{11}\\mathbf{K}_{22} - \\mathbf{K}_{12}^{2} $$\n当$\\mathbf{K}_{12}$越大，则$\\{ 1,2 \\}$同时出现在$\\mathbf{V}$的概率就越小，从这个角度想，核函数应该是呈现出某种相似性\n从正定性出发，严格的定义如下是：$\\mathbf{0}\\preceq\\mathbf{K} \\preceq \\mathbf{I}$\n举个例子：\n$$ \\mathbf{K} = \\begin{bmatrix} 1 \u0026 -0.3 \\\\ -0.3 \u0026 1 \\end{bmatrix} $$\n其特征值为$0.7, -1.3$，不满足$\\mathbf{K}-\\mathbf{0} \\succeq \\mathbf{0}$，即不是半正定矩阵\nL-Ensemble 然而，上面边缘定义只是告诉我们采样时，某个子集被「包括」的概率，并非就是这个子集，而这个问题可以通过 L-Ensemble 去解\n$$ P(\\mathbf{V}=\\mathbf{A}) \\propto \\det(\\mathbf{L}) $$\n这里的$\\mathbf{L}$省略了下标，跟上面的$\\mathbf{K}$一样，是跟$\\mathbf{V}$元素相关的子矩阵。$\\mathbf{L}$矩阵的核函数是内积是$\\boldsymbol{v_{i}^{\\top}\\boldsymbol{v_{j}}}$，$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$\n$$ \\mathbf{L} = \\mathbf{V}^{\\top}\\mathbf{V} = \\begin{bmatrix} \\langle \\boldsymbol{v_{1}}, \\boldsymbol{v_{1}} \\rangle \u0026 \\langle \\boldsymbol{v_{1}},\\boldsymbol{v_{2}} \\rangle \u0026 \\dots \u0026 \\langle \\boldsymbol{v_{1}}, \\boldsymbol{v_{k}} \\rangle \\\\ \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{1}} \\rangle \u0026 \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{2}} \\rangle \u0026 \\dots \u0026 \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{k}} \\rangle \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{1}} \\rangle \u0026 \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{2}} \\rangle \u0026 \\dots \u0026 \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{k}} \\rangle \\end{bmatrix} $$\n注意，这里指的不是概率，而是说概率「正比于」$\\mathbf{L}$矩阵的行列式，那么如何计算概率呢？也就是说我们得计算一个归一化常数（normalization constant），可以类比抛硬币，我们得去求总的抛起次数，除以它才能得到概率\n引入下述定理：\n$$ \\sum_{\\mathbf{A}\\subseteq \\mathbf{V} \\subseteq \\mathbf{S}} \\det(\\mathbf{L}) = \\det(\\mathbf{L} + \\mathbf{I_{\\bar{A}}}) $$\n其中$\\mathbf{I_{\\bar{A}}}$是将单位矩阵中与$\\mathbf{A}$相关元素全部置零，举个例子，当$\\mathbf{S} = \\{ 1,2,3 \\}, \\mathbf{A}={1,2}$时：\n$$ \\mathbf{I_{\\bar{A}}} = \\begin{bmatrix} 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$\n那么如何求归一化常数呢，即将$\\mathbf{A}=\\emptyset$，当$\\mathbf{A}$为空集时，便包括了所有的情况，即：\n$$ P(\\mathbf{V}=\\mathbf{A}) = \\frac{\\det(\\mathbf{L})}{\\det(\\mathbf{L}+\\mathbf{I})} $$\n另外，L-Emsemble 的$\\mathbf{K}, \\mathbf{L}$对应关系如下：\n$$ \\begin{align} \\mathbf{K} \u0026 = \\mathbf{L}(\\mathbf{L} + \\mathbf{I})^{-1} \\\\ \\mathbf{L} \u0026 = \\mathbf{K}(\\mathbf{I}-\\mathbf{K})^{-1} \\end{align} $$\n直观解释 那么，行列式与多样性的直观解释是什么呢？\n多样性和相似性的意思正好相反，通常我们会定义相似性为两个向量之间做点积，即为$\\boldsymbol{v_{1}}^{\\top}\\boldsymbol{v_{2}}$，直观上看，两向量夹角的余弦值$\\cos \\theta$ 越大，相似性越高，反过来看，当$\\cos \\theta$最小即为两者相似性最差，多样性最好。显然，当两向量正交时多样性最好。\n那么，对于一个子集$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$而言，该如何定义它的多样性呢？不难想出，可以通过线性无关向量的数量来定义，若两两都互不线性相关，此时的子集的多样性是最好的。直观上可以转换为构成的超平行体的体积，下方为$k=2,3$时的示意图\n图源自王树森老师的课程\n为什么呢？可以拿平行六面体为例，若其中一个向量与其他向量线性相关，那么则会坍缩成一个平面，构不成平行六面体\n图源自 3BlueBrown 对于行列式的介绍\n只有当所有向量两两都线性无关时，构成的超平行体体积最大，即多样性最好\n而行列式可以表示体积，下式中$\\text{vol}$代表体积（volume），此时$k=d$，$\\mathbf{V}$为方阵\n$$ \\det(\\mathbf{V})= \\text{vol}(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})) $$\n也就是说，我们可以通过行列式的大小来定义多样性\n那么，$\\mathbf{L}$的行列式是否也跟体积有关呢？答案是肯定的：\n$$ \\det(\\mathbf{L}) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots,\\boldsymbol{v_{k}})\\bigg)^{2} $$\n接下来证明这一结论：\n由于$k \\leq d$，因为$d$维空间至多存在$d$个两两线性无关的向量，那么肯定存在一个$k$维子空间$\\mathcal{H}$，存在正交矩阵$\\mathbf{R} \\in \\mathbb{R}^{d \\times d}$，对向量$\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}}$进行旋转，使得$\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}}$都落在子空间$\\mathcal{H}$上。不妨设$\\mathcal{H}$的基底是前$k$个标准正交基，那么：\n$$ \\mathbf{R}\\boldsymbol{v_{i}} = \\begin{bmatrix} \\boldsymbol{u_{i}} \\\\ \\mathbf{0} \\end{bmatrix} $$\n$\\boldsymbol{u_{i}} \\in \\mathbb{R}^{k}$，$\\mathbf{0}$一共有$d-k$个，因为用$\\mathcal{H}$的基底向量表示，后面只能为$0$，将$\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}},\\dots, \\boldsymbol{u_{k}}$当作$\\mathbf{U}$的列，就有：\n$$ \\mathbf{RV} = \\begin{bmatrix} \\mathbf{U} \\\\ \\mathbf{0} \\end{bmatrix}, \\mathbf{U} \\in \\mathbb{R}^{k \\times k} $$\n显然，$\\mathcal{P}(\\boldsymbol{u_{1}}, \\dots)$与$\\mathcal{P}([\\boldsymbol{u_{1}};\\boldsymbol{0}],\\dots )$两者体积相等\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}},\\dots, \\boldsymbol{u_{k}})) = \\text{vol}\\bigg(\\mathcal{P}\\bigg(\\begin{bmatrix} \\boldsymbol{u_{1}} \\\\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{u_{2}} \\\\ \\boldsymbol{0} \\\\ \\end{bmatrix}, \\dots, \\begin{bmatrix} \\boldsymbol{u_{k}} \\\\ \\boldsymbol{0} \\end{bmatrix}\\bigg)\\bigg) $$\n那么：\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}}, \\dots, \\boldsymbol{u_{k}})) = \\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) $$\n由于对超平面体进行旋转不改变其体积（注意，这里是旋转而不是一般的线性变换，一般的线性变换不具备该性质）\n$$ \\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})\\bigg) $$\n那么：\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}}, \\dots, \\boldsymbol{u_{k}})) =\\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) $$\n又因为$\\mathbf{R}$正交矩阵，即$\\mathbf{R}^{\\top}\\mathbf{R} = \\mathbf{I}$，那么：\n$$ \\mathbf{U}^{\\top}\\mathbf{U} = (\\mathbf{RV})^{\\top}\\mathbf{RV} = \\mathbf{V}^{\\top}(\\mathbf{R}^{\\top}\\mathbf{R)}\\mathbf{V} = \\mathbf{V}^{\\top}\\mathbf{V} $$\n所以：\n$$ \\det(\\mathbf{V}^{\\top}\\mathbf{V}) = \\det(\\mathbf{U}^{\\top}\\mathbf{U}) = \\det(\\mathbf{U})^{2} = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})\\bigg)^{2} $$\n从 L-Emsemble 角度看，被采样的概率正比于构成的超平面体的体积，即两两之间线性无关更容易被采样出\nDemo 接下来我们用例子来看一下是否 DPP 能够采样出更有多样性的子集\nfrom torch import det, eye from transformers import set_seed from transformers import BertModel, BertTokenizer set_seed(42) pretrain_path = \"fabriceyhc/bert-base-uncased-imdb\" model = BertModel.from_pretrained(pretrain_path).cuda() tk = BertTokenizer.from_pretrained(pretrain_path) input_text = [ \"I am happy because the weather is extremely good!\", \"I hate the bad weather\", \"The weather is extremely good!\", ] inputs = tk(input_text, max_length=128, return_tensors=\"pt\", truncation=True, padding=True) inputs = {k: v.cuda() for k, v in inputs.items()} outputs = model(**inputs).pooler_output.T vtv = outputs.T @ outputs group_12 = vtv[:2][:, [0, 1]] I = eye(2).cuda() p_12 = det(group_12) / det(group_12 + I) group_13 = vtv[[0, 2]][:, [0, 2]] p_13 = det(group_13) / det(group_13 + I) print('采样到第一个和第二个的概率：%f'%p_12) print('采样到第一个和第三个的概率：%f'%p_13) # 采样到第一个和第二个的概率：0.983567 # 采样到第一个和第三个的概率：0.923823 然而，对于一个大小为$n$的集合，一共有$2^{n}$种组合，如何快速地进行 DPP 的计算以及如何最快找到大小为$k$的多样性最大的子集是比较困难的，留给下一篇 post\n","wordCount":"2889","inLanguage":"en","datePublished":"2023-04-21T15:20:00+08:00","dateModified":"2023-04-21T15:20:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://yunpengtai.top/posts/determinantal-point-process/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"http://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://yunpengtai.top/archives/ title=归档><span>归档</span></a></li><li><a href=http://yunpengtai.top/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=http://yunpengtai.top/categories/%E6%8A%98%E8%85%BE title=折腾><span>折腾</span></a></li><li><a href=http://yunpengtai.top/tags/ title=标签><span>标签</span></a></li><li><a href=http://yunpengtai.top/friends/ title=友人><span>友人</span></a></li><li><a href=http://yunpengtai.top/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=http://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>Determinantal Point Process</h1><div class=post-meta><span title='2023-04-21 15:20:00 +0800 CST'>April 21, 2023</span>&nbsp;·&nbsp;2889 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e8%be%b9%e7%bc%98%e5%88%86%e5%b8%83 aria-label=边缘分布>边缘分布</a></li><li><a href=#l-ensemble aria-label=L-Ensemble>L-Ensemble</a></li><li><a href=#%e7%9b%b4%e8%a7%82%e8%a7%a3%e9%87%8a aria-label=直观解释>直观解释</a></li><li><a href=#demo aria-label=Demo>Demo</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>在机器学习中，我们通常会面临一个问题：给定一个集合<code>$\mathbf{S}$</code>，从中寻找<code>$k$</code>个样本构成子集<code>$\mathbf{V}$</code>，尽量使得子集的质量高同时多样性好。比如在推荐系统中，我们就希望给用户推荐的东西尽可能的有质量，同时具有差异性。</p><p>而使得采样的子集尽可能具备多样性便是行列式点过程（Determinantal Point Process）大展身手的地方，俗称 DPP</p><h2 id=边缘分布>边缘分布<a hidden class=anchor aria-hidden=true href=#边缘分布>#</a></h2><p>首先引入 DPP 的边缘分布定义，当我们某次采样出子集<code>$\mathbf{A}$</code>，「包括」<code>$\mathbf{V} = [\boldsymbol{v_{1}},\boldsymbol{v_{2}},\dots,\boldsymbol{v_{k}}] \in \mathbb{R}^{{d \times k}}$</code>的概率：</p><p><code>$$ P(\mathbf{V} \subseteq \mathbf{A}) = \det(\mathbf{K_{V}}) $$</code></p><p><code>$\mathbf{K}$</code>是核矩阵（Kernel Matrix），即：</p><p><code>$$ \mathbf{K}_{ij} = k(\boldsymbol{v_{i}}, \boldsymbol{v_{j}}) $$</code></p><p><code>$\mathbf{K_{V}}$</code>是由<code>$\mathbf{V}$</code>中元素构成的子矩阵，举个例子，假如<code>$\mathbf{V}=\{ 1,2 \}$</code>，那么：</p><p><code>$$ P(\mathbf{V} \subseteq \mathbf{A}) = \det(\mathbf{K_{V}}) = \left|\begin{array}{cc} \mathbf{K}_{11} & \mathbf{K}_{12} \\ \mathbf{K}_{21} & \mathbf{K}_{22} \end{array}\right| = \mathbf{K}_{11}\mathbf{K}_{22} - \mathbf{K}_{12}^{2} $$</code></p><p>当<code>$\mathbf{K}_{12}$</code>越大，则<code>$\{ 1,2 \}$</code>同时出现在<code>$\mathbf{V}$</code>的概率就越小，从这个角度想，核函数应该是呈现出某种相似性</p><p>从正定性出发，严格的定义如下是：<code>$\mathbf{0}\preceq\mathbf{K} \preceq \mathbf{I}$</code></p><p>举个例子：</p><p><code>$$ \mathbf{K} = \begin{bmatrix} 1 & -0.3 \\ -0.3 & 1 \end{bmatrix} $$</code></p><p>其特征值为<code>$0.7, -1.3$</code>，不满足<code>$\mathbf{K}-\mathbf{0} \succeq \mathbf{0}$</code>，即不是半正定矩阵</p><h2 id=l-ensemble>L-Ensemble<a hidden class=anchor aria-hidden=true href=#l-ensemble>#</a></h2><p>然而，上面边缘定义只是告诉我们采样时，某个子集被「包括」的概率，并非就是这个子集，而这个问题可以通过 L-Ensemble 去解</p><p><code>$$ P(\mathbf{V}=\mathbf{A}) \propto \det(\mathbf{L}) $$</code></p><p>这里的<code>$\mathbf{L}$</code>省略了下标，跟上面的<code>$\mathbf{K}$</code>一样，是跟<code>$\mathbf{V}$</code>元素相关的子矩阵。<code>$\mathbf{L}$</code>矩阵的核函数是内积是<code>$\boldsymbol{v_{i}^{\top}\boldsymbol{v_{j}}}$</code>，<code>$\mathbf{V} = [\boldsymbol{v_{1}},\boldsymbol{v_{2}},\dots,\boldsymbol{v_{k}}] \in \mathbb{R}^{{d \times k}}$</code></p><p><code>$$ \mathbf{L} = \mathbf{V}^{\top}\mathbf{V} = \begin{bmatrix} \langle \boldsymbol{v_{1}}, \boldsymbol{v_{1}} \rangle & \langle \boldsymbol{v_{1}},\boldsymbol{v_{2}} \rangle & \dots & \langle \boldsymbol{v_{1}}, \boldsymbol{v_{k}} \rangle \\ \langle \boldsymbol{v_{2}},\boldsymbol{v_{1}} \rangle & \langle \boldsymbol{v_{2}},\boldsymbol{v_{2}} \rangle & \dots & \langle \boldsymbol{v_{2}},\boldsymbol{v_{k}} \rangle \\ \vdots & \vdots & \ddots & \vdots \\ \langle \boldsymbol{v_{k}},\boldsymbol{v_{1}} \rangle & \langle \boldsymbol{v_{k}},\boldsymbol{v_{2}} \rangle & \dots & \langle \boldsymbol{v_{k}},\boldsymbol{v_{k}} \rangle \end{bmatrix} $$</code></p><p>注意，这里指的不是概率，而是说概率「正比于」<code>$\mathbf{L}$</code>矩阵的行列式，那么如何计算概率呢？也就是说我们得计算一个归一化常数（normalization constant），可以类比抛硬币，我们得去求总的抛起次数，除以它才能得到概率</p><p>引入下述定理：</p><p><code>$$ \sum_{\mathbf{A}\subseteq \mathbf{V} \subseteq \mathbf{S}} \det(\mathbf{L}) = \det(\mathbf{L} + \mathbf{I_{\bar{A}}}) $$</code></p><p>其中<code>$\mathbf{I_{\bar{A}}}$</code>是将单位矩阵中与<code>$\mathbf{A}$</code>相关元素全部置零，举个例子，当<code>$\mathbf{S} = \{ 1,2,3 \}, \mathbf{A}={1,2}$</code>时：</p><p><code>$$ \mathbf{I_{\bar{A}}} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} $$</code></p><p>那么如何求归一化常数呢，即将<code>$\mathbf{A}=\emptyset$</code>，当<code>$\mathbf{A}$</code>为空集时，便包括了所有的情况，即：</p><p><code>$$ P(\mathbf{V}=\mathbf{A}) = \frac{\det(\mathbf{L})}{\det(\mathbf{L}+\mathbf{I})} $$</code></p><p>另外，L-Emsemble 的<code>$\mathbf{K}, \mathbf{L}$</code>对应关系如下：</p><p><code>$$ \begin{align} \mathbf{K} & = \mathbf{L}(\mathbf{L} + \mathbf{I})^{-1} \\ \mathbf{L} & = \mathbf{K}(\mathbf{I}-\mathbf{K})^{-1} \end{align} $$</code></p><h2 id=直观解释>直观解释<a hidden class=anchor aria-hidden=true href=#直观解释>#</a></h2><p>那么，行列式与多样性的直观解释是什么呢？</p><p>多样性和相似性的意思正好相反，通常我们会定义相似性为两个向量之间做点积，即为<code>$\boldsymbol{v_{1}}^{\top}\boldsymbol{v_{2}}$</code>，直观上看，两向量夹角的余弦值<code>$\cos \theta$</code> 越大，相似性越高，反过来看，当<code>$\cos \theta$</code>最小即为两者相似性最差，多样性最好。显然，当两向量正交时多样性最好。</p><p>那么，对于一个子集<code>$\mathbf{V} = [\boldsymbol{v_{1}},\boldsymbol{v_{2}},\dots,\boldsymbol{v_{k}}] \in \mathbb{R}^{{d \times k}}$</code>而言，该如何定义它的多样性呢？不难想出，可以通过线性无关向量的数量来定义，若两两都互不线性相关，此时的子集的多样性是最好的。直观上可以转换为构成的超平行体的体积，下方为<code>$k=2,3$</code>时的示意图</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2023/04/21/zFmw6A3LSoWxNlD.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2023/04/21/zFmw6A3LSoWxNlD.png#center alt=图源自王树森老师的课程 width=600px height=250px><figcaption><p><a href="https://www.youtube.com/watch?v=HjpJeUSekKs&amp;list=PLvOO0btloRntAi-VnV06M1Bu0X1xljUUP&amp;index=30">图源自王树森老师的课程</a></p></figcaption></figure></a><p>为什么呢？可以拿平行六面体为例，若其中一个向量与其他向量线性相关，那么则会坍缩成一个平面，构不成平行六面体</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2023/04/21/6pE7uKy8PFaoHjR.gif><figure class=align-center><img loading=lazy src=https://s2.loli.net/2023/04/21/6pE7uKy8PFaoHjR.gif#center alt="图源自 3BlueBrown 对于行列式的介绍" width=500px height=300px><figcaption><p><a href="https://www.youtube.com/watch?v=Ip3X9LOh2dk">图源自 3BlueBrown 对于行列式的介绍</a></p></figcaption></figure></a><div class="notice notice-note"><div class=notice-title><svg xmlns="http://www.w3.org/2000/svg" class="icon notice-icon" viewBox="0 0 512 512"><path d="M504 256A248 248 0 118 256a248 248 0 01496 0zm-248 50a46 46 0 100 92 46 46 0 000-92zm-44-165 8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z"/></svg></div><p>只有当所有向量两两都线性无关时，构成的超平行体体积最大，即多样性最好</p></div><p>而行列式可以表示体积，下式中<code>$\text{vol}$</code>代表体积（volume），此时<code>$k=d$</code>，<code>$\mathbf{V}$</code>为方阵</p><p><code>$$ \det(\mathbf{V})= \text{vol}(\mathcal{P}(\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \dots, \boldsymbol{v_{k}})) $$</code></p><p>也就是说，我们可以通过行列式的大小来定义多样性</p><p>那么，<code>$\mathbf{L}$</code>的行列式是否也跟体积有关呢？答案是肯定的：</p><p><code>$$ \det(\mathbf{L}) = \text{vol}\bigg(\mathcal{P}(\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \dots,\boldsymbol{v_{k}})\bigg)^{2} $$</code></p><p>接下来证明这一结论：</p><p>由于<code>$k \leq d$</code>，因为<code>$d$</code>维空间至多存在<code>$d$</code>个两两线性无关的向量，那么肯定存在一个<code>$k$</code>维子空间<code>$\mathcal{H}$</code>，存在正交矩阵<code>$\mathbf{R} \in \mathbb{R}^{d \times d}$</code>，对向量<code>$\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \dots, \boldsymbol{v_{k}}$</code>进行旋转，使得<code>$\mathbf{R}\boldsymbol{v_{1}}, \mathbf{R}\boldsymbol{v_{2}}, \dots, \mathbf{R}\boldsymbol{v_{k}}$</code>都落在子空间<code>$\mathcal{H}$</code>上。不妨设<code>$\mathcal{H}$</code>的基底是前<code>$k$</code>个标准正交基，那么：</p><p><code>$$ \mathbf{R}\boldsymbol{v_{i}} = \begin{bmatrix} \boldsymbol{u_{i}} \\ \mathbf{0} \end{bmatrix} $$</code></p><p><code>$\boldsymbol{u_{i}} \in \mathbb{R}^{k}$</code>，<code>$\mathbf{0}$</code>一共有<code>$d-k$</code>个，因为用<code>$\mathcal{H}$</code>的基底向量表示，后面只能为<code>$0$</code>，将<code>$\boldsymbol{u_{1}}, \boldsymbol{u_{2}},\dots, \boldsymbol{u_{k}}$</code>当作<code>$\mathbf{U}$</code>的列，就有：</p><p><code>$$ \mathbf{RV} = \begin{bmatrix} \mathbf{U} \\ \mathbf{0} \end{bmatrix}, \mathbf{U} \in \mathbb{R}^{k \times k} $$</code></p><p>显然，<code>$\mathcal{P}(\boldsymbol{u_{1}}, \dots)$</code>与<code>$\mathcal{P}([\boldsymbol{u_{1}};\boldsymbol{0}],\dots )$</code>两者体积相等</p><p><code>$$ \text{vol}(\mathcal{P}(\boldsymbol{u_{1}}, \boldsymbol{u_{2}},\dots, \boldsymbol{u_{k}})) = \text{vol}\bigg(\mathcal{P}\bigg(\begin{bmatrix} \boldsymbol{u_{1}} \\ \boldsymbol{0} \end{bmatrix}, \begin{bmatrix} \boldsymbol{u_{2}} \\ \boldsymbol{0} \\ \end{bmatrix}, \dots, \begin{bmatrix} \boldsymbol{u_{k}} \\ \boldsymbol{0} \end{bmatrix}\bigg)\bigg) $$</code></p><p>那么：</p><p><code>$$ \text{vol}(\mathcal{P}(\boldsymbol{u_{1}}, \boldsymbol{u_{2}}, \dots, \boldsymbol{u_{k}})) = \text{vol}\bigg(\mathcal{P}(\mathbf{R}\boldsymbol{v_{1}}, \mathbf{R}\boldsymbol{v_{2}}, \dots, \mathbf{R}\boldsymbol{v_{k}})\bigg) $$</code></p><p>由于对超平面体进行旋转不改变其体积（注意，这里是旋转而不是一般的线性变换，一般的线性变换不具备该性质）</p><p><code>$$ \text{vol}\bigg(\mathcal{P}(\mathbf{R}\boldsymbol{v_{1}}, \mathbf{R}\boldsymbol{v_{2}}, \dots, \mathbf{R}\boldsymbol{v_{k}})\bigg) = \text{vol}\bigg(\mathcal{P}(\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \dots, \boldsymbol{v_{k}})\bigg) $$</code></p><p>那么：</p><p><code>$$ \text{vol}(\mathcal{P}(\boldsymbol{u_{1}}, \boldsymbol{u_{2}}, \dots, \boldsymbol{u_{k}})) =\text{vol}\bigg(\mathcal{P}(\mathbf{R}\boldsymbol{v_{1}}, \mathbf{R}\boldsymbol{v_{2}}, \dots, \mathbf{R}\boldsymbol{v_{k}})\bigg) $$</code></p><p>又因为<code>$\mathbf{R}$</code>正交矩阵，即<code>$\mathbf{R}^{\top}\mathbf{R} = \mathbf{I}$</code>，那么：</p><p><code>$$ \mathbf{U}^{\top}\mathbf{U} = (\mathbf{RV})^{\top}\mathbf{RV} = \mathbf{V}^{\top}(\mathbf{R}^{\top}\mathbf{R)}\mathbf{V} = \mathbf{V}^{\top}\mathbf{V} $$</code></p><p>所以：</p><p><code>$$ \det(\mathbf{V}^{\top}\mathbf{V}) = \det(\mathbf{U}^{\top}\mathbf{U}) = \det(\mathbf{U})^{2} = \text{vol}\bigg(\mathcal{P}(\boldsymbol{v_{1}}, \boldsymbol{v_{2}}, \dots, \boldsymbol{v_{k}})\bigg)^{2} $$</code></p><p>从 L-Emsemble 角度看，被采样的概率正比于构成的超平面体的体积，即两两之间线性无关更容易被采样出</p><h2 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h2><p>接下来我们用例子来看一下是否 DPP 能够采样出更有多样性的子集</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>det</span><span class=p>,</span> <span class=n>eye</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>set_seed</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertModel</span><span class=p>,</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>set_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pretrain_path</span> <span class=o>=</span> <span class=s2>&#34;fabriceyhc/bert-base-uncased-imdb&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BertModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrain_path</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tk</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrain_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>input_text</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;I am happy because the weather is extremely good!&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;I hate the bad weather&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The weather is extremely good!&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tk</span><span class=p>(</span><span class=n>input_text</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>v</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span><span class=o>.</span><span class=n>pooler_output</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>vtv</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>outputs</span>
</span></span><span class=line><span class=cl><span class=n>group_12</span> <span class=o>=</span> <span class=n>vtv</span><span class=p>[:</span><span class=mi>2</span><span class=p>][:,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>I</span> <span class=o>=</span> <span class=n>eye</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>p_12</span> <span class=o>=</span> <span class=n>det</span><span class=p>(</span><span class=n>group_12</span><span class=p>)</span> <span class=o>/</span> <span class=n>det</span><span class=p>(</span><span class=n>group_12</span> <span class=o>+</span> <span class=n>I</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>group_13</span> <span class=o>=</span> <span class=n>vtv</span><span class=p>[[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>]][:,</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>p_13</span> <span class=o>=</span> <span class=n>det</span><span class=p>(</span><span class=n>group_13</span><span class=p>)</span> <span class=o>/</span> <span class=n>det</span><span class=p>(</span><span class=n>group_13</span> <span class=o>+</span> <span class=n>I</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;采样到第一个和第二个的概率：</span><span class=si>%f</span><span class=s1>&#39;</span><span class=o>%</span><span class=n>p_12</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;采样到第一个和第三个的概率：</span><span class=si>%f</span><span class=s1>&#39;</span><span class=o>%</span><span class=n>p_13</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 采样到第一个和第二个的概率：0.983567</span>
</span></span><span class=line><span class=cl><span class=c1># 采样到第一个和第三个的概率：0.923823</span>
</span></span></code></pre></div><p>然而，对于一个大小为<code>$n$</code>的集合，一共有<code>$2^{n}$</code>种组合，如何快速地进行 DPP 的计算以及如何最快找到大小为<code>$k$</code>的多样性最大的子集是比较困难的，留给下一篇 post</p></div><div style="margin-top:2em;padding:1em;border:0 solid;border-radius:10px;background-color:var(--code-bg)"><h3>如果您想要引用，请考虑如下格式：</h3><div style=padding-top:.5em>台运鹏. (Apr. 21, 2023). 《Determinantal Point Process》[Blog
post]. Retrieved from http://yunpengtai.top/posts/determinantal-point-process/</div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>@online{blog-a1f3ef8572fbb0abe2ce90766c3ead26,
</span></span><span class=line><span class=cl>        title={Determinantal Point Process},
</span></span><span class=line><span class=cl>        author={Yunpeng Tai},
</span></span><span class=line><span class=cl>        year={2023},
</span></span><span class=line><span class=cl>        month={Apr},
</span></span><span class=line><span class=cl>        note={http://yunpengtai.top/posts/determinantal-point-process/},
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div><div style=padding-bottom:.4em>自由转载-非商用-非衍生-保持署名（<a href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-SA 4.0）</a></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://yunpengtai.top/tags/ml/>ML</a></li></ul><nav class=paginav><a class=prev href=http://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/><span class=title>« Prev</span><br><span>Fast Greedy MAP Inference for DPP</span></a>
<a class=next href=http://yunpengtai.top/posts/generalized-linear-models/><span class=title>Next »</span><br><span>Generalized Linear Models</span></a></nav></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/artalk@2.8.6/dist/Artalk.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/@artalk/plugin-katex@0.2.4/dist/artalk-plugin-katex.min.js></script><div id=Comments></div><script>const savedTheme=localStorage.getItem("pref-theme");let darkMode="auto";savedTheme!==null&&(darkMode=savedTheme==="dark");const artalk=Artalk.init({el:"#Comments",pageKey:"",pageTitle:"Determinantal Point Process",server:"https://comment.yunpengtai.top",site:"Tai's Blog",darkMode,versionCheck:!1});document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?artalk.setDarkMode(!1):artalk.setDarkMode(!0)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=http://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.min.js",function(){pangu.spacingPage()})</script><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>