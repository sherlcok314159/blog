<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>放大镜下的 InfoNCE | Tai's Blog</title><meta name=keywords content="contrastive learning"><meta name=description content="区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，"><meta name=author content><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="eAKh7zszsOtNde1wyq_sUo95ZPH4zTTJhR-_ol4VWDs"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://yunpengtai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yunpengtai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yunpengtai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://yunpengtai.top/apple-touch-icon.png><link rel=mask-icon href=https://yunpengtai.top/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={loader:{load:["[tex]/boldsymbol"]},tex:{tags:"all",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:{"[+]":["boldsymbol"]}}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js integrity="sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y3CX2RWEDY",{anonymize_ip:!1})}</script><meta property="og:title" content="放大镜下的 InfoNCE"><meta property="og:description" content="区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，"><meta property="og:type" content="article"><meta property="og:url" content="https://yunpengtai.top/posts/infonce/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-14T18:02:00+08:00"><meta property="article:modified_time" content="2023-07-14T18:02:00+08:00"><meta property="og:site_name" content="Tai's Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yunpengtai.top/posts/"},{"@type":"ListItem","position":3,"name":"放大镜下的 InfoNCE","item":"https://yunpengtai.top/posts/infonce/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"放大镜下的 InfoNCE","name":"放大镜下的 InfoNCE","description":"区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，","keywords":["contrastive learning"],"articleBody":"区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，直接采用自归一化，即：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{1} $$\n首先对于每个训练样本来说，我们再从噪声分布$q(x)$中采样$k-1$个样本，我们的目的即是区分出真实样本：\n$$ \\begin{align} L(\\boldsymbol{\\theta}) \u0026 = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w}|\\boldsymbol{c})+\\sum_{j=1}^{k-1}q(\\boldsymbol{w}_{j})}\\\\ \u0026 = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j} \\sim q} \\log \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}) + \\sum_{j=1}^{k-1} q(\\boldsymbol{w}_{j})} \\end{align} $$\n最小化上述 loss 到底有什么好处呢？\n互信息来访 对于训练样本$i$来说，它是真实样本的概率为：\n$$ \\begin{align} p(d=i) \u0026 = \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})\\prod_{j=1, i\\neq j}^{k}q(\\boldsymbol{w}_{j})}{\\sum_{j=1}^{k}p(\\boldsymbol{w}_{j}|\\boldsymbol{c})\\prod_{l=1,l\\neq j}^{k}q(\\boldsymbol{w}_{l})} \\\\ \u0026= \\frac{\\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{i})}}{\\sum_{j=1}^{k} \\frac{p(\\boldsymbol{w}_{j}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{j})}} \\end{align} $$\n转换的方法即是上下同除以$\\prod_{i=1}^{k} q(\\boldsymbol{w}_{i})$\nInfoNCE 的目的即为正确区分出真实样本，而真实样本都来自训练集（与噪声集区分），也就是说对于 loss 而言，最优解即为$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$正比于 density ratio：\n$$ u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{i})} $$\n我们知道，互信息的计算如下：\n$$ I(\\boldsymbol{w}, \\boldsymbol{c}) = \\text{KL}(p(\\boldsymbol{w}, \\boldsymbol{c})\\|p(\\boldsymbol{w})p(\\boldsymbol{c})) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}, \\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})} $$\n为了和上述 loss 有关，很自然做出以下变化：\n$$ I(\\boldsymbol{w}, \\boldsymbol{c}) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})p(\\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})} = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log {\\color{blue}\\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w})}} $$\n也就是说如果我们要$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$来表示互信息，还得$q(x) = p(x)$，也就是噪声分布即为模型分布（unconditioned）\n那么：\n$$ u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{{\\color{blue}p(\\boldsymbol{w}_{i})}} $$\n此时完整的 loss 即为：\n$$ L(\\boldsymbol{\\theta }) = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{j=1}^{k} u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{j}, \\boldsymbol{c})} $$\n当我们要使得训练样本$\\boldsymbol{w}$是真实样本的概率变得最大，即一定程度最大化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$，然后最小化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{j}, \\boldsymbol{c})$，即最大化$\\boldsymbol{w}, \\boldsymbol{c}$之间的互信息，最小化负样本与$\\boldsymbol{c}$之间的互信息\n注意：现在用的时候并非像一开始通过真实样本的概念，而是可以将$\\boldsymbol{w}, \\boldsymbol{c}$看成是一对正样本，噪声样本看作负样本\nHardness-Aware 同时 InfoNCE 会着重关照那些跟真实样本$\\boldsymbol{c}$更相似的负样本，这类也叫做「Hard-Negative」，而相应地会轻视一些与$\\boldsymbol{c}$本来就没有那么相似的负样本，即「Hardness-Aware」\n$$ \\begin{align} L(\\boldsymbol{\\theta }) \u0026 = - \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j}\\sim q} \\log \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})+\\sum_{j=1}^{k-1} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{j}, \\boldsymbol{c})} \\\\ \u0026= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j}\\sim q} \\log \\frac{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}} )}{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}})} \\\\ \u0026= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j} \\sim q} \\left[s_{\\boldsymbol{w}, \\boldsymbol{c}} - \\log\\left( \\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j},\\boldsymbol{c}}) \\right)\\right] \\end{align} $$\n对于$\\boldsymbol{w}_{j}$而言，与$\\boldsymbol{c}$的相似性越大，则在损失函数中占比越大，可以看成是惩罚越大，然而仅凭这个没办法做到上述性质，比如下面的 loss 也可以：\n$$ L_{\\text{simple}}(\\boldsymbol{\\theta }) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\left[-s_{\\boldsymbol{w}, \\boldsymbol{c}} + \\sum_{j=1}^{k-1} s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}\\right] $$\n然而实验证明，这个 loss 效果并不是很好\n接着，我们来求下损失函数相对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的偏导：\n不妨就对单个样本$\\boldsymbol{w}, \\boldsymbol{c}$来看：\n$$ \\frac{ \\partial L_{\\text{simple}} }{ \\partial s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}} } = 1 $$\n也就是说 loss 对于所有$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化一样敏感，即「一视同仁」，没有给损失函数的优化注入其他信息\n$$ \\frac{ \\partial L }{ \\partial s_{\\boldsymbol{w}_{j},\\boldsymbol{c}} } = \\frac{\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}})}{\\exp(s_{\\boldsymbol{w},\\boldsymbol{c}} )+ \\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, c})} $$\n而对于 InfoNCE 而言，对于所有负样本而言，分母是固定的，而「分子越大」，即$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$越大，则此时 loss 对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化则越敏感\n总结说就是，InfoNCE 这种 loss 在优化时会注入一种偏好，更偏向于把与真实样本相似的噪声样本给区分开来\n温度系数 一般现在用的时候，还会加入一个调节因子：$\\tau$\n$$ L(\\boldsymbol{\\theta})= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau )}{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau)+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}/\\tau)} $$\n那么：\n$$ \\frac{ \\partial L }{ \\partial s_{\\boldsymbol{w}_{j},\\boldsymbol{c}} } = \\frac{1}{\\tau}\\frac{\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}/\\tau)}{\\exp(s_{\\boldsymbol{w},\\boldsymbol{c}}/\\tau )+ \\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, c}/\\tau)} $$\n推论 1：当$\\tau$很小时，比如$0.02$那么由上式可得 loss 会格外关注于将与真实样本相似的噪声样本给区分开。换句话说，$\\tau$越小，对 hard-negative 的惩罚越大\n下面我们引入一个 kernel 来衡量一个分布的均匀性（uniform），$f(\\boldsymbol{x})$代表模型输出的表示\n$$ L_{\\text{uniformity}}(f;t) = \\log \\, \\mathbb{E}_{\\boldsymbol{x}, \\boldsymbol{y} \\sim \\mathcal{D}} \\left[e^{-t\\|f(\\boldsymbol{x}) - f(\\boldsymbol{y})\\|_{2}^{2}}\\right](t \\in \\mathbb{R}_{+}) $$\n在Wang et., al中，作者采用不同的$\\tau$来对比模型产生表示的均匀性\n当$\\tau$比较小时，模型产生的表征空间更加均匀，随着$\\tau$的增大，均匀性被破坏\n而对于一个好的对比学习的表征空间应该满足「locally clustered，globally uniform」，比如下图，当表征均匀分布在球面上时，此时用一个超平面（线性分类器）便可分开\n推论 2：当$\\tau$较小时，用 infoNCE loss 训出的模型表征空间比较均匀，当$\\tau$增大时，表征空间的均匀性被逐步破坏\n上代码 当下的算力已经足够支撑直接用概率而非自归一化了，其实会发现 InfoNCE loss 跟多分类交叉熵损失一样：\nimport torch.nn.functional as F def infoNCE(q_vectors, c_vectors, labels, tau=1): \"\"\"Compute the InfoNCE Loss in http://arxiv.org/abs/1807.03748. Params: - q_vectors: Tensor. Shape: (bs, d_model). Vector presentation of query. - c_vectors: Tensor. Shape: (num_pos_neg_contexts, d_model). Vector presentation of context. - labels: Tensor. Shape: (bs, ). Indices for the positive contexts. - tau: Scalar. The parameter to control penalty on hard negatives (smaller =\u003e harder). A good initialization value can be 0.02 or 0.05. Examples: \u003e\u003e\u003e q_vectors = torch.randn((2, 4)) \u003e\u003e\u003e c_vectors = torch.randn((4, 4)) # each query has 4 contexts \u003e\u003e\u003e labels = torch.tensor([1, 3]) \u003e\u003e\u003e print(infoNCE(q_vectors, c_vectors, labels)) \"\"\" scores = (q_vectors @ c_vectors.T) / tau return F.cross_entropy(scores, labels) ","wordCount":"2196","inLanguage":"en","datePublished":"2023-07-14T18:02:00+08:00","dateModified":"2023-07-14T18:02:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunpengtai.top/posts/infonce/"},"publisher":{"@type":"Organization","name":"Tai's Blog","logo":{"@type":"ImageObject","url":"https://yunpengtai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yunpengtai.top accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://yunpengtai.top/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yunpengtai.top/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yunpengtai.top/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yunpengtai.top/friends/ title=Friends><span>Friends</span></a></li><li><a href=https://yunpengtai.top/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yunpengtai.top>Home</a>&nbsp;»&nbsp;<a href=https://yunpengtai.top/posts/>Posts</a></div><h1 class=post-title>放大镜下的 InfoNCE</h1><div class=post-meta><span title='2023-07-14 18:02:00 +0800 CST'>July 14, 2023</span>&nbsp;·&nbsp;2196 words</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8c%ba%e5%88%86%e7%9c%9f%e5%ae%9e%e6%a0%b7%e6%9c%ac aria-label=区分真实样本>区分真实样本</a></li><li><a href=#%e4%ba%92%e4%bf%a1%e6%81%af%e6%9d%a5%e8%ae%bf aria-label=互信息来访>互信息来访</a></li><li><a href=#hardness-aware aria-label=Hardness-Aware>Hardness-Aware</a></li><li><a href=#%e6%b8%a9%e5%ba%a6%e7%b3%bb%e6%95%b0 aria-label=温度系数>温度系数</a></li><li><a href=#%e4%b8%8a%e4%bb%a3%e7%a0%81 aria-label=上代码>上代码</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h3 id=区分真实样本>区分真实样本<a hidden class=anchor aria-hidden=true href=#区分真实样本>#</a></h3><p>前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，直接采用自归一化，即：</p><p><code>$$ p(\boldsymbol{w}|\boldsymbol{c}) = \frac{u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})}{1} $$</code></p><p>首先对于每个训练样本来说，我们再从噪声分布<code>$q(x)$</code>中采样<code>$k-1$</code>个样本，我们的目的即是区分出真实样本：</p><p><code>$$ \begin{align} L(\boldsymbol{\theta}) & = -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}; \boldsymbol{w}_{j} \sim q} \log \frac{p(\boldsymbol{w}|\boldsymbol{c})}{p(\boldsymbol{w}|\boldsymbol{c})+\sum_{j=1}^{k-1}q(\boldsymbol{w}_{j})}\\ & = -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D};\boldsymbol{w}_{j} \sim q} \log \frac{u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})}{u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c}) + \sum_{j=1}^{k-1} q(\boldsymbol{w}_{j})} \end{align} $$</code></p><p>最小化上述 loss 到底有什么好处呢？</p><h3 id=互信息来访>互信息来访<a hidden class=anchor aria-hidden=true href=#互信息来访>#</a></h3><p>对于训练样本<code>$i$</code>来说，它是真实样本的概率为：</p><p><code>$$ \begin{align} p(d=i) & = \frac{p(\boldsymbol{w}_{i}|\boldsymbol{c})\prod_{j=1, i\neq j}^{k}q(\boldsymbol{w}_{j})}{\sum_{j=1}^{k}p(\boldsymbol{w}_{j}|\boldsymbol{c})\prod_{l=1,l\neq j}^{k}q(\boldsymbol{w}_{l})} \\ &= \frac{\frac{p(\boldsymbol{w}_{i}|\boldsymbol{c})}{q(\boldsymbol{w}_{i})}}{\sum_{j=1}^{k} \frac{p(\boldsymbol{w}_{j}|\boldsymbol{c})}{q(\boldsymbol{w}_{j})}} \end{align} $$</code></p><p>转换的方法即是上下同除以<code>$\prod_{i=1}^{k} q(\boldsymbol{w}_{i})$</code></p><p>InfoNCE 的目的即为正确区分出真实样本，而真实样本都来自训练集（与噪声集区分），也就是说对于 loss 而言，最优解即为<code>$u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})$</code><strong>正比于</strong> density ratio：</p><p><code>$$ u_{\boldsymbol{\theta}}(\boldsymbol{w}_{i}, \boldsymbol{c}) \propto \frac{p(\boldsymbol{w}_{i}|\boldsymbol{c})}{q(\boldsymbol{w}_{i})} $$</code></p><p>我们知道，互信息的计算如下：</p><p><code>$$ I(\boldsymbol{w}, \boldsymbol{c}) = \text{KL}(p(\boldsymbol{w}, \boldsymbol{c})\|p(\boldsymbol{w})p(\boldsymbol{c})) = \mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}} \log \frac{p(\boldsymbol{w}, \boldsymbol{c})}{p(\boldsymbol{w})p(\boldsymbol{c})} $$</code></p><p>为了和上述 loss 有关，很自然做出以下变化：</p><p><code>$$ I(\boldsymbol{w}, \boldsymbol{c}) = \mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}} \log \frac{p(\boldsymbol{w}|\boldsymbol{c})p(\boldsymbol{c})}{p(\boldsymbol{w})p(\boldsymbol{c})} = \mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}} \log {\color{blue}\frac{p(\boldsymbol{w}|\boldsymbol{c})}{p(\boldsymbol{w})}} $$</code></p><p>也就是说如果我们要<code>$u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})$</code>来表示互信息，还得<code>$q(x) = p(x)$</code>，也就是噪声分布即为模型分布（unconditioned）</p><p>那么：</p><p><code>$$ u_{\boldsymbol{\theta}}(\boldsymbol{w}_{i}, \boldsymbol{c}) \propto \frac{p(\boldsymbol{w}_{i}|\boldsymbol{c})}{{\color{blue}p(\boldsymbol{w}_{i})}} $$</code></p><p>此时完整的 loss 即为：</p><p><code>$$ L(\boldsymbol{\theta }) = -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}; \boldsymbol{w}_{j} \sim q} \log \frac{u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})}{\sum_{j=1}^{k} u_{\boldsymbol{\theta}}(\boldsymbol{w}_{j}, \boldsymbol{c})} $$</code></p><p>当我们要使得训练样本<code>$\boldsymbol{w}$</code>是真实样本的概率变得最大，即一定程度最大化<code>$u_{\boldsymbol{\theta}}(\boldsymbol{w}, \boldsymbol{c})$</code>，然后最小化<code>$u_{\boldsymbol{\theta}}(\boldsymbol{w}_{j}, \boldsymbol{c})$</code>，即最大化<code>$\boldsymbol{w}, \boldsymbol{c}$</code>之间的互信息，最小化负样本与<code>$\boldsymbol{c}$</code>之间的互信息</p><p>注意：现在用的时候并非像一开始通过真实样本的概念，而是可以将<code>$\boldsymbol{w}, \boldsymbol{c}$</code>看成是一对正样本，噪声样本看作负样本</p><h3 id=hardness-aware>Hardness-Aware<a hidden class=anchor aria-hidden=true href=#hardness-aware>#</a></h3><p>同时 InfoNCE 会<strong>着重</strong>关照那些跟真实样本<code>$\boldsymbol{c}$</code>更相似的负样本，这类也叫做「Hard-Negative」，而相应地会轻视一些与<code>$\boldsymbol{c}$</code>本来就没有那么相似的负样本，即「Hardness-Aware」</p><p><code>$$ \begin{align} L(\boldsymbol{\theta }) & = - \mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D};\boldsymbol{w}_{j}\sim q} \log \frac{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})}{u_{\boldsymbol{\theta }}(\boldsymbol{w}, \boldsymbol{c})+\sum_{j=1}^{k-1} u_{\boldsymbol{\theta }}(\boldsymbol{w}_{j}, \boldsymbol{c})} \\ &= -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D};\boldsymbol{w}_{j}\sim q} \log \frac{\exp(s_{\boldsymbol{w}, \boldsymbol{c}} )}{\exp(s_{\boldsymbol{w}, \boldsymbol{c}})+\sum_{j=1}^{k-1}\exp(s_{\boldsymbol{w}_{j}, \boldsymbol{c}})} \\ &= -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D};\boldsymbol{w}_{j} \sim q} \left[s_{\boldsymbol{w}, \boldsymbol{c}} - \log\left( \exp(s_{\boldsymbol{w}, \boldsymbol{c}})+\sum_{j=1}^{k-1}\exp(s_{\boldsymbol{w}_{j},\boldsymbol{c}}) \right)\right] \end{align} $$</code></p><p>对于<code>$\boldsymbol{w}_{j}$</code>而言，与<code>$\boldsymbol{c}$</code>的相似性越大，则在损失函数中占比越大，可以看成是惩罚越大，然而仅凭这个没办法做到上述性质，比如下面的 loss 也可以：</p><p><code>$$ L_{\text{simple}}(\boldsymbol{\theta }) = \mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}; \boldsymbol{w}_{j} \sim q} \left[-s_{\boldsymbol{w}, \boldsymbol{c}} + \sum_{j=1}^{k-1} s_{\boldsymbol{w}_{j}, \boldsymbol{c}}\right] $$</code></p><p>然而实验证明，这个 loss 效果并不是很好</p><p>接着，我们来求下损失函数相对于<code>$s_{\boldsymbol{w}_{j}, \boldsymbol{c}}$</code>的偏导：</p><p>不妨就对单个样本<code>$\boldsymbol{w}, \boldsymbol{c}$</code>来看：</p><p><code>$$ \frac{ \partial L_{\text{simple}} }{ \partial s_{\boldsymbol{w}_{j}, \boldsymbol{c}} } = 1 $$</code></p><p>也就是说 loss 对于所有<code>$s_{\boldsymbol{w}_{j}, \boldsymbol{c}}$</code>的变化一样敏感，即「一视同仁」，没有给损失函数的优化注入其他信息</p><p><code>$$ \frac{ \partial L }{ \partial s_{\boldsymbol{w}_{j},\boldsymbol{c}} } = \frac{\exp(s_{\boldsymbol{w}_{j}, \boldsymbol{c}})}{\exp(s_{\boldsymbol{w},\boldsymbol{c}} )+ \sum_{j=1}^{k-1}\exp(s_{\boldsymbol{w}_{j}, c})} $$</code></p><p>而对于 InfoNCE 而言，对于所有负样本而言，分母是固定的，而「分子越大」，即<code>$s_{\boldsymbol{w}_{j}, \boldsymbol{c}}$</code>越大，则此时 loss 对于<code>$s_{\boldsymbol{w}_{j}, \boldsymbol{c}}$</code>的变化则越敏感</p><p>总结说就是，InfoNCE 这种 loss 在优化时会注入一种偏好，更偏向于把与真实样本相似的噪声样本给区分开来</p><h3 id=温度系数>温度系数<a hidden class=anchor aria-hidden=true href=#温度系数>#</a></h3><p>一般现在用的时候，还会加入一个调节因子：<code>$\tau$</code></p><p><code>$$ L(\boldsymbol{\theta})= -\mathbb{E}_{\boldsymbol{w}, \boldsymbol{c} \sim \mathcal{D}; \boldsymbol{w}_{j} \sim q} \log \frac{\exp(s_{\boldsymbol{w}, \boldsymbol{c}}/\tau )}{\exp(s_{\boldsymbol{w}, \boldsymbol{c}}/\tau)+\sum_{j=1}^{k-1}\exp(s_{\boldsymbol{w}_{j}, \boldsymbol{c}}/\tau)} $$</code></p><p>那么：</p><p><code>$$ \frac{ \partial L }{ \partial s_{\boldsymbol{w}_{j},\boldsymbol{c}} } = \frac{1}{\tau}\frac{\exp(s_{\boldsymbol{w}_{j}, \boldsymbol{c}}/\tau)}{\exp(s_{\boldsymbol{w},\boldsymbol{c}}/\tau )+ \sum_{j=1}^{k-1}\exp(s_{\boldsymbol{w}_{j}, c}/\tau)} $$</code></p><blockquote><p><strong>推论 1</strong>：当<code>$\tau$</code>很小时，比如<code>$0.02$</code>那么由上式可得 loss 会格外关注于将与真实样本相似的噪声样本给区分开。换句话说，<code>$\tau$</code>越小，对 hard-negative 的惩罚越大</p></blockquote><p>下面我们引入一个 kernel 来衡量一个分布的均匀性（uniform），<code>$f(\boldsymbol{x})$</code>代表模型输出的表示</p><p><code>$$ L_{\text{uniformity}}(f;t) = \log \, \mathbb{E}_{\boldsymbol{x}, \boldsymbol{y} \sim \mathcal{D}} \left[e^{-t\|f(\boldsymbol{x}) - f(\boldsymbol{y})\|_{2}^{2}}\right](t \in \mathbb{R}_{+}) $$</code></p><p>在<a href=http://arxiv.org/abs/2012.09740>Wang et., al</a>中，作者采用不同的<code>$\tau$</code>来对比模型产生表示的均匀性</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2023/07/14/NqJHTufMDeYsgw7.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2023/07/14/NqJHTufMDeYsgw7.png#center width=650 height=300></figure></a><p>当<code>$\tau$</code>比较小时，模型产生的表征空间更加均匀，随着<code>$\tau$</code>的增大，均匀性被破坏</p><p>而对于一个好的对比学习的表征空间应该满足「locally clustered，globally uniform」，比如<a href=http://arxiv.org/abs/2005.10242>下图</a>，当表征均匀分布在球面上时，此时用一个超平面（线性分类器）便可分开</p><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script>
<a data-fancybox=gallery href=https://s2.loli.net/2023/07/14/PrJYciVyDloQNWM.png><figure class=align-center><img loading=lazy src=https://s2.loli.net/2023/07/14/PrJYciVyDloQNWM.png#center width=440 height=440></figure></a><blockquote><p><strong>推论 2</strong>：当<code>$\tau$</code>较小时，用 infoNCE loss 训出的模型表征空间比较<strong>均匀</strong>，当<code>$\tau$</code>增大时，表征空间的均匀性被逐步破坏</p></blockquote><h3 id=上代码>上代码<a hidden class=anchor aria-hidden=true href=#上代码>#</a></h3><p>当下的算力已经足够支撑直接用概率而非自归一化了，其实会发现 InfoNCE loss 跟多分类交叉熵损失一样：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>infoNCE</span><span class=p>(</span><span class=n>q_vectors</span><span class=p>,</span> <span class=n>c_vectors</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>tau</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compute the InfoNCE Loss in
</span></span></span><span class=line><span class=cl><span class=s2>    http://arxiv.org/abs/1807.03748.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Params:
</span></span></span><span class=line><span class=cl><span class=s2>        - q_vectors: Tensor. Shape: (bs, d_model). Vector presentation of query.
</span></span></span><span class=line><span class=cl><span class=s2>        - c_vectors: Tensor. Shape: (num_pos_neg_contexts, d_model). Vector presentation of context.
</span></span></span><span class=line><span class=cl><span class=s2>        - labels: Tensor. Shape: (bs, ). Indices for the positive contexts.
</span></span></span><span class=line><span class=cl><span class=s2>        - tau: Scalar. The parameter to control penalty on hard negatives (smaller =&gt; harder).
</span></span></span><span class=line><span class=cl><span class=s2>               A good initialization value can be 0.02 or 0.05.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Examples:
</span></span></span><span class=line><span class=cl><span class=s2>        &gt;&gt;&gt; q_vectors = torch.randn((2, 4))
</span></span></span><span class=line><span class=cl><span class=s2>        &gt;&gt;&gt; c_vectors = torch.randn((4, 4)) # each query has 4 contexts
</span></span></span><span class=line><span class=cl><span class=s2>        &gt;&gt;&gt; labels = torch.tensor([1, 3])
</span></span></span><span class=line><span class=cl><span class=s2>        &gt;&gt;&gt; print(infoNCE(q_vectors, c_vectors, labels))
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q_vectors</span> <span class=o>@</span> <span class=n>c_vectors</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>/</span> <span class=n>tau</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span></code></pre></div></div><blockquote class=quote-copyright>Author: Yunpengtai<p>Link: https://yunpengtai.top/posts/infonce/<p>License: CC BY-NC-SA 4.0. You must provide a link to the source.</blockquote><footer class=post-footer><ul class=post-tags><li><a href=https://yunpengtai.top/tags/contrastive-learning/>contrastive learning</a></li></ul><nav class=paginav><a class=prev href=https://yunpengtai.top/posts/efficient-tricks-for-llm/><span class=title>« Prev</span><br><span>Efficient Tricks for LLMs</span></a>
<a class=next href=https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/><span class=title>Next »</span><br><span>NCE的朋友们</span></a></nav></footer><!doctype html><head><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://unpkg.com/katex@v0.16/dist/katex.min.css></head><body><div id=waline></div><script type=module>
    import { init } from 'https://unpkg.com/@waline/client@v3/dist/waline.js';
    import katex from 'https://unpkg.com/katex@0.16/dist/katex.mjs';

    init({
      el: '#waline',
      lang: 'en-US',
      reaction: true,
      avatar: "mp",
      serverURL: 'https:\/\/example.yunpengtai.top\/',
      AUTHOR_EMAIL: '2111783652@qq.com',
      texRenderer: (blockMode, tex) =>
          katex.renderToString(tex, {
            displayMode: blockMode,
            throwOnError: false,
            output: 'html'
      }),
      emoji: [
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs',
        'https://unpkg.com/@waline/emojis@1.2.0/tw-emoji',
        'https://unpkg.com/@waline/emojis@1.2.0/bilibili'
    ],
      reaction:[
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheart.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatrainbow.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatattentionreverse.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheartbroken.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatopenmouth.png',
        'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatcoffee.png'
      ],
      locale: {
        placeholder: '可以匿名评论哦~ QQ邮箱可自动获取头像 Anything to say? It can be anonymous.',
        level0: '潜水',
        level1: '冒泡',
        level2: '摸鱼',
        level3: '话痨',
        level4: '话满天',
        level5: '龙王',
      }
  });
  </script></body></html></article></main><footer class=footer><span>&copy; 2024 <a href=https://yunpengtai.top>Tai's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/sherlcok314159/MyPaperMod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("//cdn.bootcss.com/pangu/4.0.7/pangu.min.js",function(){pangu.spacingPage()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>