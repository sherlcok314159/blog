[{"content":" 问题 LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇：\nLLM 目前在数学问题上取得的进展 有哪些技术路线在未来可能会更进一步提升 LLM 解决数学问题的能力？ 在以下部分不妨来讨论上面两个问题\n技术路线 那么在目前的模型中，分别有哪些方案呢？这里只会介绍各种路线关于数学的 key points，不会特别关注其他细节。另外 MathGPT 和 Abel 目前没有详细的 report，不会涉及\nInference Prompts LLM 直接输出答案经常会出错，第一条路线是通过 prompt 的不同方法来让模型不断「shift the distribution」，让模型不断调整输出分布，让结果更稳定和准确\nCoT 众所周知，CoT（Chain of Thought）是一种通过让模型一步步思考来提高模型推理能力的，下图右侧是 CoT\n下图横轴代表参数规模，当时通过 CoT 在 PaLM 上甚至超过了监督训练的 SOTA，CoT 这种方法就感觉像是「refine 模型输出的分布」，一步步思考也就意味着输出尽可能往正确的上面靠，相比之下，直接输出就像是「一锤定音」\nSelf-Verification 这篇工作就是进行反向验证，这篇工作看到还是比较开心的，刚好验证了自己之前的猜想\n举个例子来说，我们问模型一个问题，模型回答了两种答案：\n接下来我们将这个答案作为已知量，将题干中的变量当做未知，去反推，发现不一致，则说明答案有误\nFORBAR FORBAR 代指的是 Forward-Backward Reasoning，跟上面几乎一致，可以看个例子：\nForward 就是我们之前让模型直接算的，而 Backward 的意思就是我把题干中的一个变量给遮盖掉，变成 x，而我告诉你答案，我要反问你 x 是啥\n不同的是 Self-Verification 是用以验证，不能扩充成为 Examples，而 FORBAR 就可以作为 Examples\n效果是相当不错的：\nPoT Program of Thought 是不同于 CoT 的工作，出发点也很简单，比较复杂的数值推理任务，靠一步步推理，其实 LLM 还是会搞错\n那就有人想了，能不能不让 LLM 来计算，让 LLM 学会调用 Python 程序来搞呢？程序计算肯定比 LLM 自己来靠谱，这就是 PoT 干的事情：\n效果也是会比 CoT 好上一些，下图是 Few-Shot 的结果：\nMajority Voting 多数投票法（或称之为 self-consistency）是在 CoT 的基础上，对结果进行投票，在推理时，我们采样多个可能的解决方案，然后看最后那个预测结果出现最多，就选哪个\n结果也是比较显著的，比单独的 CoT 要好：\nWork Memory Galactica 提出了「工作记忆」的概念，先引入人类思考的习惯，当我让你求几个数字的平均值时，比如 43, 29, 51, 13，你会写一些过程在纸上，这种写出来的称之为 External Work Memory，即为外在工作记忆\n仔细看图中倒数两行的位置，有个 thinking，有些人算 136/4 是可以在脑中完成计算的，这种被称为 Internal Work Memory，而 Galatica 正是将两种记忆方式结合在一起解决问题。让我们看个 Work Memory 的例子：\n有人说，乍一看，这不就是 CoT 吗？一步步展开思考，没毛病吧，但你看 calculate 那里，这个过程是可以不需要 LLM 自己得出答案的，它可以只写个代码让计算机执行 Python 程序，而 LLM 做的是下达指令，读取输出即可\n具体而言，它跟 CoT 的区别或者说 CoT 的不足之处在于：\nCoT 相当于上面求平均值一直写在纸上的过程，但是有些 low-level 的思考人类是不需要写下来的，而 LLM 做不到这一点，这也是为什么 CoT 会产生看似正确但又模糊的答案，而 Work Memory 是将 internal 的思考交由更准确的工具去做，比如上述算力大小的例子，写程序，然后执行代码，最后 LLM 只需要读取就行了\n这个方法的难点在于数据集的准备，而 Galatica 是通过以下方法来创建：\n通过程序来控制一些变量，来生成例子（OneSmallStep） 从现成的数据集中拿（Workout, Khan Problems） 另一种就是直接用\u0026lt;work\u0026gt;这种模版转换（GSM8k train） mCoT 即代表 Majority Voting + CoT 一起的，可以看到 mCoT 还是效果优于工作记忆的，不过工作记忆并没有经过很好的养成，数据集既不大也不多样，后面可以优化的空间肯定更多\n需要注意的是，不能单纯的拿\\\u0026lt;work\u0026gt;和 mCoT 对比，因为上面 Galatica 的 prompt 数据集是预训练过的 OPRO 这篇文章我也比较喜欢，目前 LLM 对于 prompt 不稳定，我猜测可能是不一致的原因：用人工优化的 prompt 来调整 LLM 内在输出分布，那很自然就有一个想法，能不能让 LLM 自己来优化使用的 prompt 呢？\n这就是 OPRO 做的事情，将 LLM 当做是优化器，不同于以往传统的优化任务，有各种公式做约束，OPRO 是用「自然语言描述」做约束，比如 Find the most effective prompt for this problem，具体框架如下：\n首先是用最开始 prompt，接着让负责优化 LLM 产生出多个 prompt，再根据目标用 负责打分 LLM 进行打分，将打分后的 prompt 和分数一起放进下一次优化中，最后输出分数最高的 prompt\n下图是 GMS8k 在优化 prompt 过程中的准确度，这里图可能有误解，其实没有 train，只是在优化过程中找到 prompt，然后在 eval set 进行评测准确率\nReward Models 第二条路线是改进 RLHF（Reinforcement Learning from Human Feedback）\nProcess Supervision We\u0026rsquo;ve trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”).\n目前的奖励模型大多基于「结果监督」，追求让模型产出正确的结果，而这样的弊端在于两点：\n模型中间的思考过程是未知的，解释性和可靠性不高 无法真正实现 alignment with humans，只是去对齐结果 In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.\n那么 OpenAI 的解决方案即引入「过程监督」，让模型每一步思考都和人类的进行对齐，这样就可以较好地解决以上的两个问题。作者还发现，尽管是过程监督，但只对比结果依然比结果监督要好\n同时 OpenAI 开源了过程监督的数据集 PRM800K，下图中 ORM 代表「Outcome Rewarded Model」，PRM 代表「Process Rewarded Model」\n当问题越复杂时，过程监督的优点就逐渐显露出来，结果好的同时更具解释性，可谓是一举两得\nReinforcement Learning from Evol-Instruct Feedback Reinforcement Learning from Evol-Instruct Feedback（RLEIF）是 WizardMath 提出的技术，设计比较巧妙，很 nice 的工作，一共分为三步：\n第一步：是先用指令微调获得模型 Wizard-E\n第二步：接着模仿 Evol-Instruct 利用 Wizard-E 和 ChatGPT 来扩充训练集，主要是分为两个方面，让问题变得更简单（Downward）和让问题更复杂（Upward），比如加更多限制，复杂化原来的问题。然后用 Wizard-E 进行排序，获得训练 Instruction Reward Model（IRM）的数据集\n再让 Wizard-E 模型来生成解决问题的步骤，用 ChatGPT 来评判每步的正确性，以此来生成训练 Process Supervision 的数据集\n第三步便是用 PPO 联合训练两个奖励模型，具体是将两个获得的奖励相乘，模型的效果也是比较惊艳的，不过在 MATH 数据集上还差点意思（可能是参数规模不够）\n数据入手 Galactica Galactica 用于预训练的数据量比同等大模型用的要小得多，整个数据集是经过清洗的「科学领域」数据集，包括论文，参考资料，蛋白质序列，化学公式等等，大小为仅有 106 billion tokens\n从上述表格可以看出，论文占了大头（83%），令人惊讶的是 CommonCrawl 仅仅占了 1%\n还将prompt数据集加入了预训练之中 作者还提到 prompt prertraining 可以增强模型 general 的能力：\nMinerva 无独有偶，Minerva 同样收集了来自 arxiv 上的论文，有趣的是 General 的语料也是占比很少\n另外还收集了带有数学公式的网站：\n不一样的是利用方式，Minerva 采用的模型基座是 PaLM，然后在收集的数据上进行 Auto-regressive loss 微调\nRejection Sampling FineTuning 上面的 Galatica 和 Minvera 都是收集数据，是否可以让 LLM 生成数据呢？\nRFT（Rejection Sampling FineTuning）是阿里的工作，方法比较符合直觉，直接让微调后的模型来生成不同的例子用以扩充数据集，然后根据一些准则来筛选：\n答案不对的 计算结果和 Python 结果不一致的 可以看到效果比单纯的 SFT 要好上不少：\nAugGSM8K 还是那句话，我们可以用 LLM 来进行数据的扩充，问题是怎么扩充呢？这篇工作针对问题和回复分别进行了增强：\n关于问题，分别采用了五种增强策略（见下图左列），这些原则的设计还是比较符合直觉的\n对于回复，作者是采用不同的温度来生成多样化的推理路径，当然也会对于明显错误的进行过滤\n数据集概览如下：\n生成的数据集证明也是有效果的：\nMetaMath 这篇文章跟上面的 AugGSM8K 算是同期工作，也是利用 LLM 来对数据集进行增广，然后进行微调\n它主要是对问题进行增广，分为以下几种：\nRephrase Question Self-Verification FORBAR 效果的话基本和 AugGSM8K 也差不多，除了 70B 是 QLoRA，其他的参数是 LLaMA 的\nMAmmoTH 这篇工作是我数据分支线中很喜欢的一篇，其实 RFT 类通过 Bootstrap 来搞数据集的路子，会丧失「通用性」，在 GMS8K 上效果好，而在 MMLU-Math 上就有可能掉很多点；而 Galatica 和 Minvera 付出的代价要很多\n那么 MAmmoTH 就构建了一个数据集：MathInstruct：\n下图是其与其他数据集的对比：\n特点有三个：\n量大，一共有 200K 覆盖比较多的数学领域 Prompt 同时包括了 CoT 和 PoT 其实有些时候我觉得搞数据集是在水文章，但是有些时候是真的很有意义，比如 ImageNet，以及这一篇工作，方法再 fancy，数据缺少带来的问题是很难解决的\n效果也很顶，比 RFT 以及 Wizard 更具备通用性，是 Llama 就能有如此效果，相信换更大的模型肯定会有不错的提升\n架构 MathGLM 具备算数能力 这篇工作是相当有趣，它也解决了我的困惑，若 LLM 从一开始就学数值推理，能不能做好？同时，这篇工作给了一个全新的视角来做一个「专家模型」\n首先是其数据构造环节，很有启发性，无论是 CoT，还是工作区记忆，都强调需要把细节尽可能写下来，LLM 不能像人一样跳跃，这篇工作直接将推理的结果改成一步步得出\n接下来选用的主力 backbone 是 2B 的 Transformer-Decoder，你没有看错，这篇文章并没有使用 LLM Backbone，而是用 AutoRegressive loss 直接用上面数据集去训练\n下图的测试例子一共有 9592 条，直接碾压 GPT-4。当然，我认为这里的是裸模型，没加任何操作，如果用好的操作，GPT-4 应该可以做得更好。因为你拿一个垂域和一个通用模型比，至少也应该给 GPT-4 一些更好的 Prompt Method，或者一些上下文学习的例子\n不过这个实验结果证明了以下两点：\nDecoder-Only 架构的确可以学习到算数规则 参数的规模不需要那么大，2B 就能有很好的能力 通用一点的专家模型 为了让 MathGLM 可以解决文本描述的问题，MathGLM 还需要变得更通用，于是把目光放到了带有描述的数据集，同样的，将原来的解题步骤进行了细化，这样的好处是，既可以学习到数学知识，又可以建模文本，相当于比之前的专家模型更具通用性\n路在何方 那么理想化 LLM 能解决数学问题的标准是什么呢？\n题目的解答得正确 解题过程正确可解释 这两点是有可能做到的。综合以上的论文，我大胆预测一下未来的 Math LLM 可能的发展趋势，思路肯定是做一个较为通用的「专家模型」，具体怎么做呢？\n数据的收集和处理 数据的收集前面的工作可谓是百花齐放，核心思路就三个：\n尽可能人工去搞高质量的数学推理数据集，你像 LLM 在文本领域能成功，肯定离不开大量的高质量数据集 如果特别强领域特性的没有，就找更接近的，比如论文这种科学领域的语料 实在不行，就想办法让 LLM 来自我产生数据集（Bootstrap），但这种很依赖于模型，且会引入模型的内在 Bias，但不失为提升模型的手段 数据的处理这块看着比较简单，其实不然\n举个例子，MetaMath 就将答案改写成一步步过程，引入第一个数据原则：detailed，把 LLM 当成弱智，越详细越好\n第二个是 Minerva 的处理，它将公式单独处理成 LaTeX 中见到的样子，就相当于你用特殊的 token 来包裹公式，用某种方式来提示 LLM，第二个原则：保留不同于文本的模态特征\n专家模型的路线 这一点其实 MathGLM 和 Galatica 给了我不少启发，但目前有一个问题尚未解决：\n如果先预训练 Math LLM，后期去建模文本，究竟能保留多少通用性，或者反过来，如何保留数学的能力，说到底就是 how to be general and specific，如何衡量「通用性」和「专用型」，是值得考量的\n决定之后，其实就可以借鉴这两个模型的路子，比如将 Prompt 数据集直接放入预训练中，以及用 AutoRegressive loss 去建模数值推理的例子等\n预训练完毕，再利用 SFT 或是 RLHF 类方法去进一步微调\n参数的规模我觉得应该不会需要很大，相反，对于数据如何利用是值得考量的，正如在 Galatica 中提到的：作者们发现当重复训练时，性能也会稳步上升，作者将此归因于高质量的数据\n更强的推理方法 从简单的 Zero-Shot 再到 CoT，到 OPRO，经历了太多 Prompt 方法的变迁，我想未来应该很会有，但趋势应该是如何将 LLM 本身的知识引入其中来选择或构造 Prompt，这种一致性带来的提升会更稳定和持久\n最后一个问题 How to leverage LLM\u0026rsquo;s intrinsic ability to do reasoning?\n这也是我最近一直在想的问题，你看在数据那块，你会发现让模型生成一些例子再放进去推理会比直接推理要好一些，这都是模型自己的能力，有没有什么更优雅的方法可以将这种能力抽离出来\n换句话说，我们对 LLM 本身能力的压榨是不是还有上升的空间？\nReferences https://openai.com/research/improving-mathematical-reasoning-with-process-supervision https://galactica.org/static/paper.pdf https://arxiv.org/abs/2203.11171 https://arxiv.org/abs/2201.11903 https://arxiv.org/abs/2206.14858 http://arxiv.org/abs/2309.03409 https://arxiv.org/pdf/2308.09583.pdf https://arxiv.org/abs/2304.12244 http://arxiv.org/abs/2308.01825 https://arxiv.org/abs/2309.05653 https://arxiv.org/abs/2211.12588 http://arxiv.org/abs/2310.05506 http://arxiv.org/abs/2309.03241 https://arxiv.org/abs/2309.12284 https://arxiv.org/abs/2308.07758 https://arxiv.org/abs/2212.09561 ","permalink":"https://yunpengtai.top/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E4%B9%8B%E8%B7%AF/","summary":"问题 LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇： LLM 目前在数学问题上取得的进展","title":"大模型的数学之路"},{"content":"如何高效训练或推理大模型一般在两点：如何装得下以及如何更快\n这里讲一些主要的并行概念，不会深挖原理，只会介绍 key points，看它们分别为加速和适配显存做了什么贡献\nData Parallelism 对于常规的单机多卡用户而言，数据并行（DP）应该不陌生，在 PyTorch 中就有 DistributedDataParallel，主要原理就是将模型复制到每个 GPU 上，并将数据进行切分，然后在每个 GPU 上单独进行前向传播，接着在主进程上将输出聚集，将输出进行分发，在主进程上聚集梯度，然后进行更新，再将更新好的模型复制到其他 GPU 上\n当我们说 DP Degree 为 2 时，则需要两张 GPU\n得益于并行计算，相比于单卡速度会有不错的提升，需要注意的是，速度的增长很难是线性的，因为中间各种分发和聚集的过程会有通信的损失，这一点在高效训练中经常需要考虑\nZeRO Data Parallelism ZeRO 是DeepSpeed开发的，下面来简要讲讲：\n可以看到 Baseline 代表的就是上面原始的 DP，可以看到蓝色的代表模型参数，绿色代表的是优化器状态，剩下的是梯度\n$os, g, p$ 分别代表的是优化器状态，梯度以及参数。ZeRO 有三个阶段：$P_{os}, P_{os+g}, P_{os+g+p}$，$P$的意思是切分\nStage 1：我们只对优化器状态进行切分 Stage 2：同时对优化器状态和梯度进行切分 Stage 3：对优化器状态，梯度和参数进行切分 其实不用想的很复杂，跟原始的 DP 相比，你可以理解为只是原来在每张 GPU 上的优化器状态，梯度和参数变成了一部分而已\n同时，需要注意，ZeRO 需要在 FP16 进行计算\n因而，ZeRO 主要是为训练而生的，只有 Stage 3 和推理相关\nTensor Parallelism 张量并行就是原来一张卡上进行的张量操作在多张 GPU 上进行，可以看到原来的$\\mathbf{XA}$可以被拆分为以下过程：\n本质上就是将张量按照行或列进行拆分，分别计算，然后聚集得到结果，这样的好处是可以将很大的张量放到几个 GPU 上来适配显存\n由于张量计算是比较普遍的，因而若想进行 TP，对于设备间的通信要求比较高，当你有好几个节点的时候，不建议节点之间使用 TP，通信将是负累，在Bloom里作者说PCIe速度比节点间要快得多\nPipeline Parallelism 流水线并行的操作主要就是为了适配显存的，当你的显卡不足以放下模型的时候，你可以将模型的组成部分进行切分然后放到不同的 GPU 上\n=================== =================== | 0 | 1 | 2 | 3 | | 4 | 5 | 6 | 7 | =================== =================== GPU0 GPU1 比方说上图，模型一共有 8 层，我们将模型一分为二，放到不同的 GPU 上面，当我们涉及到那一层，就把相关的中间输出和数据转移到哪张卡上\n原始的 PP 效率问题很严重，当你在某张卡上进行运算时，其他卡都是空闲状态\n在下图中，上图是原始的 PP，从上往下分别代表四个 GPU，F 是代表 Forward，而 B 代表 Backward\n下面的即为改进方案，将每个 Batch 切分为 micro-batches，拿灰色的举例，被切分为 4 个 micro-batches\n在 Bloom 作者的实践中，当一个词表很大（250K）时，word embedding 会比 transformer 的 block 占用更大显存，此时就需要将 word embedding 层也看做是一个 transformer block\nDP+PP 当然，不同的并行方法也可以组合，DP+PP 就是一个例子。假设我们目前有 4 张卡，我们可以将 2 张卡就看做是一张卡，那我们就可以进行 DP 为 2 的并行，而两张卡里又可以进行 PP 为 2 的并行\nDP+PP+TP 当然，这两者还可以和 TP 结合在一起，DP=PP=TP=2，这时候就需要八张卡了，这种并行方式叫做 3D Parallel\n异构空间管理 上面无论是 PP 还是 TP 都在说，放另一张卡上，假设我没有那么多卡咋办，此时就引入了异构操作了，既然放不下，就将模型的 block 进行切分，切分之后尽可能塞满 GPU，放不下的就放在 CPU 和硬盘上\nGemini Gemini 是 Colossal AI 开发的功能，比 ZeRO 多了动态异构空间管理，同时引入了 chunk 机制，可以让通信比原先更加高效\n当用 ZeRO 来切分参数时，每次通信都得申请一块内存，通信完毕再释放，这样就会有内存碎片化的情况；同时，通信是以 Tensor 为粒度进行通信，会导致网络带宽无法充分利用，一般而言，传输消息越长带宽利用率越高\n对于以上两个问题，Gemini 采用以下方式解决：\n将计算顺序连续的一组参数直接存入一个 chunk，避免了内存碎片的问题； 如果一个参数多次发生计算，即会发生多次通信，效率不高，就会将和它有关的一组参数全部放进 chunk 里，降低了通信消耗； 同时，小 Tensor 无法充分利用带宽，Gemini 会将小 Tensor 聚集起来放在 chunk 里变成大 Tensor，然后一次性计算 ZeRO DP+PP+TP 当 ZeRO DP 和 PP（或是 TP）结合在一起时，一般 Stage 1 是可以激活的，然而 Stage 2 和 3 是否被激活，可以看看激活后是否真正达成了自己的目的，因为激活后通信带来的损失会越来越多\nBF16 Bloom 的作者不建议训练 LLM 用 FP16，因为在训练过程中很容易遇到数值溢出问题（loss 很不稳定，如下图），而 BF16 不会遭遇这种问题\n然而，BF16 带来的代价就是精度会降低，不过只要多训一点，精度就会慢慢上来，所以也可以接受\n然而用 BF16 之后 Loss 就平滑不少了\nFused CUDA Kernels GPUs 一般做两件事：计算以及拷贝数据，当 GPU 拷贝数据时，其计算单元是空闲的，比如以下例子：\nc = torch.add(a, b) e = torch.max([c,d]) 原始的做法就是首先从内存读入 a, b 的值，然后进行运算，将 c 的值返回至内存；接下来再从内存读入 c 和 d 的值，进行运算\n而融合的 kernel 就不必返回 c 的值到内存，只需要放到 GPU registers，只需要取出 d 的值进行计算即可，比原来更高效。\nReferences https://colossalai.org/zh-Hans/docs/features/zero_with_chunk https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism https://arxiv.org/abs/1910.02054 https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/ https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/ ","permalink":"https://yunpengtai.top/posts/efficient-tricks-for-llm/","summary":"如何高效训练或推理大模型一般在两点：如何装得下以及如何更快 这里讲一些主要的并行概念，不会深挖原理，只会介绍 key points，看它们分别为加速和","title":"Efficient Tricks for LLMs"},{"content":"区分真实样本 前面的讲的NCE系列方法是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接采用自归一化，即：\n`$$ p(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{1}\n$$ `\n首先对于每个训练样本来说，我们再从噪声分布$q(x)$中采样$k-1$个样本，我们的目的即是区分出真实样本：\n` $$\n\\begin{align} L(\\boldsymbol{\\theta}) \u0026amp; = -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}{j} \\sim q} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w}|\\boldsymbol{c})+\\sum*{j=1}^{k-1}q(\\boldsymbol{w}{j})}\\ \u0026amp; = -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}{j} \\sim q} \\log \\frac{u{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{u*{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}) + \\sum*{j=1}^{k-1} q(\\boldsymbol{w}*{j})} \\end{align}\n$$ `\n最小化上述 loss 到底有什么好处呢？\n互信息来访 对于训练样本$i$来说，它是真实样本的概率为：\n` $$\n\\begin{align} p(d=i) \u0026amp; = \\frac{p(\\boldsymbol{w}{i}|\\boldsymbol{c})\\prod{j=1, i\\neq j}^{k}q(\\boldsymbol{w}{j})}{\\sum{j=1}^{k}p(\\boldsymbol{w}{j}|\\boldsymbol{c})\\prod{l=1,l\\neq j}^{k}q(\\boldsymbol{w}{l})} \\ \u0026amp;= \\frac{\\frac{p(\\boldsymbol{w}{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}{i})}}{\\sum{j=1}^{k} \\frac{p(\\boldsymbol{w}{j}|\\boldsymbol{c})}{q(\\boldsymbol{w}{j})}} \\end{align}\n$$ `\n转换的方法即是上下同除以$\\prod_{i=1}^{k} q(\\boldsymbol{w}_{i})$\nInfoNCE 的目的即为正确区分出真实样本，而真实样本都来自训练集（与噪声集区分），也就是说对于 loss 而言，最优解即为$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$正比于 density ratio：\n` $$\nu*{\\boldsymbol{\\theta}}(\\boldsymbol{w}*{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}{i})}\n$$ `\n我们知道，互信息的计算如下：\n` $$\nI(\\boldsymbol{w}, \\boldsymbol{c}) = \\text{KL}(p(\\boldsymbol{w}, \\boldsymbol{c})|p(\\boldsymbol{w})p(\\boldsymbol{c})) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}, \\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})}\n$$ `\n为了和上述 loss 有关，很自然做出以下变化：\n` $$\nI(\\boldsymbol{w}, \\boldsymbol{c}) = \\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})p(\\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})} = \\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log {\\color{blue}\\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w})}}\n$$ `\n也就是说如果我们要$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$来表示互信息，还得$q(x) = p(x)$，也就是噪声分布即为模型分布（unconditioned）\n那么：\n` $$\nu*{\\boldsymbol{\\theta}}(\\boldsymbol{w}*{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}{i}|\\boldsymbol{c})}{{\\color{blue}p(\\boldsymbol{w}{i})}}\n$$ `\n此时完整的 loss 即为：\n` $$\nL(\\boldsymbol{\\theta }) = -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}{j} \\sim q} \\log \\frac{u*{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum*{j=1}^{k} u*{\\boldsymbol{\\theta}}(\\boldsymbol{w}*{j}, \\boldsymbol{c})}\n$$ `\n当我们要使得训练样本$\\boldsymbol{w}$是真实样本的概率变得最大，即一定程度最大化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$，然后最小化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{j}, \\boldsymbol{c})$，即最大化$\\boldsymbol{w}, \\boldsymbol{c}$之间的互信息，最小化负样本与$\\boldsymbol{c}$之间的互信息\n注意：现在用的时候并非像一开始通过真实样本的概念，而是可以将$\\boldsymbol{w}, \\boldsymbol{c}$看成是一对正样本，噪声样本看作负样本\nHardness-Aware 同时 InfoNCE 会着重关照那些跟真实样本$\\boldsymbol{c}$更相似的负样本，这类也叫做「Hard-Negative」，而相应地会轻视一些与$\\boldsymbol{c}$本来就没有那么相似的负样本，即「Hardness-Aware」\n` $$\n\\begin{align} L(\\boldsymbol{\\theta }) \u0026amp; = - \\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}{j}\\sim q} \\log \\frac{u*{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{u*{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})+\\sum*{j=1}^{k-1} u*{\\boldsymbol{\\theta }}(\\boldsymbol{w}{j}, \\boldsymbol{c})} \\ \u0026amp;= -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}{j}\\sim q} \\log \\frac{\\exp(s{\\boldsymbol{w}, \\boldsymbol{c}} )}{\\exp(s*{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum*{j=1}^{k-1}\\exp(s*{\\boldsymbol{w}*{j}, \\boldsymbol{c}})} \\ \u0026amp;= -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}{j} \\sim q} \\left[s_{\\boldsymbol{w}, \\boldsymbol{c}} - \\log\\left( \\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j},\\boldsymbol{c}}) \\right)\\right] \\end{align}\n$$ `\n对于$\\boldsymbol{w}_{j}$而言，与$\\boldsymbol{c}$的相似性越大，则在损失函数中占比越大，可以看成是惩罚越大，然而仅凭这个没办法做到上述性质，比如下面的 loss 也可以：\n` $$\nL*{\\text{simple}}(\\boldsymbol{\\theta }) = \\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}{j} \\sim q} \\left[-s{\\boldsymbol{w}, \\boldsymbol{c}} + \\sum{j=1}^{k-1} s*{\\boldsymbol{w}_{j}, \\boldsymbol{c}}\\right]\n$$ `\n然而实验证明，这个 loss 效果并不是很好\n接着，我们来求下损失函数相对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的偏导：\n不妨就对单个样本$\\boldsymbol{w}, \\boldsymbol{c}$来看：\n` $$\n\\frac{ \\partial L*{\\text{simple}} }{ \\partial s*{\\boldsymbol{w}_{j}, \\boldsymbol{c}} } = 1\n$$ `\n也就是说 loss 对于所有$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化一样敏感，即「一视同仁」，没有给损失函数的优化注入其他信息\n` $$\n\\frac{ \\partial L }{ \\partial s*{\\boldsymbol{w}{j},\\boldsymbol{c}} } = \\frac{\\exp(s{\\boldsymbol{w}{j}, \\boldsymbol{c}})}{\\exp(s{\\boldsymbol{w},\\boldsymbol{c}} )+ \\sum*{j=1}^{k-1}\\exp(s*{\\boldsymbol{w}*{j}, c})}\n$$ `\n而对于 InfoNCE 而言，对于所有负样本而言，分母是固定的，而「分子越大」，即$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$越大，则此时 loss 对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化则越敏感\n总结说就是，InfoNCE 这种 loss 在优化时会注入一种偏好，更偏向于把与真实样本相似的噪声样本给区分开来\n温度系数 一般现在用的时候，还会加入一个调节因子：$\\tau$\n` $$\nL(\\boldsymbol{\\theta})= -\\mathbb{E}{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}{j} \\sim q} \\log \\frac{\\exp(s*{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau )}{\\exp(s*{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau)+\\sum*{j=1}^{k-1}\\exp(s*{\\boldsymbol{w}_{j}, \\boldsymbol{c}}/\\tau)}\n$$ `\n那么：\n` $$\n\\frac{ \\partial L }{ \\partial s*{\\boldsymbol{w}{j},\\boldsymbol{c}} } = \\frac{1}{\\tau}\\frac{\\exp(s{\\boldsymbol{w}{j}, \\boldsymbol{c}}/\\tau)}{\\exp(s{\\boldsymbol{w},\\boldsymbol{c}}/\\tau )+ \\sum*{j=1}^{k-1}\\exp(s*{\\boldsymbol{w}*{j}, c}/\\tau)}\n$$ `\n推论 1：当$\\tau$很小时，比如$0.02$那么由上式可得 loss 会格外关注于将与真实样本相似的噪声样本给区分开。换句话说，$\\tau$越小，对 hard-negative 的惩罚越大\n下面我们引入一个 kernel 来衡量一个分布的均匀性（uniform），$f(\\boldsymbol{x})$代表模型输出的表示\n` $$\nL*{\\text{uniformity}}(f;t) = \\log \\mathbb{E}*{\\boldsymbol{x}, \\boldsymbol{y} \\sim \\mathcal{D}} \\left[e^{-t|f(\\boldsymbol{x}) - f(\\boldsymbol{y})|_{2}^{2}}\\right](t \\in \\mathbb{R}_{+})\n$$ `\n在Wang et, al中，作者采用不同的$\\tau$来对比模型产生表示的均匀性\n当$\\tau$比较小时，模型产生的表征空间更加均匀，随着$\\tau$的增大，均匀性被破坏\n而对于一个好的对比学习的表征空间应该满足「locally clustered，globally uniform」，比如下图，当表征均匀分布在球面上时，此时用一个超平面（线性分类器）便可分开\n推论 2：当$\\tau$较小时，用 infoNCE loss 训出的模型表征空间比较均匀，当$\\tau$增大时，表征空间的均匀性被逐步破坏\n上代码 当下的算力已经足够支撑直接用概率而非自归一化了，其实会发现 InfoNCE loss 跟多分类交叉熵损失一样：\nimport torch.nn.functional as F def infoNCE(q_vectors, c_vectors, labels, tau=1): \u0026#34;\u0026#34;\u0026#34;Compute the InfoNCE Loss in http://arxiv.org/abs/1807.03748. Params: - q_vectors: Tensor. Shape: (bs, d_model). Vector presentation of query. - c_vectors: Tensor. Shape: (num_pos_neg_contexts, d_model). Vector presentation of context. - labels: Tensor. Shape: (bs, ). Indices for the positive contexts. - tau: Scalar. The parameter to control penalty on hard negatives (smaller =\u0026gt; harder). A good initialization value can be 0.02 or 0.05. Examples: \u0026gt;\u0026gt;\u0026gt; q_vectors = torch.randn((2, 4)) \u0026gt;\u0026gt;\u0026gt; c_vectors = torch.randn((4, 4)) # each query has 4 contexts \u0026gt;\u0026gt;\u0026gt; labels = torch.tensor([1, 3]) \u0026gt;\u0026gt;\u0026gt; print(infoNCE(q_vectors, c_vectors, labels)) \u0026#34;\u0026#34;\u0026#34; scores = (q_vectors @ c_vectors.T) / tau return F.cross_entropy(scores, labels) References http://arxiv.org/abs/1807.03748 https://jxmo.io/posts/nce http://arxiv.org/abs/2012.09740 http://arxiv.org/abs/2005.10242 $$ ","permalink":"https://yunpengtai.top/posts/infonce/","summary":"区分真实样本 前面的讲的NCE系列方法是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接采用自归一","title":"放大镜下的 InfoNCE"},{"content":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbol{c}$，要去词表$\\mathcal{V}$挑选$\\boldsymbol{w}$来生成：\nNegative Sampling 其实要说 NCE 的缺点，就是需要花时间调参数，比如说$k$和噪声分布$p_{n}$的选择，而负采样则固定下来这两个参数，不同的负采样对于这两个参数的选择也不尽相同，这里介绍比较简单的一种\n我们规定，当$\\mathcal{D}=1$时代表从训练集采样，而$\\mathcal{D}=0$则代表从噪声中采样，为了表达简便，对$\\boldsymbol{c}$进行省略，即$p(\\mathcal{D}=1|\\boldsymbol{c}, \\boldsymbol{w}) = p(\\mathcal{D}=1|\\boldsymbol{w})$\nNCE 会这样求样本来自哪个采样的概率：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n而负采样说，如果词表大小是$|\\mathcal{V}|$，不妨就采样$|\\mathcal{V}|$个噪声，而且每个噪声是等概率出现的，那么：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{1}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\end{align} $$\n综合上面的式子来看，其实负采样是 NCE 的一种\nImportance Sampling 我们记需要计算的分母（配分函数）为$Z(\\boldsymbol{\\theta })$，注意：$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})=\\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}))$，$s_{\\boldsymbol{\\theta}}$为打分函数\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\underbrace{ \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) }_{ Z(\\boldsymbol{\\theta }) }} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} $$\n那么：\n$$ \\begin{align} Z(\\boldsymbol{\\theta }) \u0026amp; = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) {\\color{blue}\\frac{p_{n}(\\boldsymbol{w}')}{p_{n}(\\boldsymbol{w}')}} \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} p_{n}(\\boldsymbol{w}') \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\\\ \u0026amp;= \\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\end{align} $$\n跟 NCE 不同的是，重要性采样并没有把配分函数当成参数，而是利用噪声数据去拟合它：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})/p_{n}(\\boldsymbol{w}')} $$\n同样地操作，我们可以采样$k$个噪声样本来去近似期望，即：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) \\approx \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}p_{n}(\\boldsymbol{w}_{i})u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{i}, \\boldsymbol{c})/p_{n}(\\boldsymbol{w}_{i})} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w_{i}}, \\boldsymbol{c})} $$\n仔细看，其实重要性采样是利用蒙特卡洛采样将分母从一开始的$|\\mathcal{V}|$项降低到$k$项\n可能读者还比较熟悉 InfoNCE，但是它的思想不是为了去「近似配分」，这里就不放在一起展开了\n","permalink":"https://yunpengtai.top/posts/nce%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/","summary":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbo","title":"NCE的朋友们"},{"content":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对较小影响不大；然而，如果数值大于某个特定值之后变成了$\\inf$（数值上溢 Overflow ），还有一种情况是当数值特别小时，会被近似为$0$（数值下溢 Underflow），这两种情形若继续计算误差将会进一步累积，那么就可能导致原本理论上成立的到实现时就不行了，此时就有必要对数值稳定性进行分析\n举个例子：\nimport numpy a = np.array([65599.], dtype=np.float16) print(a) b = np.array([1e-10], dtype=np.float16) print(b) [inf] # Overflow [0.] # Underflow 基础知识 再进一步解释解决方法时还是先铺垫基础知识：\n众所周知，各种数值在计算机底层是通过比特（bit）来进行表示的，有$0$和$1$，比如用$8$个比特就可以表示$[-2^{7}, 2^{7}-1]$（有一位是符号位）\n那么 float16 的意思是用$16$位比特来表示浮点数，同理 float32 的意思是用$32$位比特\n按照表示精度来看： $$ \\text{float64 \u0026gt; float32 \u0026gt; float16} $$ 按照占用空间来看：\n$$ \\text{float16 \u0026lt; float32 \u0026lt; float64 } $$\n$\\inf$的意思是超过了可以表示的范围，有$-\\inf$和$\\inf$两种，而NaN的产生大概可以分为几种情况：\n对负数开根号 对$\\inf$进行运算 除以$0$ 那么如何查看不同表示的范围呢？\nnp.finfo(np.float16) finfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16) 当然，numpy 这里有个小坑，就是你输入的值较大或略微超过范围时，反而会用另一个数来表示，只有大到一定程度时，才会用$\\inf$，详见官网的release notes\nFloating-point arrays and scalars use a new algorithm for decimal representations, giving the shortest unique representation. This will usually shorten float16 fractional output, and sometimes float32 and float128 output. float64 should be unaffected.\na = np.array([65504.], dtype=np.float16) print(a) b = np.array([65388.], dtype=np.float16) print(b) c = np.array([65700.], dtype=np.float16) print(c) [65500.] [65380.] [inf] 注意，这里的大是相对于计算精度来说的，而不是你感觉的，当你把精度调成float32，上面都会打印原来输入的结果\ne之殇 在上高中时，特别喜爱$e^{x}$，有很多好的性质，比如求导等于本身，然而在很多数值溢出的情形，总有它的参与，这是因为机器学习中很多东西都会和它挂钩，比如各种激活函数便有它的影子\n看个例子：\na = np.exp(np.array([654.], dtype=np.float16)) print(a) [inf] 那么我如何知道输入大概多大会导致溢出呢？可以参见下述公式，当输入为$x$，计算$e^{x}$用十进制数表示大概有多少位\n$$ \\log_{10}(e^x) = x \\log_{10}(e) $$\n举个例子，上面我们看到float16当最大位数是$5$位（科学计数法后面得$+1$），那么根据上述公式你就可以算出最大可被接受的$x$，进而进行一些后处理：\ndef compute_max(n): return int(n * math.log(10)) print(compute_max(5)) # 11 我们来试试看：\na = np.exp(np.array([11.], dtype=np.float16)) print(a) b = np.exp(np.array([12.], dtype=np.float16)) print(b) [59870.] [inf] 接下来按照消除溢出的方法看看几大类常见例子：\n归一化指数 当$\\exp(x_{i})$形式出现，就会考虑通过代数恒等变换使得指数上多一些部分来进行归一化\nsoftmax 不妨假设$\\boldsymbol{x} \\in \\mathbb{R}^{n}$，当某个$x_{i}$特别大时，$\\exp(x_{i})$就会出现数值上溢，然后当分子分母都是$\\inf$的时候就会出现NaN，$0$是因为分母是$\\inf$导致的\nimport numpy as np def softmax(x): exp = np.exp(x) return exp / exp.sum(-1) x = np.array([-1., 20000, 0.1], dtype=np.float16) softmax(x) # array([ 0., nan, 0.]) 那么，有什么好的方法来防止这种数值溢出呢，我们通过一些不改变原式的代数运算可以做到：\n$$ \\begin{align} \\mathrm{softmax}(x_{j}) \u0026amp; = \\frac{e^{x_{j}}}{\\sum_{i} e^{x_{i}}} \\\\ \u0026amp;= {\\color{blue}\\frac{c}{c}} \\cdot \\frac{ e^{x_{j}}}{ \\sum_{i} e^{x_{i}}} \\\\ \u0026amp;= \\frac{e^{x_{j} + \\log c}}{ \\sum_{i} e^{x_{i} +\\log c}} \\end{align} $$ 观察上式，不难发现，我们可以控制常数$c$来对$x_{i}$进行规范化，相当于加上偏移量（offset）\n比较简单的做法即为设置$\\log c = -\\max(\\boldsymbol{x})$\n那么：\n$$ \\mathrm{softmax}(x_{j}) = \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i} - \\max(\\boldsymbol{x})}} $$\n代码实现即为：\ndef softmax(x): x -= max(x) exp = np.exp(x) return exp / exp.sum(-1) softmax(x) # array([0., 1., 0.]) 因为最大的数减去自身变为了$0$，$e^{0} = 1$，就不会有什么影响了\n这个与PyTorch官方实现也是一致的：\nimport torch import torch.nn.functional as F x = torch.tensor([-1., 20000, 0.1]) F.softmax(x) # tensor([0., 1., 0.]) Logsumexp 同理再看一个类似的：\n$$ \\begin{align} \\text{Logsumexp}(\\boldsymbol{x}) \u0026amp;= \\log \\sum_{i} e^{x_{i}} \\\\ \u0026amp;= \\log \\sum_{i} e^{x_{i}} \\frac{c}{c} \\\\ \u0026amp;= \\log \\left( \\frac{1}{c} \\sum_{i} e^{x_{i}} e^{\\log c}\\right) \\\\ \u0026amp;= -\\log c + \\log \\sum_{i} e^{x_{i} + \\log c} \\end{align} $$\n取$\\log c=-\\max(\\boldsymbol{x})$，那么：\n$$ \\text{logsumexp}(\\boldsymbol{x}) = \\max(\\boldsymbol{x}) + \\log \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})} $$\ndef logsumexp(x): maximum = max(x) x -= maximum exp = np.exp(x) return maximum + np.log(exp.sum(-1)) 跟 PyTorch 官方实现一致：\nprint(torch.logsumexp(x, dim=-1, keepdim=False)) print(logsumexp(x.numpy())) # tensor(200000.) # 200000.0 展开log内部 就是将log明显可能出现数值溢出的部分拆出来，刚刚logsumexp就是利用了这个道理\nlog-softmax 尽管softmax现在稳定了，然而log-softmax还是有风险溢出，比如上面的$\\log 0$，这也是数值上溢\n$$ \\mathrm{LogSoftmax}(x_{j}) = \\log\\left(\\frac{e^{x_{j}}}{\\sum_{i} e ^{x_{i}}}\\right) $$ 我们将其拆开： $$ \\begin{align} \\mathrm{LogSoftmax}(x_{j}) \u0026amp; = \\log \\left( \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}} \\right) \\\\ \u0026amp;= \\log(e^{x_{j} - \\max(\\boldsymbol{x})}) - \\log \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) \\\\ \u0026amp;= x_{j} - \\max(\\boldsymbol{x}) - \\log \\underbrace{ \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) }_{ \\ge 1 } \\end{align} $$\n因为所有的$x_{i}$中肯定有最大的一个，那么$\\exp(x_{i} -\\max(\\boldsymbol{x}))=1$，剩下的肯定是正数\ndef log_softmax(x): x -= max(x) exp = np.exp(x) return x - np.log(exp.sum(-1)) 同样与PyTorch一致，后面是两个框架显示机制不同，从这个角度也可以看出，都是32位时，PyTorch会更精准\nx = torch.tensor([-1., 200000, 0.1]) print(F.log_softmax(x, dim=-1)) print(log_softmax(x.numpy())) # tensor([-200001.0000, 0.0000, -199999.9062]) # array([-200001. , 0. , -199999.9], dtype=float32) 截断 当输入大于某种阈值，直接输出原来的输入\nSoftplus 下面是PyTorch官方对于Softplus的实现\n$$ \\text{Softplus}({x}) = \\begin{cases} \\log (1 + e^{x}), \u0026amp; x \\leq \\text{threshold} \\\\ x, \u0026amp; \\text{otherwise} \\end{cases} $$ 官方的意思很简单，当$x$大于阈值（这里是置之为$20$），直接不变输出\neps 急救包 当分母可能出现为$0$时，给它加上一个较小的正数$\\varepsilon$\nLayer Normalization $$ \\text{LayerNorm}(\\boldsymbol{x}) = \\gamma \\left(\\frac{\\boldsymbol{x} - \\bar{\\boldsymbol{x}}}{\\sigma + {\\color{blue}\\varepsilon}} \\right) + \\beta $$ 分母加上一个$\\varepsilon$来防止变为$0$：\nclass LayerNorm(nn.Module): def init(self, features, eps=1e-6): super().__init__() self.gamma = nn.Parameter(torch.ones(features)) self.beta = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.gamma * (x - mean) / (std + self.eps) + self.beta ","permalink":"https://yunpengtai.top/posts/numerical-stability/","summary":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对","title":"Numerical Stability"},{"content":"引言 我们规定，训练集记为$\\mathcal{D}$，我们从中取一个样本$\\boldsymbol{x}$，其训练集标签为$y_{\\mathcal{D}}$，一般假设训练集是从一个真实的分布中采样而来，而真实分布不可见，算法通过训练集来近似整个分布。而采样的过程中存在噪声$\\varepsilon$，也就是说有可能因为噪声$y_{\\mathcal{D}}$和真实标签的期望$y$不一致，举个例子，手写数字体识别数据集中存在不少标注错误，此时它们真实的标签和训练集中的就不一致。\n$$ y = \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} $$ 这里为了讨论方便，不妨将噪声置为$0$，即：\n$$ \\varepsilon = \\mathbb{E}_{\\mathcal{D}} (y - y_{\\mathcal{D}}) = 0 $$\n我们还有个在$\\mathcal{D}$上训练好的模型，其对于$\\boldsymbol{x}$的预测记为$f(\\boldsymbol{x})$，在整个训练集上预测的期望为：\n$$ \\bar{f}(\\boldsymbol{x}) = \\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) $$\n偏差-方差分解 那么，我们将期望损失函数进行拆解：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left(f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}) - (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )\\right)^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left((f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2}-2(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x})(y_{\\mathcal{D}}-\\bar{f}(\\boldsymbol{x}))\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2} - 2\\mathbb{E}_{\\mathcal{D}}(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))(y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})) \\end{align} $$\n对于式中最后一项，展开看看：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))(y_{\\mathcal{D} }-\\bar{f}(\\boldsymbol{x})) \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left((f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))y_{\\mathcal{D}} - (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))\\bar{f}(\\boldsymbol{x})\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))y_{\\mathcal{D}} - \\mathbb{E}_{\\mathcal{D}}(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))\\bar{f}(\\boldsymbol{x}) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})\\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})\\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) + \\bar{f}(\\boldsymbol{x})^{2} \\\\ \u0026amp;= {\\color{blue}\\mathbb{E}_{\\mathcal{D}}f(\\boldsymbol{x}) \\cdot\\mathbb{E}_{\\mathcal{D} } y_{\\mathcal{D}}} - \\bar{f}(\\boldsymbol{x}) \\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})^{2} + \\bar{f}(\\boldsymbol{x}) ^{ 2} \\\\ \u0026amp;= (\\mathbb{E}_{\\mathcal{D}}f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x})) \\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} \\\\ \u0026amp;= 0 \\end{align} $$\n上述推导过程中，主要利用了两个信息：\n$\\bar{f}(\\boldsymbol{x})$是个常量，因而可以直接提出来 训练集本身的分布和模型预测的情况两者是相互独立的，因而就有了蓝色部分的分解 接下来我们还得继续把偏差凑出来，这里的$y$应该是指真实标签的期望：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - y + y - \\bar{f}(\\boldsymbol{x}))^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}\\left((y_{\\mathcal{D}}-y)^{2}+ (y-\\bar{f}(\\boldsymbol{x}))^{2} + 2(y_{\\mathcal{D}}-y)(y-\\bar{f}(\\boldsymbol{x}))\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} + 2\\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)(y-\\bar{f}(\\boldsymbol{x})) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} + 2({\\color{blue}y-\\bar{f}(\\boldsymbol{x})})\\underbrace{ \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y) }_{\\varepsilon=0 } \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{D} - y)^{2} + (y - \\bar{f}(\\boldsymbol{x}))^{2} \\end{align} $$\n那么：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp; = \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + (y - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{D} - y)^{2} \\\\ \u0026amp;= \\mathbb{V}(\\boldsymbol{x}) + bias^{2}(\\boldsymbol{x}) + \\varepsilon^{2} \\end{align} $$\n泛化能力 偏差衡量了算法本身对于真实标签的拟合情况，the deviation of the expected estimator value from the true value 方差衡量了在不同训练集$\\mathcal{D}$情况下，即采样不同的训练集跟期望预测的偏差，也就是受扰动性影响，the deviation from the expected estimator value that any particular sampling of data is likely to cause（注意$f(\\boldsymbol{x})$是随着训练集而变化的，而$\\bar{f}(\\boldsymbol{x})$则是取一个任意训练集然后固定） 噪声则是衡量了期望误差的下界，也就是说，无论算法怎样，这个任务难度的初始值就是这样 ","permalink":"https://yunpengtai.top/posts/bias-variance-decomposition/","summary":"引言 我们规定，训练集记为$\\mathcal{D}$，我们从中取一个样本$\\boldsymbol{x}$，其训练集标签为$y_{\\mathca","title":"Bias Variance Decomposition"},{"content":"难以承受之重 文本生成是 NLP 任务中比较典型的一类，记参数为$\\boldsymbol{\\theta }$，给定的 context 为$\\boldsymbol{c}$，需要生成的文本记为$\\boldsymbol{w}$，我们通常通过最大似然法来使得模型预测的分布$p_{\\boldsymbol{\\theta}}$尽可能接近训练集分布$p_{d}(\\boldsymbol{w})$\n$$ \\boldsymbol{\\theta ^{\\ast}} = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta }) $$\n而在建模时，我们通常会在模型加入 Softmax 来将 score 转换为概率，使得对词表$\\mathcal{V}$所有词预测概率相加为 1：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\underbrace{ {\\color{blue}\\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}', \\boldsymbol{c})} }_{ \\text{Partition Function} }}, u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c}) = \\exp(s_{\\theta }(\\boldsymbol{w}, \\boldsymbol{c})) $$\n分母是用来归一化的，也被称作配分函数（Partition Function），为了使得表达更简便，我们将上述公式进一步压缩，将分母统称为$Z(\\boldsymbol{\\theta })$\n若是普通的多分类问题，参数量不大，求$Z(\\boldsymbol{\\theta })$感觉不到压力，可若是文本生成任务，例如，「文本____是自然语言处理的任务」，此时你得去整个词表$\\mathcal{V}$中来挑选词来填空，去计算$Z(\\boldsymbol{\\theta })$就是十分昂贵的事情了\n丢给参数 那么，有人就说了，不行那就直接交给参数处理吧，让模型自己去学，看看模型自己能不能学出归一化：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} = u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})\\exp(z^{\\boldsymbol{c}}), \\, z^{\\boldsymbol{c}} = -\\log Z(\\boldsymbol{\\theta }) $$\n接着应用最大似然：\n$$ \\begin{align} \\boldsymbol{\\theta ^{\\ast}} \u0026amp; = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta }) \\\\ \u0026amp;= \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }, \\boldsymbol{z}} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log u_{\\boldsymbol{\\theta }}(\\boldsymbol{w},\\boldsymbol{c})\\exp(z^{\\boldsymbol{c}}) \\end{align} $$ 这样的结果就是，为了最大化期望，会使得$Z(\\boldsymbol{\\theta }) \\to 0$，效果会很不好\n曲径通幽 那么 Noise Contrastive Estimation（NCE）说，既然这样，我们能不能引入参数的同时也可以出色地预估$Z(\\boldsymbol{\\theta })$呢？于是乎，它将问题从原本的多分类问题转换为二分类问题 Proxy Problem，指用新的任务或指标来完成对原本任务的建模 ，具体如下：\n首先，存在一个噪声分布$p_{n}$和经验概率分布$p_{d}$，这里$p_{d}$是从训练集提取的，就类似 word2vec 的训练，将句子切分成词 现代 NLP 基本都是 token，这里是为了表达简便 ，统计某几个词一起出现的概率，那么$p(\\boldsymbol{w}|\\boldsymbol{c})$就是对于$\\boldsymbol{c}$而言，下一个词是$\\boldsymbol{w}$的概率。举个例子，对于 love 而言：$p_{d}=\\{ \\text{games}: 0.9, \\text{study}: 0.1 \\}$\n每次从$p_{d}$中抽出一个候选词，从$p_{n}$中抽取$k$个候选词。模型的任务即为区分候选词是从训练集还是噪声中采样而来的，通过这个代理任务使得$p_{\\boldsymbol{\\theta}}(\\boldsymbol{c})$去逼近于$p_{d}(\\boldsymbol{c})$\n我们规定，当$\\mathcal{D}=1$时代表从训练集采样，而$\\mathcal{D}=0$则代表从噪声中采样，那么：\n$$ \\begin{align} p(\\boldsymbol{w}|\\mathcal{D} =1, \\boldsymbol{c}) \u0026amp; = p_{d}(\\boldsymbol{w})\\\\ p(\\boldsymbol{w}|\\mathcal{D} = 0, {\\boldsymbol{c}}) \u0026amp; = p_{n}({\\boldsymbol{w}}) \\end{align} $$\n那么总概率即为：\n$$ p_{joint}(\\boldsymbol{w}) =\\frac{1}{k+1}p_{d}(\\boldsymbol{w}) + \\frac{k}{k+1} p_{n}(\\boldsymbol{w}) $$\n接下来求一下来自哪个采样的条件概率：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p(\\mathcal{D}=1,\\boldsymbol{w})}{p_{joint}(\\boldsymbol{w})} = \\frac{p(\\boldsymbol{w}|\\mathcal{D}=1)p(\\mathcal{D}=1)}{p_{joint}(\\boldsymbol{w})} \\\\ \u0026amp; = \\frac{p_{d}(\\boldsymbol{w})}{p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n同理：\n$$ p(\\mathcal{D}=0|\\boldsymbol{w}) = \\frac{kp_{n}(\\boldsymbol{w})}{p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} $$\n$\\mathcal{D}$代表训练集和噪声集的集合，那么 NCE 的目标即为最大化期望：\n$$ \\boldsymbol{\\theta }^{\\ast} = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }}\\ \\mathbb{E}_{\\boldsymbol{w} \\sim \\mathcal{D}} \\log p(\\mathcal{D}|\\boldsymbol{w}) $$\n又因为我们想要让模型分布$p_{\\boldsymbol{\\theta }}$尽可能接近训练集分布$p_{d}$，于是我们在求条件概率时，将$p_{d}$换成$p_{\\boldsymbol{\\theta }}$，即：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n我们展开期望看看：\n$$ \\begin{align} \\mathbb{E}_{\\boldsymbol{w}\\sim \\mathcal{D}} \\log p(\\mathcal{D}|\\boldsymbol{w}) \u0026amp; = \\int_{\\boldsymbol{w}} p(\\boldsymbol{w})\\log p(\\mathcal{D}|\\boldsymbol{w}) \\, d\\boldsymbol{w} \\\\ \u0026amp;= \\int_{\\boldsymbol{w}} \\frac{1}{k+1}(p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))\\log p(\\mathcal{D}|\\boldsymbol{w})\\, d \\boldsymbol{w} \\\\ \u0026amp;= \\frac{1}{k+1} \\left( \\int_{\\boldsymbol{w}} p_{d}(\\boldsymbol{w}) \\log p(\\mathcal{D}=1|\\boldsymbol{w}) \\, d \\boldsymbol{w} + \\int _{\\boldsymbol{w}} k p_{n}(\\boldsymbol{w}) \\log p(\\mathcal{D}=0|\\boldsymbol{w}) \\, d\\boldsymbol{w} \\right) \\\\ \u0026amp;= \\frac{1}{k+1}\\bigg(\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w})\\bigg) \\end{align} $$\n上述期望计算初看肯定有两处疑问：\n第一、为啥要基于$\\boldsymbol{w}$而非$\\boldsymbol{c}$来展开概率计算呢？当然两者都可以，但是我们的目标是为了让模型分布去拟合训练集分布，若是按照$\\boldsymbol{c}$展开，也就是$p_{\\boldsymbol{\\theta }}(\\boldsymbol{c})\\approx p_{d}(\\boldsymbol{c})$，让模型预测输入的 feature 不合理\n第二、不是说好用模型分布代替数据分布吗？为什么$p(\\boldsymbol{w})$还是用的数据分布，我想可能是为了训练方便考虑，若两处都是模型分布，训练势必更难；同时，数据分布是一个既定事实，可以充当额外的信息量给模型，加快收敛\n因为$k$是常数，对优化目标函数无影响，下式省略之，那么，我们的目标函数即为：\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n极限的视角 你肯定好奇 Proxy Problem 是否可以近似原来的建模，目标函数相对于$\\boldsymbol{\\theta }$的微分告诉了我们答案\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n那么，我们求关于参数$\\boldsymbol{\\theta }$的微分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } J(\\boldsymbol{\\theta }) \u0026amp; = \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+\\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) \\\\ \u0026amp;= \\mathbb{E}_{\\boldsymbol{w}\\sim p_{d}} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} + k\\mathbb{E}_{\\boldsymbol{w}\\sim p_{n}} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n那么接下来我们拆开来求目标函数相对于参数的微分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \u0026amp; = \\frac{p_{\\theta }(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{p_{\\theta }(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})} \\frac{p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})(p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))-p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{(p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))^{2} } \\\\ \u0026amp;= \\frac{p_{\\boldsymbol{\\theta }}'(\\boldsymbol{w})kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})(p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))} \\\\ \u0026amp;= \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} {\\color{blue}\\frac{p_{\\boldsymbol{\\theta }}'(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}} \\\\ \u0026amp;=\\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\end{align} $$\n另一部分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \u0026amp; =\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{kp_{n}(\\boldsymbol{w})} \\frac{0-kp_{n}(\\boldsymbol{w})p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{(p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))^{2}} \\\\ \u0026amp;= -\\frac{p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} {\\color{blue}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}}\\\\ \u0026amp;= - \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\end{align} $$\n合起来看看：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } J(\\boldsymbol{\\theta }) \u0026amp; = \\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) -k \\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}} p_{d}(\\boldsymbol{w}) \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) -k \\sum_{\\boldsymbol{w}}p_{n}(\\boldsymbol{w})\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}} \\underbrace{ {\\color{blue}\\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}} }_{ k \\to \\infty, ratio \\to 1 }\\bigg(p_{d}(\\boldsymbol{w}) - p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})\\bigg) \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) \\\\ \u0026amp;\\approx \\sum_{\\boldsymbol{w}}\\bigg(p_{d}(\\boldsymbol{w}) - p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})\\bigg) \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) \\end{align} $$\n对于第四步的近似，可以举个例子，比如$10000/(1+10000)$，其实因为$1$太小了，可以忽略不计\n这也是为什么这个 Proxy Problem 可以 work 的原因，当采样的噪声样本足够多时，NCE 的梯度就接近于一开始我们想要直接去做最大似然的梯度\n两次近似 尽管通过引入参数可以去估计$Z(\\boldsymbol{\\theta })$很巧妙，但有一个很大的问题，对于每一组词而言，尽管$\\mathcal{V}$是一样的，然而基于的$\\boldsymbol{c}$不一致，那么$p(\\boldsymbol{w}|\\boldsymbol{c})$也是不同的，即每组词你得去保存一个参数$z^{\\boldsymbol{c}}$\n这个时候作者 不是NCE提出论文，而是http://arxiv.org/abs/1206.6426 发现了「神之一手」，直接令$Z(\\boldsymbol{\\theta })\\approx 1$，也就是俗称的 self-normalization，换句话说，压根没有转换为概率，你看到这肯定会露出不屑的表情，我也一样\n自归一化 work 的原因是什么呢？引用原著的说法：\nWe believe this is because the model has so many free parameters that meeting the approximate per-context normalization constraint encouraged by the objective function is easy.\n作者的意思就是参数很多，于是就有了 power，模型自己可以去学习归一化，当然，原著中做了对比，发现效果几乎没影响，才这么做的\n其实我看来还是目标函数选的好，因为当梯度近似为$0$时，$p_{d}$和$p_{\\boldsymbol{\\theta }}$很接近\n这里其实有个容易误解的点，其实这个目标函数只是为了拟合一对词$(\\boldsymbol{c}, \\boldsymbol{w})$：\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n对于每组词都要计算期望，即考虑所有候选可能太过奢侈，所以原著进行了第二次近似，也有一些资料是说抽取$k$个是蒙特卡洛模拟的一种\n$$ J^{\\boldsymbol{c}}(\\boldsymbol{\\theta }) = \\log p(\\mathcal{D}=1|\\boldsymbol{w}_{0}) + \\sum_{i=1}^{k} \\log p(\\mathcal{D}=0|\\boldsymbol{w}_{i}) $$\n那么对于所有的词组该如何建模呢？我们定义一个全局 NCE 进行优化就行了：\n$$ J(\\boldsymbol{\\theta }) = \\sum_{\\boldsymbol{c}} p(\\boldsymbol{c)}J^{\\boldsymbol{c}}(\\boldsymbol{\\theta }) $$\nsigmoid 客串 当然，如果你看现在很多机器学习库的实现，你会发现跟上面的式子可能有点不一样？\n进行变形一下：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{1}{1+ \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}} \\\\ \u0026amp;= \\frac{1}{1+ \\exp\\left(\\log \\left( \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})} \\right)\\right)} \\\\ \u0026amp;= \\frac{1}{1+\\exp(\\log kp_{n}(\\boldsymbol{w})-\\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}))} \\\\ \u0026amp;= \\frac{1}{1+\\exp({\\color{red}-}(\\underbrace{ \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})-\\log kp_{n}(\\boldsymbol{w})) }_{ x })} \\end{align} $$\n将里面看成一个参数$x$，那么就看到了 sigmoid 函数：\n$$ p(\\mathcal{D}=1|\\boldsymbol{w}) = \\sigma(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})-\\log kp_{n}(\\boldsymbol{w})) $$\n同理：\n$$ p(\\mathcal{D}=0|\\boldsymbol{w}) = \\sigma(\\log kp_{n}(\\boldsymbol{w}) - \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})) $$\n损失函数基本就呼之欲出了：\n$$ \\begin{align} L(\\boldsymbol{\\theta }) \u0026amp; = - \\sum_{\\boldsymbol{c}} p(\\boldsymbol{c})\\left( \\log p(\\mathcal{D}=1|\\boldsymbol{w}_{0})+\\sum_{i=1}^{k} \\log p(\\mathcal{D}=0|\\boldsymbol{w}_{i}) \\right) \\\\ \u0026amp;= -\\sum_{\\boldsymbol{c}} p(\\boldsymbol{c}) \\left( \\log \\sigma(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{0})-\\log kp_{n}(\\boldsymbol{w}_{0})) + \\sum_{i=1}^{k} \\log \\sigma(\\log kp_{n}(\\boldsymbol{w}_{i})- \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i})))\\right) \\\\ \u0026amp;= -\\sum_{\\boldsymbol{c}} p(\\boldsymbol{c})\\bigg(\\log\\sigma(s_{\\boldsymbol{\\theta }}(\\boldsymbol{c}, \\boldsymbol{w}_{0} )-\\log kp_{n}(\\boldsymbol{w}_{0}))+\\sum_{i=1}^{k} \\log \\sigma(\\log kp_{n}(\\boldsymbol{w}_{i})- s_{\\boldsymbol{\\theta }}(\\boldsymbol{c}, \\boldsymbol{w}_{i}))\\bigg) \\end{align} $$\n上代码 这里的$p_{n}$选的是 log-uniform，类别越往后出现的概率就越小，所以如果要用，可以将类别按照数目进行排序，将多的放在前面，举个例子，类别 A, B, C, D 分别出现的数目为 10, 20, 100, 15，那么类别排序就应该是 C B D A，类别 0 对应的就是 C。range_max 对应的就是类别总数，这里就是 4\n$$ \\log_{uniform}(class) = \\frac{\\log(class + 2) - \\log (class + 1)}{\\log(range_{max} + 1)} $$\n下面是训练的 loss，eval 的时候没有 noise，找出 labels 对应的 logits，然后算指标就行了，同时，这里考虑数值稳定性，用 pytorch 官方的 softplus 来取代 logsigmoid，详见Numerical Stability\nimport math from einops import repeat import torch.nn.functional as F from torch import arange, randn, tensor, log, multinomial def nce_loss(logits_pos, logits_neg, log_pn_pos, log_pn_neg, k): \u0026#34;\u0026#34;\u0026#34;Compute the noise contrastive estimation loss in https://arxiv.org/abs/1806.03664. Params: - logits_pos: Tensor. Shape: (bs, 1). Logits corresponding to labels. - logits_neg: Tensor. Shape: (bs * k, 1). Logits corresponding to sampled classes. - log_pn_pos: Tensor. Shape: (bs, 1). Log-probability of labels sampled from noise distribution. - log_pn_neg: Tensor. Shape: (bs * k, 1). Log-probability of noise candidates sampled from noise distribution. - k: int. The number of noise candidates per training example. Note: This implementation assumes each context is equally shown which leads to final averge.\u0026#34;\u0026#34;\u0026#34; logk = math.log(k) # For numerical stability, replace logsigmoid by the torch softplus # for it considers the overflow situation. # log(sigmoid(x)) = -softplus(-x) # final return also contains minus(-), thus remove all the minus(-) pos = F.softplus((logk + log_pn_pos) - logits_pos).mean() neg = F.softplus(logits_neg - (logk + log_pn_neg)).mean() return pos + neg def log_uniform(num_sampled, range_max, replacement=True): \u0026#34;\u0026#34;\u0026#34;Sample classes from log-uniform distribution.: p(class) = (log(class + 2) - log(class + 1)) / (log(range_max + 1)). sampled_classes: [0, range_max). Also note that the data distribution should follow the log_uniform. e.g., the classes should be in decreasing order of frequencey when in text generation. Params: - num_sampled: int. The number to be sampled. - range_max: int. The number of total classes. - replacement: bool. If false, sampled candidates are unique. Examples: \u0026gt;\u0026gt;\u0026gt; log_uniform(2, 10) \u0026gt;\u0026gt;\u0026gt; # tensor([7, 2]) \u0026#34;\u0026#34;\u0026#34; classes = arange(0, range_max) probs = log((classes + 2) / (classes + 1)) / math.log(range_max + 1) return probs, multinomial(probs, num_sampled, replacement=replacement) def main(): bs, k = 2, 4 num_classes = 8 logits = randn(bs, num_classes) labels = tensor([2, 4]) probs, noise_classes = log_uniform(bs * k, num_classes) logits_pos = logits.take_along_dim(labels[:, None], dim=1) log_pn_pos = probs[labels] log_pn_neg = probs[noise_classes] logits_k = repeat(logits, \u0026#39;(b 1) h -\u0026gt; (b k) h\u0026#39;, k=k) logits_neg = logits_k.take_along_dim(noise_classes.reshape(bs * k, -1), dim=1) loss = nce_loss(logits_pos, logits_neg, log_pn_pos, log_pn_neg, k) print(\u0026#39;nce loss: %f\u0026#39; %loss) if __name__ == \u0026#39;__main__\u0026#39;: main() 至于实验，先鸽一下，留在后面与 info-nce，negative-sampling 等做对比\nReferences http://proceedings.mlr.press/v9/gutmann10a.html http://arxiv.org/abs/1206.6426 https://leimao.github.io/article/Noise-Contrastive-Estimation/ https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler ","permalink":"https://yunpengtai.top/posts/noise-contrastive-estimation/","summary":"难以承受之重 文本生成是 NLP 任务中比较典型的一类，记参数为$\\boldsymbol{\\theta }$，给定的 context 为$\\boldsymbol{c}$","title":"Noise Contrastive Estimation"},{"content":"问题 先规定一些术语：记选中元素构成的集合为$\\mathcal{S}$，未选中构成的元素记为$\\mathcal{R}$，$\\mathbf{L}$是核矩阵（核函数是内积），$\\mathbf{L_{V}}$是由集合$S$的元素构成的子矩阵\n在Determinatal Point Process中我们提到在大小为$n$的集合里去挑选$k$个物品构成集合$S$, 使得$\\det(\\mathbf{L}_{\\mathbf{V}})$最大便是我们的目标，然而，怎么去里面挑选$\\mathbf{V}$却是 NP-Hard 问题，为此，Chen et al., 2018 提出了一篇比较巧妙的贪婪算法作为近似解，并且整个算法的复杂仅有$\\mathcal{O}(nk^{2})$\n暴力求解 我们人为规定了要选择$k$个，这相当于是一种前验分布，那么 k-DPP 其实就是最大化后验概率（MAP）的一种，每一步的目标就是选择会让新矩阵的行列式变得最大的元素\n$$ j = \\mathop{ \\arg \\max}_{i \\in \\mathcal{R}} \\log \\det(\\mathbf{L}_{\\mathcal{S} \\cup \\{i\\}}) - \\log \\det(\\mathbf{L}_{\\mathcal{S}}) $$\n对于一个$n\\times n$的方阵而言，求它的行列式需要$\\mathcal{O}(n^{3})$（每一轮消元的复杂度是$\\mathcal{O}(n^{2})$，而要进行$n-1$轮消元）\n这里的话，每次要对$\\mathcal{R}$所有的元素求一次行列式，而行列式的为$\\mathcal{O}(|\\mathcal{S}|^{3})$，同时需要选$k$个，复杂度变为了$\\mathcal{O}(|\\mathcal{S}|^{3} \\cdot |\\mathcal{R}| \\cdot k)$，即为$\\mathcal{O}(nk^{4})$，暴力求解的话复杂度很大，此时原作者便提出了利用 Cholesky 分解的方式来进行求解，巧妙地将复杂度降到了$\\mathcal{O}(nk^{2})$\nCholesky 分解 $\\mathbf{L}_{\\mathcal{S}}$是对称半正定矩阵，证明如下：$\\forall \\boldsymbol{z} \\in \\mathbb{R}^{n}$\n$$ \\boldsymbol{z}^{\\top}\\mathbf{L}_{\\mathcal{S}}\\boldsymbol{z} = \\boldsymbol{z}^{\\top} \\mathbf{V}^{\\top}\\mathbf{V} \\boldsymbol{z} = \\|\\mathbf{V}\\boldsymbol{z}\\|_{2}^{2} \\geq 0 \\quad \\blacksquare $$\n那么$\\mathbf{L}_{\\mathcal{S}}$存在 Cholesky 分解，即$\\mathbf{L}_{\\mathcal{S}}=\\mathbf{U}\\mathbf{U}^{\\top}$，这里的$\\mathbf{U}$是大小为$|\\mathcal{S}|\\times|\\mathcal{S}|$的下三角矩阵，$\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}$比$\\mathbf{L}_{\\mathcal{S}}$多了一行和一列，即为：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} $$\n而这里默认每个向量是经过归一化的，即$\\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u}=1$，那么$\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}$的 Cholesky 分解即为下式，其中$\\boldsymbol{c}_{i} \\in \\mathbb{R}^{1 \\times |\\mathcal{S}| }, d_{i} \\geq 0$：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}^{\\top} $$\n结合上面两式：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{U}\\mathbf{U}^{\\top} \u0026amp; \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} \\\\ \\boldsymbol{c}_{i}\\mathbf{U}^{\\top} \u0026amp; \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} \\end{bmatrix} $$\n可得：\n$$ \\boldsymbol{u}_{i} = \\mathbf{U}\\boldsymbol{c}_{i}^{\\top}, 1 = \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} $$\n那么：\n$$ \\det(\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}) = \\det \\bigg(\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}^{\\top}\\bigg) = \\det(\\mathbf{U}\\mathbf{U}^{\\top})\\cdot d_{i}^{2}= \\det(\\mathbf{L}_{\\mathcal{S}}) \\cdot d_{i}^{2} $$\n这样我们一开始的优化目标就可以简化为：\n$$ j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log(d_{i}^{2}) $$\n接下来，当我们得到$d_j$时，便可以算出$c_{j}$，那么添加$j$之后的新集合$\\mathcal{S}'$的 Cholesky 分解便可以求得：\n$$ \\mathbf{L}_{\\mathcal{S}'} = \\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}^{\\top} $$\n增量更新 接下来便是重头戏，这一轮我们已经得到了最好的$d_{j}$了，下一轮我们怎么求出最大的$d_{i}$呢？\n可以利用之前求出的$c_{i}, d_{i}$来获取当前的$c_{i}', d_{i}'$，这便是论文的核心：增量更新\n在我们选择了$j$后，$c_{i}$多了一个元素，不妨记$\\boldsymbol{c}_{i}' = [\\boldsymbol{c}_{i}, {e}_{i}]$，回忆上面的式子：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{U}\\mathbf{U}^{\\top} \u0026amp; \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} \\\\ \\boldsymbol{c}_{i}\\mathbf{U}^{\\top} \u0026amp; \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} \\end{bmatrix} $$\n也就是说，其中$\\boldsymbol{u}_{i}$就是第$i$个元素与集合$\\mathcal{S}$对应向量做内积的结果\n$$ \\boldsymbol{u}_{i} = \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} $$\n那么，类比一下，$\\mathbf{U}_{j}$是$\\mathbf{L}_{\\mathcal{S}'}$的 Cholesky 分解\n$$ \\underbrace{\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}}_{\\mathbf{U}_{j}} \\boldsymbol{c}_{i}'^{\\top} = \\boldsymbol{u}_{i}' = \\begin{bmatrix} \\boldsymbol{u}_{i} \\\\ \\mathbf{L}_{ji} \\end{bmatrix} $$\n继而：\n$$ \\langle \\boldsymbol{c}_{j},\\boldsymbol{c} _{i}\\rangle + d_{j}e_{i} = \\mathbf{L}_{ji} \\implies e_{i} = \\frac{\\mathbf{L}_{ji}-\\langle \\boldsymbol{c}_{j},\\boldsymbol{c}_{i} \\rangle }{d_{j}} $$\n求出$e_{i}$之后，我们便可以求出$d_{i}'$：\n$$ d_{i}'^{2} = 1 - \\|\\boldsymbol{c}_{i}'\\|_{2}^{2} =\\underbrace{ 1- \\|\\boldsymbol{c}_{i}\\|_{2}^{2} }_{ {\\color{blue}d_{i}^{2} }} - e_{i}^{2} = d_{i}^{2} - e_{i}^{2} $$\n流程 \\begin{algorithm} \\caption{Fast Greedy MAP Inference} \\begin{algorithmic} \\STATE $\\textbf{Input: }$ Kernel Matrix $\\mathbf{L}$, stopping criteria \\STATE $\\textbf{Initialize: } \\boldsymbol{c}_i = [], d_i^2=\\mathbf{L}_{ii}, j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log (d_i^2), \\mathcal{S}=\\{j\\}$ \\WHILE{stopping criteria not satisfied} \\FOR{$i \\in \\mathcal{R}$} \\STATE $e_i = (\\mathbf{L}_{ji}-\\langle \\boldsymbol{c}_{j},\\boldsymbol{c} _{i}\\rangle)/d_j$ \\STATE $\\boldsymbol{c}_i = [\\boldsymbol{c}_i, e_i], d_i^2=d_i^2-e_i^2$ \\ENDFOR \\STATE $j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log(d_i^2), \\mathcal{S}= \\mathcal{S} \\cup \\{j\\}$ \\ENDWHILE \\STATE $\\textbf{Output: } \\mathcal{S}$ \\end{algorithmic} \\end{algorithm} 那我们来分析一下复杂度，每选一个$j$需要进行$|\\mathcal{R}| \\cdot |\\mathcal{S}|$次操作，而$|\\mathcal{R}|\\leq n, |\\mathcal{S}| \\leq k$，也就是$\\mathcal{O}(nk)$，得进行$k$次迭代，那么总的复杂度即为$\\mathcal{O}(nk^{2})$，由$\\mathcal{O}(nk^{4})$降到$\\mathcal{O}(nk^{2})$，是不错的进步\n代码 熟悉了整个流程之后，代码想必也是呼之欲出了\nimport math import numpy as np def fast_map_dpp(kernel_matrix, max_length): cis = np.zeros((max_length, kernel_matrix.shape[0])) di2s = np.copy(np.diag(kernel_matrix)) selected = np.argmax(di2s) selected_items = [selected] while len(selected_items) \u0026lt; max_length: idx = len(selected_items) - 1 ci_optimal = cis[:idx, selected] di_optimal = math.sqrt(di2s[selected]) elements = kernel_matrix[selected, :] eis = (elements - ci_optimal @ cis[:idx, :]) / di_optimal cis[idx, :] = eis di2s -= np.square(eis) di2s[selected] = -np.inf selected = np.argmax(di2s) selected_items.append(selected) return selected_items 这里实现比较有趣的点就是，尽管伪代码中是$i \\in \\mathcal{R}$，这里其实是全部算了，但他对已选的进行了后处理，置之为$-\\infty$\n接下来我们实操一下，从句子对匹配BQ Corpus（Bank Question Corpus）拿出一条来看一下效果，首先是将其用预训练模型转换为表征向量，接着进行归一化操作，为了更好地看出DPP的效果，我们先用最大化内积来召回50个样本，再用DPP从这里召回10个具有多样性的样本：\n原句：我现在申请微粒货？ [\u0026#39;我现在申请微粒货？\u0026#39;, \u0026#39;申请微贷粒\u0026#39;, \u0026#39;申请微贷粒\u0026#39;, \u0026#39;我想申请微粒贷\u0026#39;, \u0026#39;可以么想申请微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我现在需要申请\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;, \u0026#39;我要求申请\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;申请贷款\u0026#39;, \u0026#39;如何申请微粒贷\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;想办理微粒贷业务\u0026#39;, \u0026#39;申请贷款\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;可以么想申请微粒贷\u0026#39;, \u0026#39;开通微米粒\u0026#39;, \u0026#39;想开通\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;想开通\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;开通粒微贷\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;开通申请\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;开通微\u0026#39;] 可以看到有不少重复且意思一样的样本：\n接着看DPP的效果：\n[\u0026#39;我现在申请微粒货？\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;] 可以发现里面没有重复的情况，而且语义具备多样性，而值得注意的是，此时就有一些和我们的原句意思不匹配的情况，在应用时可以自定义新的 kernel，让它同时注意相似性和多样性；或者可以对 DPP 的样本进行后处理等\nSliding Window 当$|\\mathcal{S}|$相当大的时候，就会有相似的样本开始出现，即超平行体会开始坍缩，不妨我们将$\\mathcal{S}$缩小成一个滑动窗口$\\mathcal{W}$，我们仅仅需要保证窗口内的样本具备多样性即可，即：\n$$ j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log \\det(\\mathbf{L}_{\\mathcal{W} \\cup \\{ i \\}}) - \\log \\det(\\mathbf{L}_{\\mathcal{W}}) $$\n推荐系统中有短序推荐（Short Sequence Recommendation）的说法，推荐的时候只考虑用户短期内的一些行为，而长序推荐会考虑一个较长时间跨度来进行推荐\nWindow size 的选择也是比较重要的，不妨看一些 demo：\n$$ \\begin{array}{ccc} \\hline \\text{窗口} \u0026amp; \\# \\text{不同样本} \u0026amp; \\# \\text{重复样本} \\\\ \\hline 2 \u0026amp; 2 \u0026amp; 0 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\\\ 4 \u0026amp; 1 \u0026amp; 2 \\\\ 5 \u0026amp; 1 \u0026amp; 1 \\\\ 7 \u0026amp; 1 \u0026amp; 0 \\\\ 9 \u0026amp; 0 \u0026amp; 0 \\\\ \\hline \\end{array}\\notag $$\n如果我们的目的是为了通过 Sliding Window 获取与直接 DPP 召回不一样的结果，窗口的大小要适当地小一些，然而小了导致看的范围小了，很有可能最后结果出现重复的情况，最好是将窗口设置到召回样本数目的$20\\% \\sim 30\\%$\n同时，为了防止样本重复，可以多召回一些，比较直觉的做法可以再加上一个 window 的大小，然后去重：\nw/o window [\u0026#39;开通\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;我现在申请微粒货？\u0026#39;] w/ window [\u0026#39;开通\u0026#39;, \u0026#39;开通申请\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我现在申请微粒货？\u0026#39;] 可以看到会有 3 个不一样的样本，还是比较有效的\n","permalink":"https://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/","summary":"问题 先规定一些术语：记选中元素构成的集合为$\\mathcal{S}$，未选中构成的元素记为$\\mathcal{R}$，$\\mathbf{L}","title":"Fast Greedy MAP Inference for DPP"},{"content":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子集的质量高同时多样性好。比如在推荐系统中，我们就希望给用户推荐的东西尽可能的有质量，同时具有差异性。\n而使得采样的子集尽可能具备多样性便是行列式点过程（Determinantal Point Process）大展身手的地方，俗称 DPP\n边缘分布 首先引入 DPP 的边缘分布定义，当我们某次采样出子集$\\mathbf{A}$，「包括」$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$的概率：\n`$$ P(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K_{V}})\n$$ `\n$\\mathbf{K}$是核矩阵（Kernel Matrix），即：\n` $$\n\\mathbf{K}{ij} = k(\\boldsymbol{v{i}}, \\boldsymbol{v_{j}})\n$$ `\n$\\mathbf{K_{V}}$是由$\\mathbf{V}$中元素构成的子矩阵，举个例子，假如$\\mathbf{V}=\\{ 1,2 \\}$，那么：\n` $$\nP(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K*{V}}) = \\left|\\begin{array}{cc} \\mathbf{K}*{11} \u0026amp; \\mathbf{K}{12} \\ \\mathbf{K}{21} \u0026amp; \\mathbf{K}{22} \\end{array}\\right| = \\mathbf{K}{11}\\mathbf{K}{22} - \\mathbf{K}{12}^{2}\n$$ `\n当$\\mathbf{K}_{12}$越大，则$\\{ 1,2 \\}$同时出现在$\\mathbf{V}$的概率就越小，从这个角度想，核函数应该是呈现出某种相似性 比如： $k(\\boldsymbol{v_{i}}, \\boldsymbol{v_{j}}) = \\boldsymbol{v_{i}}^{\\top} \\boldsymbol{v_{j}} $ 从正定性出发，严格的定义如下是：$\\mathbf{0}\\preceq\\mathbf{K} \\preceq \\mathbf{I}$\n举个例子：\n` $$\n\\mathbf{K} = \\begin{bmatrix} 1 \u0026amp; -0.3 \\ -0.3 \u0026amp; 1 \\end{bmatrix}\n$$ `\n其特征值为$0.7, -1.3$，不满足$\\mathbf{K}-\\mathbf{0} \\succeq \\mathbf{0}$，即不是半正定矩阵\nL-Ensemble 然而，上面边缘定义只是告诉我们采样时，某个子集被「包括」的概率，并非就是这个子集，而这个问题可以通过 L-Ensemble 去解\n` $$\nP(\\mathbf{V}=\\mathbf{A}) \\propto \\det(\\mathbf{L})\n$$ `\n这里的$\\mathbf{L}$省略了下标，跟上面的$\\mathbf{K}$一样，是跟$\\mathbf{V}$元素相关的子矩阵。$\\mathbf{L}$矩阵的核函数是内积是$\\boldsymbol{v_{i}^{\\top}\\boldsymbol{v_{j}}}$，$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$\n` $$\n\\mathbf{L} = \\mathbf{V}^{\\top}\\mathbf{V} = \\begin{bmatrix} \\langle \\boldsymbol{v*{1}}, \\boldsymbol{v*{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v*{1}},\\boldsymbol{v*{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v*{1}}, \\boldsymbol{v*{k}} \\rangle \\ \\langle \\boldsymbol{v*{2}},\\boldsymbol{v*{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v*{2}},\\boldsymbol{v*{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v*{2}},\\boldsymbol{v*{k}} \\rangle \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\langle \\boldsymbol{v*{k}},\\boldsymbol{v*{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v*{k}},\\boldsymbol{v*{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v*{k}},\\boldsymbol{v*{k}} \\rangle \\end{bmatrix}\n$$ `\n注意，这里指的不是概率，而是说概率「正比于」$\\mathbf{L}$矩阵的行列式，那么如何计算概率呢？也就是说我们得计算一个归一化常数（normalization constant），可以类比抛硬币，我们得去求总的抛起次数，除以它才能得到概率\n引入下述定理：\n` $$\n\\sum*{\\mathbf{A}\\subseteq \\mathbf{V} \\subseteq \\mathbf{S}} \\det(\\mathbf{L}) = \\det(\\mathbf{L} + \\mathbf{I*{\\bar{A}}})\n$$ `\n其中$\\mathbf{I_{\\bar{A}}}$是将单位矩阵中与$\\mathbf{A}$相关元素全部置零，举个例子，当$\\mathbf{S} = \\{ 1,2,3 \\}, \\mathbf{A}={1,2}$时：\n` $$\n\\mathbf{I_{\\bar{A}}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}\n$$ `\n那么如何求归一化常数呢，即将$\\mathbf{A}=\\emptyset$，当$\\mathbf{A}$为空集时，便包括了所有的情况，即：\n` $$\nP(\\mathbf{V}=\\mathbf{A}) = \\frac{\\det(\\mathbf{L})}{\\det(\\mathbf{L}+\\mathbf{I})}\n$$ `\n另外，L-Emsemble 的$\\mathbf{K}, \\mathbf{L}$对应关系如下：\n` $$\n\\begin{align} \\mathbf{K} \u0026amp; = \\mathbf{L}(\\mathbf{L} + \\mathbf{I})^{-1} \\ \\mathbf{L} \u0026amp; = \\mathbf{K}(\\mathbf{I}-\\mathbf{K})^{-1} \\end{align}\n$$ `\n直观解释 那么，行列式与多样性的直观解释是什么呢？\n多样性和相似性的意思正好相反，通常我们会定义相似性为两个向量之间做点积，即为$\\boldsymbol{v_{1}}^{\\top}\\boldsymbol{v_{2}}$，直观上看，两向量夹角的余弦值$\\cos \\theta$ 越大，相似性越高，反过来看，当$\\cos \\theta$最小即为两者相似性最差，多样性最好。显然，当两向量正交时多样性最好。\n那么，对于一个子集$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$而言，该如何定义它的多样性呢？不难想出，可以通过线性无关向量的数量来定义，若两两都互不线性相关，此时的子集的多样性是最好的。直观上可以转换为构成的超平行体的体积，下方为$k=2,3$时的示意图\n图源自王树森老师的课程\n为什么呢？可以拿平行六面体为例，若其中一个向量与其他向量线性相关，那么则会坍缩成一个平面，构不成平行六面体\n图源自3BlueBrown对于行列式的介绍\n只有当所有向量两两都线性无关时，构成的超平行体体积最大，即多样性最好\n而行列式可以表示体积，下式中$\\text{vol}$代表体积（volume），此时$k=d$，$\\mathbf{V}$为方阵\n` $$\n\\det(\\mathbf{V})= \\text{vol}(\\mathcal{P}(\\boldsymbol{v*{1}}, \\boldsymbol{v*{2}}, \\dots, \\boldsymbol{v_{k}}))\n$$ `\n也就是说，我们可以通过行列式的大小来定义多样性\n那么，$\\mathbf{L}$的行列式是否也跟体积有关呢？答案是肯定的：\n` $$\n\\det(\\mathbf{L}) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v*{1}}, \\boldsymbol{v*{2}}, \\dots,\\boldsymbol{v_{k}})\\bigg)^{2}\n$$ `\n接下来证明这一结论：\n由于$k \\leq d$，因为$d$维空间至多存在$d$个两两线性无关的向量，那么肯定存在一个$k$维子空间$\\mathcal{H}$，存在正交矩阵$\\mathbf{R} \\in \\mathbb{R}^{d \\times d}$，对向量$\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}}$进行旋转，使得$\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}}$都落在子空间$\\mathcal{H}$上。不妨设$\\mathcal{H}$的基底是前$k$个标准正交基，那么：\n` $$\n\\mathbf{R}\\boldsymbol{v*{i}} = \\begin{bmatrix} \\boldsymbol{u*{i}} \\ \\mathbf{0} \\end{bmatrix}\n$$ `\n$\\boldsymbol{u_{i}} \\in \\mathbb{R}^{k}$，$\\mathbf{0}$一共有$d-k$个，因为用$\\mathcal{H}$的基底向量表示，后面只能为$0$，将$\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}},\\dots, \\boldsymbol{u_{k}}$当作$\\mathbf{U}$的列，就有：\n` $$\n\\mathbf{RV} = \\begin{bmatrix} \\mathbf{U} \\ \\mathbf{0} \\end{bmatrix}, \\mathbf{U} \\in \\mathbb{R}^{k \\times k}\n$$ `\n显然，$\\mathcal{P}(\\boldsymbol{u_{1}}, \\dots)$与$\\mathcal{P}([\\boldsymbol{u_{1}};\\boldsymbol{0}],\\dots )$两者体积相等\n` $$\n\\text{vol}(\\mathcal{P}(\\boldsymbol{u*{1}}, \\boldsymbol{u*{2}},\\dots, \\boldsymbol{u*{k}})) = \\text{vol}\\bigg(\\mathcal{P}\\bigg(\\begin{bmatrix} \\boldsymbol{u*{1}} \\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{u*{2}} \\ \\boldsymbol{0} \\ \\end{bmatrix}, \\dots, \\begin{bmatrix} \\boldsymbol{u*{k}} \\ \\boldsymbol{0} \\end{bmatrix}\\bigg)\\bigg)\n$$ `\n那么：\n` $$\n\\text{vol}(\\mathcal{P}(\\boldsymbol{u*{1}}, \\boldsymbol{u*{2}}, \\dots, \\boldsymbol{u*{k}})) = \\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v*{1}}, \\mathbf{R}\\boldsymbol{v*{2}}, \\dots, \\mathbf{R}\\boldsymbol{v*{k}})\\bigg)\n$$ `\n由于对超平面体进行旋转不改变其体积（注意，这里是旋转而不是一般的线性变换，一般的线性变换不具备该性质）\n` $$\n\\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v*{1}}, \\mathbf{R}\\boldsymbol{v*{2}}, \\dots, \\mathbf{R}\\boldsymbol{v*{k}})\\bigg) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v*{1}}, \\boldsymbol{v*{2}}, \\dots, \\boldsymbol{v*{k}})\\bigg)\n$$ `\n那么：\n` $$\n\\text{vol}(\\mathcal{P}(\\boldsymbol{u*{1}}, \\boldsymbol{u*{2}}, \\dots, \\boldsymbol{u*{k}})) =\\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v*{1}}, \\mathbf{R}\\boldsymbol{v*{2}}, \\dots, \\mathbf{R}\\boldsymbol{v*{k}})\\bigg)\n$$ `\n又因为$\\mathbf{R}$正交矩阵，即$\\mathbf{R}^{\\top}\\mathbf{R} = \\mathbf{I}$，那么：\n` $$\n\\mathbf{U}^{\\top}\\mathbf{U} = (\\mathbf{RV})^{\\top}\\mathbf{RV} = \\mathbf{V}^{\\top}(\\mathbf{R}^{\\top}\\mathbf{R)}\\mathbf{V} = \\mathbf{V}^{\\top}\\mathbf{V}\n$$ `\n所以： $\\det(\\mathbf{U}^{\\top}) = \\det(\\mathbf{U})$ ` $$\n\\det(\\mathbf{V}^{\\top}\\mathbf{V}) = \\det(\\mathbf{U}^{\\top}\\mathbf{U}) = \\det(\\mathbf{U})^{2} = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v*{1}}, \\boldsymbol{v*{2}}, \\dots, \\boldsymbol{v_{k}})\\bigg)^{2}\n$$ `\n从 L-Emsemble 角度看，被采样的概率正比于构成的超平面体的体积，即两两之间线性无关更容易被采样出\nDemo 接下来我们用例子来看一下是否 DPP 能够采样出更有多样性的子集\nfrom torch import det, eye from transformers import set_seed from transformers import BertModel, BertTokenizer set_seed(42) pretrain_path = \u0026#34;fabriceyhc/bert-base-uncased-imdb\u0026#34; model = BertModel.from_pretrained(pretrain_path).cuda() tk = BertTokenizer.from_pretrained(pretrain_path) input_text = [ \u0026#34;I am happy because the weather is extremely good!\u0026#34;, \u0026#34;I hate the bad weather\u0026#34;, \u0026#34;The weather is extremely good!\u0026#34;, ] inputs = tk(input_text, max_length=128, return_tensors=\u0026#34;pt\u0026#34;, truncation=True, padding=True) inputs = {k: v.cuda() for k, v in inputs.items()} outputs = model(**inputs).pooler_output.T vtv = outputs.T @ outputs group_12 = vtv[:2][:, [0, 1]] I = eye(2).cuda() p_12 = det(group_12) / det(group_12 + I) group_13 = vtv[[0, 2]][:, [0, 2]] p_13 = det(group_13) / det(group_13 + I) print(\u0026#39;采样到第一个和第二个的概率：%f\u0026#39;%p_12) print(\u0026#39;采样到第一个和第三个的概率：%f\u0026#39;%p_13) # 采样到第一个和第二个的概率：0.983567 # 采样到第一个和第三个的概率：0.923823 然而，对于一个大小为$n$的集合，一共有$2^{n}$种组合，如何快速地进行 DPP 的计算以及如何最快找到大小为$k$的多样性最大的子集是比较困难的，留给下一篇 post $$\n","permalink":"https://yunpengtai.top/posts/determinantal-point-process/","summary":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子","title":"Determinantal Point Process"},{"content":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员\n$$ p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) $$ 其中$\\eta$被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而$T(y)$被称为统计充分量（sufficient statistic），通常而言：$T(y) = y$；$a(\\eta)$是对数配分函数（log partition）\n例子 接下来展示下伯努利分布和高斯分布都是指数族的一员\n期望为$\\phi$的伯努利分布且$y \\in \\{1, 0 \\}$，那么：\n$$ p(y=1) = \\phi; p(y=0) = 1-\\phi $$\n我们将上式压缩一下：\n$$ \\begin{align} p(y;\\phi ) \u0026amp; = \\phi^{y}(1-\\phi )^{1-y} \\\\ \u0026amp;= \\exp \\bigg(\\log(\\phi^{y}(1-\\phi )^{1-y})\\bigg) \\\\ \u0026amp;= \\exp(y\\log \\phi + (1-y)\\log(1-\\phi )) \\\\ \u0026amp;= \\exp\\left( \\left( \\log \\frac{\\phi }{1-\\phi } \\right)y + \\log(1-\\phi) \\right) \\end{align} $$\n那么可以对比着指数族的定义，易得$b(y)=1$以及 取倒数可以很方便简化分式 ：\n$$ \\begin{align} \\eta \u0026amp; = \\log\\left( \\frac{\\phi }{1-\\phi } \\right) \\\\ e^{\\eta } \u0026amp; = \\frac{\\phi}{1-\\phi } \\\\ {\\color{red}e^{-\\eta }} \u0026amp; = \\frac{1-\\phi}{\\phi } = \\frac{1}{\\phi } - 1 \\\\ \\phi \u0026amp; = \\frac{1}{1+e^{-\\eta }} \\end{align} $$\n发现很有趣的一点，$\\phi(\\eta)$不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：\n$$ \\begin{align} a(\\eta ) \u0026amp; = -\\log(1-\\phi ) = -\\log\\left( 1-\\frac{1}{1+e^{-\\eta }} \\right) \\\\ \u0026amp;= \\log (1+e^{\\eta }) \\end{align} $$\n接下来讨论高斯分布，因为$\\sigma^{2}$对$\\theta, h_{\\theta }(x)$是不影响的（相当于常数） 通过最大似然，发现$\\sigma$对目标函数的形式无影响 ，这里为了简化表示，约定$\\sigma^{2}=1$\n$$ \\begin{align} p(y;\\mu ) \u0026amp; = \\frac{1}{\\sqrt{ 2\\pi }}\\exp\\left( -\\frac{(y-\\mu )^{2}}{2} \\right) \\\\ \u0026amp;= \\underbrace{ \\frac{1}{\\sqrt{ 2\\pi }} \\exp\\left( -\\frac{y^{2}}{2} \\right) }_{ b(y) }\\exp\\left( \\mu y - \\frac{\\mu^{2}}{2} \\right) \\end{align} $$\n比对定义，可以发现：\n$$ \\eta = \\mu; a(\\eta ) = -\\frac{\\eta^{2}}{2} $$\n性质 $$ p(y;\\eta ) = b(y)\\exp (\\eta^{\\mathbf{T}}T(y) - a(\\eta)) $$ 性质1：指数族分布的期望是$a(\\eta)$对$\\eta$的一阶微分\n下面来证明上述观点：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) \u0026amp; = b(y) \\exp(\\eta^{\\mathbf{T}}y - a(\\eta )) \\left( y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\right) \\\\ \u0026amp;= yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\end{align} $$\n那么：\n$$ y p(y;\\eta ) = \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) $$ 又因为：\n$$ \\begin{align} \\mathbb{E}[Y;\\eta ] \u0026amp; = \\mathbb{E}[Y|X;\\eta] \\\\ \u0026amp;= \\int yp(y;\\eta ) \\,dy \\\\ \u0026amp;= \\int \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026amp;= \\int \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\, dy + \\int p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\int p(y;\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } \\cdot 1 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\cdot 1 \\\\ \u0026amp;= 0 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } a(\\eta) \\quad \\blacksquare \\end{align} $$\n性质2：指数族分布的方差是$a(\\eta)$对$\\eta$的二阶微分\n下面来证明方差：\n$$ \\begin{align} \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) \u0026amp; = \\frac{ \\partial }{ \\partial \\eta } \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg) \\\\ \u0026amp;= y\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg)\\bigg(y- \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= y^{2}p(y;\\eta ) - 2yp(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) + p(y;\\eta ) (\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) )^{2} - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) -p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\end{align} $$\n那么： $$ \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) +p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) $$\n又因为：\n$$ \\begin{align} \\mathbb{V}[Y;\\eta ] \u0026amp; = \\mathbb{V}[Y|X;\\eta] \\\\ \u0026amp;= \\int \\left( y - \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\right)^{2} p(y;\\eta)\\, dy \\\\ \u0026amp;= \\int \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }p(y;\\eta) + p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }a(\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta )\\int p(y;\\eta ) \\, dy \\\\ \u0026amp;= 0 + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\quad \\blacksquare \\end{align} $$ 性质3：指数族分布的NLL loss是concave的\n接下来证明NLL Loss是凸函数，其中$a(\\eta) \\in \\mathbb{R}^{m}, \\mathbf{y} \\in \\mathbb{R}^{m}$：\n$$ \\begin{align} J(\\eta) \u0026amp; = -\\log \\sum_{i}^{m} p(y^{(i)};\\eta_{i} ) \\\\ \u0026amp;= -\\log \\sum_{i}^{m} b(y^{(i)})\\exp(\\eta_{i} T(y^{(i)}) - a(\\eta_{i})) \\\\ \u0026amp;= -\\log \\sum_{i}^{m} b(y^{(i)}) \\exp( \\eta_{i} y^{(i)}- a(\\eta_{i})) \\\\ \u0026amp;= a(\\eta) -\\sum_{i}^{m} \\log b(y^{(i)}) + \\eta_{i}y^{(i)} \\end{align} $$\n同时因为自身的协方差矩阵是「半正定」的：\n$$ \\begin{align} \\nabla_{\\eta }^{2} J(\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) = \\mathbb{V}[\\mathbf{y};\\eta ] \\implies PSD\\quad \\blacksquare \\end{align} $$ 当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点\n构建GLM 在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：\n$y|x; \\theta \\sim \\mathrm{ExponentialFamily}(\\eta )$，也就是说，给定$x, \\theta$ 我们可以得到$y$的分布就是带有参数$\\eta$的指数族分布 给定$x$，我们需要去预测$y$，在指数族中，$y=T(y)$，也就是说我们得去预测期望，即学得的假设$h$需要去预测期望，即：$h(x) = \\mathbb{E}[y|x]$，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子： $$ h_{\\theta }(x) = p(y=1|x;\\theta ) = 0 \\cdot p(y=0|x;\\theta) + 1 \\cdot p(y=1|x;\\theta) = \\mathbb{E}[y|x;\\theta ] $$ 自然参数$\\eta$与$x$的联系是线性的，即$\\eta = \\theta^{\\mathbf{T}}x$，若$\\eta \\in \\mathbb{R}^{n}$，则$\\eta_{i} = \\theta_{i}^{\\mathbf{T}}{x}$ GLM例子 OLS Ordinary Least Squares（线性回归）中的$y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，即服从高斯分布，其中$\\eta = \\mu$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[y|x;\\theta] \\\\ \u0026amp; = \\mu \\\\ \u0026amp;= \\eta \\\\ \u0026amp;= \\theta^{\\mathbf{T}}{x} \\end{align} $$ 第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设\n逻辑回归 逻辑回归中$y \\in {0, 1}$，自然就想到了伯努利分布，即$y|x;\\theta \\sim \\text{Bernoulli}(\\phi )$，其中$\\phi=1/1+ e^{-\\eta }$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[y|x;\\theta ] \\\\ \u0026amp; = \\phi \\\\ \u0026amp;= \\frac{1}{1 + e^{-\\eta }} \\\\ \u0026amp;= \\frac{1}{1 + e^{-\\theta^{\\mathbf{T}}x}} \\end{align} $$ 是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来\n$g(\\eta) = \\mathbb{E}[T(y);\\eta ]$被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而$g^{-1}$被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）\nsoftmax回归 构建GLM 之前逻辑回归中是只有两类，当$y \\in \\{1, 2, \\dots, k\\}$，即现在是$k$分类，分布是multinomial distribution，接下来让我们构建GLM：\n规定$\\phi_{i}$规定了输出$y_{i}$的概率，那么$\\phi_{i}, \\dots, \\phi_{k-1}$即是我们的参数，那你肯定好奇为什么$\\phi_{k}$不是，因为输出所有类的概率之和为$1$，即$\\phi_{k}$可被其他的概率表示：\n$$ \\phi_{k} = 1-\\sum_{i}^{k-1} \\phi_{i} $$\n以往的$T(y)=y$，对于多分类而言，我们采用独热编码（one-hot），即：\n$$ T(1) = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2) = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\dots ,T(k-1) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, T(k) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$\n注意$T(y) \\in \\mathbb{R}^{k-1}$，因为$T(k)$定义为全零向量，那么如何表示$T(y)$的第$i$个元素呢？ 指示函数当满足条件时为$1$，否则为$0$ $$ (T(y))_{i} = \\mathbb{1}\\{y=i\\} $$\n接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当$y$取一个具体值时，只有一个指示函数为$1$，其他为$0$，即$\\phi_{i}^{0} = 1$\n$$ \\begin{align} p(y;\\phi ) \u0026amp; = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{\\mathbb{1}\\{y=k\\}}_{k} \\\\ \u0026amp; = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{1-\\sum_{i}^{k-1}\\mathbb{1}\\{y=i\\}}_{k} \\\\ \u0026amp;= \\phi_{1}^{(T(y))_{1}} \\phi_{2}^{(T(y)_{2})} \\dots \\phi_{k}^{1-\\sum_{i}^{k-1}(T(y))_{i}} \\\\ \\end{align} $$\n继续变形来跟定义做比较：\n$$ \\begin{align} p(y; \\phi)\u0026amp;= \\exp \\bigg((T(y))_{1}\\log \\phi_{1} + (T(y))_{2} \\log \\phi_{2} + \\dots +(1-\\sum_{i}^{k-1}(T(y))_{i})\\log \\phi_{k} \\bigg) \\\\ \u0026amp;= \\exp \\bigg((T(y))_{1}\\log \\frac{\\phi_{1}}{\\phi_{k}} + (T(y))_{2}\\log \\frac{\\phi_{2}}{\\phi_{k}} + \\dots+ (T(y))_{k-1}\\log \\frac{\\phi_{k-1}}{\\phi_{k}} + \\log \\phi_{k}\\bigg) \\end{align} $$\n那么：\n$$ \\eta = \\begin{bmatrix} \\log (\\phi_{1} / \\phi_{k}) \\\\ \\log (\\phi_{2} / \\phi_{k}) \\\\ \\vdots \\\\ \\log(\\phi_{k-1} / \\phi_{k}) \\end{bmatrix}, a(\\eta ) = -\\log(\\phi_{k}), b(y) =1 $$\n链接函数容易发现是：\n$$ \\eta_{i} = \\log \\frac{\\phi_{i}}{\\phi_{k}} $$\n接下来求响应函数：\n$$ \\begin{align} e^{\\eta_{i}} \u0026amp; = \\frac{\\phi_{i}}{\\phi_{k}} \\\\ \\phi_{k}e^{\\eta_{i}} \u0026amp; = \\phi_{i} \\\\ \\phi_{k}\\sum_{i}^{k} e^{\\eta_{i}} \u0026amp; = \\sum_{i}^{k} \\phi_{i} = 1 \\end{align} $$\n那么：\n$$ \\phi_{k} = \\frac{1}{\\sum_{i}^{k} e^{\\eta_{i}}} $$\n将$\\phi_{k}$代入上式：\n$$ \\phi_{i} = \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} $$\n这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：\n$$ \\begin{align} p(y=i|x;\\theta ) \u0026amp; = \\phi_{i} \\\\ \u0026amp;= \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} \\\\ \u0026amp;= \\frac{e^{\\theta_{i}^{\\mathbf{T}}x}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x}} \\end{align} $$\n多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：\n$$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[T(y)|x;\\theta ] \\\\ \u0026amp;= \\mathbb{E}\\left[\\begin{array}{c|} \\mathbb{1}\\{y=1\\} \\\\ \\mathbb{1}\\{y=2\\} \\\\ \\vdots \\\\ \\mathbb{1}\\{y=k-1\\} \\end{array} x;\\theta \\right] \\end{align} $$\n又因为：\n$$ \\mathbb{E}[(T(y))_{i}] = \\phi_{i} $$\n为啥会这样呢？因为对于$(T(y))_{i}$只有两个可能，$1$或$0$，那么它的期望是不是：\n$$ \\mathbb{E}[(T(y))_{i}] = 1 \\cdot \\phi_{i} + 0 \\cdot (1-\\phi ) = \\phi_{i} $$\n$$ h_{\\theta }(x)= \\begin{bmatrix} \\phi_{1} \\\\ \\phi_{2} \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix}= \\begin{bmatrix} \\frac{\\exp ({\\theta_{1}^{\\mathbf{T}}x)}}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\frac{\\exp ({\\theta_{2}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\vdots \\\\ \\frac{\\exp ({\\theta_{k-1}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\end{bmatrix} $$\n也就是说我们的假设函数需要输出每个类的概率，尽管只有$k-1$类，$\\phi_{k} = 1- \\sum_{i}^{k-1} \\phi_{i}$得到\n接下来进行最大似然估计并对$\\ell(\\theta)$进行化简：\n$$ \\begin{align} \\ell(\\theta ) \u0026amp; = \\sum_{i} \\log p(y^{(i)}|x^{(i)};\\theta ) \\\\ \u0026amp;= \\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026amp;= \\sum_{i}^{m} \\sum_{l=1}^{k} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026amp;= \\sum_{i}^{m} \\sum_{l=1}^{k} {\\color{red}\\mathbb{1}\\{y^{(i)}=l\\}} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left( \\log e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}} \\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left(\\theta_{l}^{\\mathbf{T}}x^{(i)}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)}-\\left( \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\underbrace{ \\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} }_{ 1 }\\right) \\\\ \u0026amp;= \\sum_{i}^{m} \\bigg(\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)} - \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\bigg) \\end{align} $$\n上述化简主要利用了指示函数的性质以及$\\log$的运算法则，同时$\\theta \\in \\mathbb{R}^{k \\times n}$，我们利用布局法来求：\n$$ \\begin{align} \\frac{ \\partial \\ell(\\theta ) }{ \\partial \\theta_{pq} } \u0026amp; = \\sum_{i}^{m} \\mathbb{1}\\{y^{(i)} = p\\} x^{(i)}_{q} - \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\\\ \u0026amp;= \\sum_{i}^{m} \\left( \\mathbb{1}\\{y^{(i)} = p\\} - \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)x^{(i)}_{q} \\end{align} $$\n因为这里是最大化$\\ell(\\theta)$，作为损失函数还应加个负号，这样才是最小，即\n$$ J(\\theta ) = -\\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} $$\n对应的微分如下：\n$$ \\frac{ \\partial J(\\theta ) }{ \\partial \\theta_{pq} } = \\sum_{i}^{m} \\left( \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} - \\mathbb{1}\\{y^{(i)} = p\\} \\right)x^{(i)}_{q} $$\n交叉熵 我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？\n联想交叉熵的定义：\n$$ H(P, Q) = - \\mathbb{E}_{x \\sim P}[\\log Q(x)] $$\n即使得模型输出的分布尽可能靠近训练集原来的分布：\n$$ \\theta^{\\ast} = \\mathop{\\arg \\min}_{\\theta} -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] $$\n我们接下来展开期望的计算：\n$$ -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] = \\frac{1}{m}\\underbrace{ -\\sum_{i}^{m} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} }_{ J(\\theta) } $$\n两者其实就差一个常数，本质是一样的\n","permalink":"https://yunpengtai.top/posts/generalized-linear-models/","summary":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) $$ 其中$\\eta$被称为分布的自然参数（nat","title":"Generalized Linear Models"},{"content":" 这是基于 Hugo 系列主题第一篇文章，因为之前是在 Jekyll 上进行渲染，故而 Hello World也有更新\n为啥变动 那么为啥从 Jekyll 变到 Hugo 呢？原因其实有几点：\n之前用的主题看着不好看，感觉很拥挤，之前想改没空，想要个简洁干净，专注于内容的主题 Jekyll 在服务器端的渲染速度实在是不快（本地却很快，不懂） Hugo 的设计更符合我的直觉，而且方便自定义好玩的功能，比如 shortcode 功能 主要是被另一位博主 Li\u0026rsquo;s Blog 用的主题吸引，就立马换成了新的主题 PaperMod，然而这个主题虽然简洁，但是少了一些我认为必须得有的功能，比如渲染公式，侧边目录等。于是任着喜欢「折腾」的性子，从零开始学 Hugo 的语法，然后四处借鉴学习，花了两天时间添加了一些功能，当然，这篇文章主要介绍相关的 feature，不会涉及到具体的实现\n没有一个主题能完全满足个人的需求，还是从底层原理出发，这样才能「自由自在」地进行更改\n支持features 基于的主题 PaperMod 的相关 features 便不在继续介绍，介绍一些我加入的特性\n公式渲染：这个主题一开始是没有公式渲染功能的，我通过引入 MathJax 来完成这一点，行内公式如：$a+b=c$，行间公式效果如下：\n$$ e^{\\pi i} + 1 = 0 $$\n然而，Hugo 是先将markdown渲染成HTML，接着才会轮到Mathjax进行渲染，这就导致很多公式就渲染不出，想了两种办法：- 将公式用div标签套起来，这样就不会被误判，然而这会增加平时写公式的负担，用shortcode来完成，还是有点臃肿 - 将公式用代码的样式包裹起来，再写一点js来保留原来公式的部分，比如公式（公式依旧带美元符号），这个更简便\n从平时的 markdown 公式书写转到带有代码样式包裹的公式可用下列代码： ```python def inline_replace(line): new_line = '' patterns = line.split('$') for (idx, l) in enumerate(patterns): tex = '`$' if idx % 2 == 0 else '$`' if idx == len(patterns)-1 and tex == '`$': tex = '' new_line += l + tex return new_line path = 'content/posts/Determinantal-Point-Process.md' lines = [line.rstrip() for line in open(path, encoding='utf-8')] count = 0 new_lines = [] for line in lines: if line == '': new_lines[-1] += '\\n' continue if line == '$$': if count % 2 == 0: new_lines.append('`$$') else: new_lines.append('$$`') count += 1 continue new_lines.append(inline_replace(line)) with open(path, 'w', encoding='utf-8') as w: for line in new_lines: w.write(line + '\\n') ``` 伪代码的书写：引入伪代码图片实在是丑得难以接受，我通过引入额外的js来实现这一点，本身是以HTML形式书写的，为了减轻负担，立马改成了shortcode，示例如下： \\begin{algorithm} \\caption{Quicksort} \\begin{algorithmic} \\PROCEDURE{Quicksort}{$A, p, r$} \\IF{$p \u003c r$} \\STATE $q = $ \\CALL{Partition}{$A, p, r$} \\STATE \\CALL{Quicksort}{$A, p, q - 1$} \\STATE \\CALL{Quicksort}{$A, q + 1, r$} \\ENDIF \\ENDPROCEDURE \\PROCEDURE{Partition}{$A, p, r$} \\STATE $x = A[r]$ \\STATE $i = p - 1$ \\FOR{$j = p$ \\TO $r - 1$} \\IF{$A[j] \u003c x$} \\STATE $i = i + 1$ \\STATE exchange $A[i]$ with $A[j]$ \\ENDIF \\STATE exchange $A[i]$ with $A[r]$ \\ENDFOR \\ENDPROCEDURE \\end{algorithmic} \\end{algorithm} 评论功能：这个功能我在之前的主题也实现过，不同于各种基于 github 的评论系统，需要账号，waline 不需要账户，十分方便，而且可以加入很多有趣的表情，可以在页面最底部输入试试看\n侧边目录：原主题对目录的设置是将目录固定放在文章顶部，这样不利于对于长文整体的把握，故而将目录移动到了侧边，并且修改了一些 css 设置，自动高亮当前目录并且给它添加下划线，这样可以清楚地发现此时的章节\nMarginNote：以往 markdown 都是仅支持脚注，这样不利于文章的阅读，查看相关引用信息需要移至页面底部再返回，而 MarginNote 则给了作者更多自由的空间和方便读者阅读 The Elements of Typographic Style, 2004 而且移动至相关的标注会自动高亮对应的 MarginNote，更加方便读者阅读；同时过长会自动转行，方便进行较长的标注 Sidenotes give more life and variety to the page and are the easiest of all to find and read, If carefully designed, they need not enlarge either the page or the cost of printing it. 不过为了保持设计的简便性，没有对移动端优化（谁在手机端看文章的啊，雾）\n代码渲染：原先 PaperMod 对代码的渲染真是一言难尽，属实是丑到我了，于是乎，重新自定义了相关主题，主体是用了 a11y-light.min.css 在其之上又自定义了一些颜色： 示例代码 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() # for convenience self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\u0026#34;\\tIn Model: input size\u0026#34;, input.size(), \u0026#34;output size\u0026#34;, output.size()) return output 更新时间：原先主题根本没有更新时间的设置，这样不利于读者查看文章最新的时间，文章的开头便有更新的时间，可以查看\n著作权声明：现在网上各种抄袭成风，需要对自己的文章进行声明，添加至文章末尾，例如本文用的是CC BY-NC-SA 4.0. Attribution-NonCommercial-ShareAlike 4.0 International 字体：原来的字体看着有点不舒服，这里将中文换成了霞鹜文楷，观感很不错，英文的话是在几个字体中选一个：-apple-system, BlinkMacSystemFont, segoe ui, Roboto, Oxygen, Ubuntu, Cantarell 这样看起来也很舒服，选取Lorem Ipsum 进行测试 Dummy text to test the appearance Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n中英文空白：汉学家称中文和英文之间的空白字元为「盘古之白」，大抵是因为劈开了全形字和半形字之间的混沌。在书写时适当地留白会提升观感，这个是通过盘古的js来实现的，手敲空格噼里啪啦的，倒不是很方便\nnotice：通过shortcode 添加了各种的提示框，看起来很不错，比如： 一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。 一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。 一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。 友链：添加了友链功能，这样方便添加别人的博客，加强朋友之间的联系，可查看主页的 Friends\n对于 markdown 引用的改进，原来的引用比较丑，下面是改进后的引用： Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell 点击图片进行放大，对于图片大，缩放多的情况，点击放大是很有必要的，不妨试试看： 致谢 搭建自己的主题借鉴了很多别人的经验，这里列出进行致谢：\nPaperMod，这是一开始基于的主题，感谢作者的开源\n评论系统的支持，感谢 Waline，做的很干净\n关于对代码主题的更改，主要借鉴了 HightLight.js 官方的demo\n公式的渲染方面 MathJax 做的很nice\n伪代码的书写主要借鉴了Pseudocode.js的官方教程，必须加鸡腿\n侧边目录和一些 shortcode 的实现借鉴了 Sulv\u0026rsquo;s Blog以及 Guan Qirui\n字体还得感谢 霞骛文楷，比那些收费的丑字体好太多了\n盘古之白是通过pangu.js实现的，点赞\nMarginNote 的实现借鉴了 kennethfriedman和 scripter\n图片放大主要用了fancybox的功能，用起来也比较方便\n秉持着开源的精神，我的网站源码也抽离出来放在这里，欢迎使用或者PR\n","permalink":"https://yunpengtai.top/posts/hello-world/","summary":"这是基于 Hugo 系列主题第一篇文章，因为之前是在 Jekyll 上进行渲染，故而 Hello World也有更新 为啥变动 那么为啥从 Jekyll 变到 Hugo 呢？原因其实有几点： 之前用的主题看","title":"新的主题"},{"content":"鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！\n关于此部分的代码，可以去这里查看\n在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的机器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份友谊值得一辈子铭记！这篇文章作为礼物赠与挚友。\n并行化的原因 我们在两种情况下进行并行化训练：\n模型一张卡放不下：我们需要将模型不同的结构放置到不同的GPU上运行，这种情况叫ModelParallel(MP) 单卡batch size过小：有些时候数据的最大长度调的比较高（e.g., 512），可用的bs就很小，较小的bs会导致收敛不稳定，因而将数据分发到多个GPU上进行并行训练，这种情况叫DataParallel(DP)。当然，DP肯定还可以加速训练，常见于大模型的训练中 这里只讲一下DP在pytorch中的原理和相关实现，即DataParallel和DistributedParallel\nData Parallel 实现原理 实现就是循环往复一个过程：数据分发，模型复制，各自前向传播，汇聚输出，计算损失，梯度回传，梯度汇聚更新，可以参见下图\npytorch中部分关键源码截取如下：\ndata_parallel源码 def data_parallel( module, input, device_ids, output_device=None ): if not device_ids: return module(input) if output_device is None: output_device = device_ids[0] # 复制模型 replicas = nn.parallel.replicate(module, device_ids) # 拆分数据 inputs = nn.parallel.scatter(input, device_ids) replicas = replicas[:len(inputs)] # 各自前向传播 outputs = nn.parallel.parallel_apply(replicas, inputs) # 汇聚输出 return nn.parallel.gather(outputs, output_device) 代码使用 因为运行时会将数据平均拆分到GPU上，所以我们准备数据的时候， batch size = per_gpu_batch_size * n_gpus\n同时，需要注意主GPU需要进行汇聚等操作，因而需要比单卡运行时「多留出一些空间」\nimport torch.nn as nn # device_ids默认所有可使用的设备 # output_device默认cuda:0 net = nn.DataParallel(model, device_ids=[0, 1, 2], output_device=None, dim=0) # input_var can be on any device, including CPU output = net(input_var) 接下来看个更详细的例子，需要注意的是被DP包裹之后涉及到模型相关的，需要调用DP.module，比如「加载模型」\ndata_parallel示例 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() # for convenience self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\u0026#34;\\tIn Model: input size\u0026#34;, input.size(), \u0026#34;output size\u0026#34;, output.size()) return output bs, input_size, output_size = 6, 8, 10 # define inputs inputs = torch.randn((bs, input_size)).cuda() model = Model(input_size, output_size) if torch.cuda.device_count() \u0026gt; 1: print(\u0026#34;Let\u0026#39;s use\u0026#34;, torch.cuda.device_count(), \u0026#34;GPUs!\u0026#34;) # dim = 0 [6, xxx] -\u0026gt; [2, ...], [2, ...], [2, ...] on 3 GPUs model = nn.DataParallel(model) # 先DataParallel，再cuda model = model.cuda() outputs = model(inputs) print(\u0026#34;Outside: input size\u0026#34;, inputs.size(), \u0026#34;output_size\u0026#34;, outputs.size()) # assume 2 GPUS are available # Let\u0026#39;s use 2 GPUs! # In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10]) # In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10]) # Outside: input size torch.Size([6, 8]) output_size torch.Size([6, 10]) # save the model torch.save(model.module.state_dict(), PATH) # load again model.module.load_state_dict(torch.load(PATH)) # do anything you want 如果经常使用huggingface，这里有两个误区需要小心：\ndata_parallel两个误区 # data parallel object has no save_pretrained model = xxx.from_pretrained(PATH) model = nn.DataParallel(model).cuda() model.save_pretrained(NEW_PATH) # error # 因为model被DP wrap了，得先取出模型 # model.module.save_pretrained(NEW_PATH) # HF实现貌似是返回N个loss（N为GPU数量） # 然后对N个loss取mean outputs = model(**inputs) loss, logits = outputs.loss, outputs.logits loss = loss.mean() loss.backward() # 返回的logits是汇聚后的 # HF实现和我们手动算loss有细微差异 # 手动算略好于HF loss2 = loss_fct(logits, labels) assert loss != loss2 True 显存不均匀 了解前面的原理后，就会明白为什么会显存不均匀。因为GPU0比其他GPU多了汇聚的工作，得留一些显存，而其他GPU显然是不需要的。那么，解决方案就是让其他GPU的batch size开大点，GPU0维持原状，即不按照默认实现的平分数据\n首先参考这里我们继承原来的DataParallel，这里我们给定第一个GPU的bs就可以，这个是实际的bs而不是乘上梯度后的。假如你想要总的bs为64，梯度累积为2，一共2张GPU，而一张最多只能18，那么保险一点GPU0设置为14，GPU1是18，也就是说你DataLoader每个batch大小是32，gpu0_bsz=14\nclass BalancedDataParallel(DataParallel): def __init__(self, gpu0_bsz, *args, **kwargs): self.gpu0_bsz = gpu0_bsz super().__init__(*args, **kwargs) 核心代码就在于我们重新分配chunk_sizes，实现思路就是将总的减去第一个GPU的再除以剩下的设备，源码的话有些死板，用的时候不妨参考我的：\n我修改后的scatter def scatter(self, inputs, kwargs, device_ids): # 不同于源码，获取batch size更加灵活 # 支持只有kwargs的情况，如model(**inputs) if len(inputs) \u0026gt; 0: bsz = inputs[0].size(self.dim) elif kwargs: bsz = list(kwargs.values())[0].size(self.dim) else: raise ValueError(\u0026#34;You must pass inputs to the model!\u0026#34;) num_dev = len(self.device_ids) gpu0_bsz = self.gpu0_bsz # 除第一块之外每块GPU的bsz bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1) if gpu0_bsz \u0026lt; bsz_unit: # adapt the chunk sizes chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - 1) delta = bsz - sum(chunk_sizes) # 补足偏移量 # 会有显存溢出的风险，因而最好给定的bsz是可以整除的 # e.g., 总的=52 =\u0026gt; bsz_0=16, bsz_1=bsz_2=18 # 总的=53 =\u0026gt; bsz_0=16, bsz_1=19, bsz_2=18 for i in range(delta): chunk_sizes[i + 1] += 1 if gpu0_bsz == 0: chunk_sizes = chunk_sizes[1:] else: return super().scatter(inputs, kwargs, device_ids) return scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim) 优缺点 优点：便于操作，理解简单 缺点：GPU分配不均匀；每次更新完都得销毁线程（运行程序后会有一个进程，一个进程可以有很多个线程）重新复制模型，因而速度慢 DDP 实现原理 与DataParallel不同的是，Distributed Data Parallel会开设多个进程而非线程，进程数 = GPU数，每个进程都可以独立进行训练，也就是说代码的所有部分都会被每个进程同步调用，如果你某个地方print张量，你会发现device的差异 sampler会将数据按照进程数切分，确保不同进程的数据不同 每个进程独立进行前向训练 每个进程利用Ring All-Reduce进行通信，将梯度信息进行聚合 每个进程同步更新模型参数，进行新一轮训练 如何确保数据不同呢？\nDistributedSampler的源码 # 判断数据集长度是否可以整除GPU数 # 如果不能，选择舍弃还是补全，进而决定总数 # If the dataset length is evenly divisible by # of replicas # then there is no need to drop any data, since the dataset # will be split equally. if (self.drop_last and len(self.dataset) % self.num_replicas != 0): # num_replicas = num_gpus self.num_samples = math.ceil((len(self.dataset) - self.num_replicas) /self.num_replicas) else: self.num_samples = math.ceil(len(self.dataset) / self.num_replicas) self.total_size = self.num_samples * self.num_replicas # 根据是否shuffle来创建indices if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) indices = torch.randperm(len(self.dataset), generator=g).tolist() else: indices = list(range(len(self.dataset))) if not self.drop_last: # add extra samples to make it evenly divisible padding_size = self.total_size - len(indices) if padding_size \u0026lt;= len(indices): # 不够就按indices顺序加 # e.g., indices为[0, 1, 2, 3 ...]，而padding_size为4 # 加好之后的indices[..., 0, 1, 2, 3] indices += indices[:padding_size] else: indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size] else: # remove tail of data to make it evenly divisible. indices = indices[:self.total_size] assert len(indices) == self.total_size # subsample # rank代表进程id indices = indices[self.rank:self.total_size:self.num_replicas] return iter(indices) 接下来用Ring All-Reduce可以降低通信成本\n首先将每块GPU上的梯度拆分成四个部分\\(1\\)，比如\\(g_0 = [a_0; b_0; c_0; d_0]\\)，此部分原理主要参考王老师\n所有GPU的传播都是同步进行的，传播的规律有两条：\n只与自己「下一个位置」的GPU进行通信，比如0 \u0026gt; 1，3 \u0026gt; 0 四个部分，哪块GPU上占的多，就由该块GPU往它下一个传，初始从主节点传播，即GPU0，你可以想象跟接力一样，a传b，b负责传给c 第一次传播如下：\n那么结果就是：\n那么，按照谁多谁往下传的原则，此时应该是GPU1往GPU2传a0和a1，GPU2往GPU3传b1和b2，以此类推\n接下来再传播就会有GPU3 a的部分全有，GPU0上b的部分全有等，就再往下传\n再来几遍便可以使得每块GPU上都获得了来自其他GPU的梯度啦\n代码使用 第一个是后端的选择，即数据传输协议，当使用CPU时可以选择gloo或mpi，而GPU则可以是nccl\n接下来是一些参数的解释\nArg Meaning group 一次发起的所有进程构成一个group，除非想更精细通信，创建new_group world_size 一个group中进程数目，即为GPU的数量 rank 进程id，主节点rank=0，其他的在0和world_size-1之间 local_rank 进程在本地节点/机器的id 举个例子，假如你有两台服务器（又被称为node），每台服务器有4张GPU，那么，world_size即为8，rank=[0, 1, 2, 3, 4, 5, 6, 7], 每个服务器上的进程的local_rank为[0, 1, 2, 3]\n然后是初始化方法的选择，有TCP和共享文件两种，一般指定rank=0为master节点\nTCP显而易见是通过网络进行传输，需要指定主节点的ip（可以为主节点实际IP，或者是localhost）和空闲的端口\nimport torch.distributed as dist dist.init_process_group(backend, init_method=\u0026#39;tcp://ip:port\u0026#39;, rank=rank, world_size=world_size) 共享文件的话需要手动删除上次启动时残留的文件，加上官方有一堆警告，还是建议使用TCP\ndist.init_process_group(backend, init_method=\u0026#39;file://Path\u0026#39;, rank=rank, world_size=world_size) 这里先讲用launch初始化的方法，关于torch.multiprocessing留到后面讲\n在启动后，rank和world_size都会自动被DDP写入环境中，可以提前准备好参数类，如argparse这种\nargs.rank = int(os.environ[\u0026#39;RANK\u0026#39;]) args.world_size = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) args.local_rank = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) 首先，在使用distributed包的任何其他函数之前，按照tcp方法进行初始化，需要注意的是需要手动指定一共可用的设备CUDA_VISIBLE_DEVICES\nlaunch初始化 def dist_setup_launch(args): # tell DDP available devices [NECESSARY] os.environ[\u0026#39;CUDA_VISIBLE_DEVICES\u0026#39;] = args.devices args.rank = int(os.environ[\u0026#39;RANK\u0026#39;]) args.world_size = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) args.local_rank = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) dist.init_process_group(args.backend, args.init_method, rank=args.rank, world_size=args.world_size) # this is optional, otherwise you may need to specify the # device when you move something e.g., model.cuda(1) # or model.to(args.rank) # Setting device makes things easy: model.cuda() torch.cuda.set_device(args.rank) print(\u0026#39;The Current Rank is %d | The Total Ranks are %d\u0026#39; %(args.rank, args.world_size)) 接下来创建DistributedSampler，是否pin_memory，根据你本机的内存决定。pin_memory的意思是提前在内存中申请一部分专门存放Tensor。假如说你内存比较小，就会跟虚拟内存，即硬盘进行交换，这样转义到GPU上会比内存直接到GPU耗时。\n因而，如果你的内存比较大，可以设置为True；然而，如果开了导致卡顿的情况，建议关闭\n加载数据 from torch.utils.data import DataLoader, DistributedSampler train_sampler = DistributedSampler(train_dataset, seed=args.seed) train_dataloader = DataLoader(train_dataset, pin_memory=True, shuffle=(train_sampler is None), batch_size=args.per_gpu_train_bs, num_workers=args.num_workers, sampler=train_sampler) eval_sampler = DistributedSampler(eval_dataset, seed=args.seed) eval_dataloader = DataLoader(eval_dataset, pin_memory=True, batch_size=args.per_gpu_eval_bs, num_workers=args.num_workers, sampler=eval_sampler) 然后加载模型，跟DataParallel不同的是需要提前放置到cuda上，还记得上面关于设置cuda_device的语句嘛，因为设置好之后每个进程只能看见一个GPU，所以直接model.cuda()，不需要指定device\n同时，我们必须给DDP提示目前是哪个rank\nfrom torch.nn.parallel import DistributedDataParallel as DDP model = model.cuda() # tell DDP which rank model = DDP(model, find_unused_parameters=True, device_ids=[rank]) 注意，当模型带有Batch Norm时：\nif args.syncBN: nn.SyncBatchNorm.convert_sync_batchnorm(model).cuda() 现在说训练的事情，每个epoch开始训练的时候，记得用sampler的set_epoch，这样使得每个epoch打乱顺序是不一致的\n关于梯度回传和参数更新，跟正常情况无异\nfor epoch in range(epochs): # record epochs train_dataloader.sampler.set_epoch(epoch) outputs = model(inputs) loss = loss_fct(outputs, labels) loss.backward() optimizer.step() optimizer.zero_grad() 这里有一点需要小心，这个loss是各个进程的loss之和，如果想要存储每个step平均损失，可以进行all_reduce操作，进行平均，不妨看官方的小例子来理解下：\nreduce相关代码 \u0026gt;\u0026gt;\u0026gt; # All tensors below are of torch.int64 type. \u0026gt;\u0026gt;\u0026gt; # We have 2 process groups, 2 ranks. \u0026gt;\u0026gt;\u0026gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank \u0026gt;\u0026gt;\u0026gt; tensor tensor([1, 2]) # Rank 0 tensor([3, 4]) # Rank 1 \u0026gt;\u0026gt;\u0026gt; dist.all_reduce(tensor, op=ReduceOp.SUM) \u0026gt;\u0026gt;\u0026gt; tensor tensor([4, 6]) # Rank 0 tensor([4, 6]) # Rank 1 @torch.no_grad() def reduce_value(value, average=True): world_size = get_world_size() if world_size \u0026lt; 2: # 单GPU的情况 return value dist.all_reduce(value) if average: value /= world_size return value 看到这，肯定有小伙伴要问，那这样我们是不是得先求平均损失再回传梯度啊，不用，因为，当我们回传loss后，参照这里，DDP会自动对所有梯度进行平均，也就是说回传后我们更新的梯度和DP或者单卡同样batch训练都是一致的\nloss = loss_fct(...) loss.backward() # 注意在backward后面 loss = reduce_value(loss, world_size) mean_loss = (step * mean_loss + loss.item()) / (step + 1) 还有个注意点就是学习率的变化，这个是和batch size息息相关的，如果batch扩充了几倍，也就是说step比之前少了很多，还采用同一个学习率，肯定会出问题的，这里，我们进行线性增大，感兴趣可以看讨论\nN = world_size lr = args.lr * N 肯定有人说，诶，你线性增大肯定不能保证梯度的variance一致了，正确的应该是正比于\\(\\sqrt{N}\\)，关于这个的讨论不妨参考这里\n接下来，细心的同学肯定好奇了，如果验证集也切分了，metric怎么计算呢？此时就需要咱们把每个进程得到的预测情况集合起来，t就是一个我们需要gather的张量，最后将每个进程中的t按照第一维度拼接，先看官方小例子来理解all_gather\ngather相关代码 \u0026gt;\u0026gt;\u0026gt; # All tensors below are of torch.int64 dtype. \u0026gt;\u0026gt;\u0026gt; # We have 2 process groups, 2 ranks. \u0026gt;\u0026gt;\u0026gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)] \u0026gt;\u0026gt;\u0026gt; tensor_list [tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1 \u0026gt;\u0026gt;\u0026gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank \u0026gt;\u0026gt;\u0026gt; tensor tensor([1, 2]) # Rank 0 tensor([3, 4]) # Rank 1 \u0026gt;\u0026gt;\u0026gt; dist.all_gather(tensor_list, tensor) \u0026gt;\u0026gt;\u0026gt; tensor_list [tensor([1, 2]), tensor([3, 4])] # Rank 0 [tensor([1, 2]), tensor([3, 4])] # Rank 1 def sync_across_gpus(t, world_size): gather_t_tensor = [torch.zeros_like(t) for _ in range(world_size)] dist.all_gather(gather_t_tensor, t) return torch.cat(gather_t_tensor, dim=0) 可以简单参考我前面提供的源码的evaluate部分，我们首先将预测和标签比对，把结果为bool的张量存储下来，最终gather求和取平均。\n这里还有个有趣的地方，tensor默认的类型可能是int，bool型的res拼接后自动转为0和1了，另外bool型的张量是不支持gather的\neval函数 def eval(...) results = torch.tensor([]).cuda() for step, (inputs, labels) in enumerate(dataloader): outputs = model(inputs) res = (outputs.argmax(-1) == labels) results = torch.cat([results, res], dim=0) results = sync_across_gpus(results, world_size) mean_acc = (results.sum() / len(results)).item() return mean_acc 模型保存，参考部分官方教程，我们只需要在主进程保存模型即可，注意，这里是被DDP包裹后的，DDP并没有state_dict，这里barrier的目的是为了让其他进程等待主进程保存模型，以防不同步\n保存模型 def save_checkpoint(rank, model, path): if is_main_process(rank): # All processes should see same parameters as they all # start from same random parameters and gradients are # synchronized in backward passes. # Therefore, saving it in one process is sufficient. torch.save(model.module.state_dict(), path) # Use a barrier() to keep process 1 waiting for process 0 dist.barrier() 加载的时候别忘了map_location，我们一开始会保存模型至主进程，这样就会导致cuda:0显存被占据，我们需要将模型remap到其他设备\ndef load_checkpoint(rank, model, path): # remap the model from cuda:0 to other devices map_location = {\u0026#39;cuda:%d\u0026#39; % 0: \u0026#39;cuda:%d\u0026#39; % rank} model.module.load_state_dict( torch.load(path, map_location=map_location) ) 运行结束后记得销毁进程：\ndef cleanup(): dist.destroy_process_group() cleanup() 然后如何启动呢？在终端输入下列命令「单机多卡」\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS main.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 目前torch 1.10以后更推荐用run\ntorch.distributed.launch -\u0026gt; torch.distributed.run / torchrun 多机多卡是这样的：\n多机多卡启动 # 第一个节点启动 python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=0 \\ --master_addr=\u0026#34;192.168.1.1\u0026#34; \\ --master_port=1234 main.py # 第二个节点启动 python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=1 \\ --master_addr=\u0026#34;192.168.1.1\u0026#34; \\ --master_port=1234 main.py 第二个方法就是利用torch的多线程包\nimport torch.multiprocessing as mp # rank mp会自动填入 def main(rank, arg1, ...): pass if __name__ == \u0026#39;__main__\u0026#39;: mp.spawn(main, nprocs=TOTAL_GPUS, args=(arg1, ...)) 这种运行的时候就跟正常的python文件一致：\npython main.py 优缺点 优点： 相比于DP而言，不需要反复创建和销毁线程；Ring-AllReduce算法提高通信效率；模型同步方便 缺点：操作起来可能有些复杂，一般可满足需求的可先试试看DataParallel ","permalink":"https://yunpengtai.top/posts/diving-in-distributed-training/","summary":"鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！ 关于此部分的代码，可以去这","title":"Diving in distributed training in PyTorch"},{"content":"1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.\n$$ \\mathbf{w}^{\\tau + 1} = \\mathbf{w}^{\\tau} - \\eta \\nabla_{\\mathbf{w}^{\\tau}} E \\tag{1.1} $$\nwhere \\(\\eta, \\tau, E\\) label learning rate (\\(\\eta \u0026gt; 0\\)), the iteration step and the loss function.\nWait! But why is the negative gradient?\n2. Why negative gradient The function increases the most sharply by following the direction of the gradient.\nThe below is an example. The three-dimensional plane is \\(z = F(x, y)\\). The black point is on the plane. You can try to move the point to see how the arrow changes. Interestingly, the arrow always points to the direction which leads to the biggest increase of the function value. Note that when you move one step, the gradient just changes. Thus if you still want to increase the function value in the most sharp way, another computation is needed.\nThe starting point of the arrow is the mapping of the black point to the \\(xoy\\) plane. The arrow is parallel to the gradient.\nthe illustration of the direction of gradient Let us use another graph to better understand what the mapping means. The left graph is contour plot while the right is the plane. The red point is just the mapping of the black point to $xoy$ plane. The blue arrow is just the direction of the gradient. And you can move the point to feel about it.\nanother illustration That is the intuitive way to feel about the gradient. Furthermore, we can just try to prove it.\nConsider a Taylor Expansion:\n$$ \\begin{aligned} F(\\mathbf{r_0} + \\mathbf{r}) \u0026= F(\\mathbf{r}_0) + \\nabla_{\\mathbf{r}_0}F \\cdot \\mathbf{r}\\\\ \u0026= F(\\mathbf{r}_0) + \\|\\nabla_{\\mathbf{r}_0}F\\|\\cdot \\|\\mathbf{r}\\| \\cdot \\cos \\theta \\end{aligned}\\tag{2.1} $$ When you decide to move a small step, the two magnitudes are certain. If $\\theta=0$, you can maximize the function value (i.e. in the direction of the gradient).\nThus if we want to minimize our loss function, we need to go in the opposite direction of the gradient. That is why we need a negative gradient. Also, note that Taylor Expansion only applies to small \\(\\Delta x\\) which further requires \\(\\eta\\) to be small (e.g. \\(2 \\times 10^{-5}, 5 \\times 10^{-5}\\)).\nBut how to compute the gradient needs a powerful technique: back-propagation.\n3. Definition of back-propagation Back-propagation allows information from the cost to then flow backwards through the network, in order to compute the gradients used to adjust the parameters.\nBack-propagation can be new to the novices, but it does exist in the life widely. For instance, the loss can be your teacher\u0026rsquo;s attitude towards you. If you fail in one examination, your teacher can be disappointed with you. Then, he can tell your parents about your failure. Your parents then ask you to work harder to win the examination.\nYour parents can be seen as hidden units in the neural network, and you are the parameter of the network. Your teacher\u0026rsquo;s bad attitude towards your failure can ask you to make adjustments: working harder. Similarly, the loss can require the parameters to make adjustments via gradients.\n4. Chain Rule Suppose \\(z = f(y), y = g(x) \\implies z = (f \\circ g)(x)\\), how to calculate the derivative of \\(z\\) with respect to \\(x\\)? The chain rule of calculus is used to compute the derivatives of functions formed by composing other functions whose derivatives are known.\n$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} \\tag{4.1} $$\n5. Case Study Let\u0026rsquo;s first see an important example. In fully connected layers, one input neuron sends information (i.e., multiplied by weights) to every output neuron. Denote \\(w_{ji}\\) as the weight from \\(x_i\\) to \\(y_j\\). Then for every output neuron (e.g., \\(y_j\\)), it accepts the information sent by every input neuron:\n$$ y_{j}= \\sum\\limits_{i} w_{ji} x_{i} \\tag{5.1} $$\nThen the partial derivative of \\(y_j\\) with respect to \\(x_i\\):\n$$ \\frac{\\partial y_j}{\\partial x_{i}}= w_{ji} \\tag{5.2} $$\nLet\u0026rsquo;s see another example. Comes from Bishop-Pattern-Recognition-and-Machine-Learning-2006 And we can represent it by the computational graph below.\nAnd we can perform a forward propagation according to the computational graph.\n$$ \\begin{align} h_{j} \u0026= \\sum\\limits_{i} w_{ji}^{(1)} x_{i} \\tag{5.3} \\\\ z_{j} \u0026= f(h_{j}) \\tag{5.4} \\\\ y_{k} \u0026= \\sum\\limits_{j}w_{kj}^{(2)} z_{j} \\tag{5.5} \\end{align} $$ where\n$$ f(h) = \\tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} \\tag{5.6} $$\nA useful feature of this activation is that its derivative can be expressed in a particularly simple form:\n$$ f\u0026rsquo;(h) = 1 - f(h)^2 \\tag{5.7} $$\nThe error function can be mean squared errors:\n$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum\\limits_{k}(y_{k}- \\hat{y}_k)^2 \\tag{5.8} $$\nIf we want to update the parameters, we need first to compute the partial derivative of \\(E(\\mathbf{w})\\) with respect to them.\n$$ \\frac{\\partial E(\\mathbf{w})}{\\partial w_{kj}^{(2)}} = \\frac{\\partial E(\\mathbf{w})}{\\partial y_{k}} \\frac{\\partial y_k}{\\partial w_{kj}^{(2)}} = (y_{k}- \\hat{y}_k)z_{j} \\tag{5.9} $$\n$$ \\begin{align} \\frac{\\partial E(\\mathbf{w})}{\\partial w_{ji}^{(1)}} \u0026amp;= \\frac{\\partial E(\\mathbf{w})}{\\partial h_{j}}\\frac{\\partial h_j}{\\partial w_{ji}^{(1)}} = \\left(\\frac{\\partial E(\\mathbf{w})}{\\partial z_{j}} \\frac{\\partial z_j}{\\partial h_j}\\right)x_{i} \\tag{5.10} \\end{align} $$\n$$ \\frac{\\partial E(\\mathbf{w})}{\\partial z_j} = \\sum\\limits_{k}\\frac{\\partial E(\\mathbf{w})}{\\partial y_{k}}\\frac{\\partial y_k}{\\partial z_{j}}= \\sum\\limits_{k} (y_{k}- \\hat{y}_{k}) w_{kj}^{(2)}\\tag{5.11} $$\n\\(\\text{Remark.}\\) \\(z_j\\) can send information to all the output neurons (e.g., \\(y_k\\)), thus we need to sum over all the derivatives with respect to \\(z_j\\).\nSubstituting \\(\\text{(4.11)}\\) into \\(\\text{(4.10)}\\) we obtain\n$$ \\frac{\\partial E(\\mathbf{w})}{\\partial w_{ji}^{(1)}} = (1 - z_j^2)x_{i} \\sum\\limits_{k} (y_{k}- \\hat{y}_{k}) w_{kj}^{(2)} \\tag{5.12} $$ 6. Interpretation Recall the Taylor approximation of the two variables function:\n$$ f(x, y) = f(x_0, y_0) + f_x (x- x_0) + f_y(y-y_0) \\tag{6.1} $$\n\\(\\text{Remark.}\\) \\((x, y)\\) needs to be close to \\((x_0, y_0)\\), otherwise the approximation can fail.\nWe can transform \\(\\text{(5.1)}\\) into \\(\\text{(5.3)}\\):\n$$ \\begin{align} f(x,y) - f(x_{0},y_0) \u0026= f_x (x- x_0) + f_y(y-y_0) \\tag{6.2}\\\\ \\implies \\Delta f \u0026= f_x \\Delta x + f_y \\Delta y\\tag{6.3} \\end{align} $$ If we apply \\(\\text{(5.3)}\\) in the example above, we can obtain\n$$ \\Delta E(\\mathbf{w}) = \\nabla_{\\mathbf{w}}E(\\mathbf{w}) \\Delta \\mathbf{w} \\tag{6.4} $$\nFrom another perspective, a small change in the parameters will propagate into a small change in object function by getting multiplied by the gradient.\nTo summarize, back-propagation allows information to flow backwards through the network. This information can tell the model a small change in one particular parameter can result in what change in the object function. And gradient descent can use this information to adjust the parameters for optimizing the object function.\n","permalink":"https://yunpengtai.top/posts/back-propagation/","summary":"1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.\n$$ \\mathbf{w}^{\\tau + 1} = \\mathbf{w}^{\\tau} - \\eta \\nabla_{\\mathbf{w}^{\\tau}} E \\tag{1.1} $$\nwhere \\(\\eta, \\tau, E\\) label learning rate (\\(\\eta \u0026gt; 0\\)), the iteration step and the loss function.","title":"Going Deeper into Back-propagation"},{"content":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog\u0026rsquo;s interesting part.\nNowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others\u0026rsquo; work (e.g. reproducing a BERT) because everything is there for you. However, when designing a NN or facing a new task, you are most probably trapped somewhere.\nAnd this blog is meant to guide you to handle new problems.\nLet\u0026rsquo;s first begin with some basic rules. Hope you guys enjoy it!\nRush into training neural networks leads to suffering. Training NN is not like writing the common code. For instance, if you plug in a int while it needs a string, errors just come out. However, writing the code about NN can not be so easy for it won\u0026rsquo;t show you bugs automatically (only if you make big mistakes). Sweating the details always pays off. Someone may say the details are infinite and can stop you from marching. Note that the details mentioned here are all necessary instead of some trivialities. And sweating the details can reduce your pain. Observation leads to intuition. Sadly, if you just keep thinking about something, inspiration will never come to you. For instance, if you want to upgrade the algorithm, you had better observe the data where the algorithm fails instead of just thinking about the algorithm. Trusting your intuition instead of your implementation. Sometimes when you come up with a new idea, the implementation of it may go wrong to some degree. When the result is opposite to your assumption, always check your code first before doubting your idea. Quicker, then better. When you are trying to test your hypothesis, use the most efficient way to verify it as fast as possible. Then let us go through concrete parts.\n1. Familiar with Data At this stage, you need to do some basic analysis and data mining. Assume we have a classification dataset in NLP. There are several aspects to think about.\nYou need to know the label distribution especially and visualize it.\nFor instance, if you observe long-tailed distribution (e.g. the number of the instances for good emotion is 900 while for the bad emotion is 10), then some methods such as data augmentation or cost-sensitive loss functions can play their part. For your interest, you can refer to this up-to-date survey.\nSimilarly, you may also need to know the distribution of the length of sequence. Thus you can set the appropriate maximum sequence length for your task. Moreover, you can also pass the original data through the feature extractor (such as BERT) to gain their representation. Then you can cluster them.\nGreed is good\nI strongly suggest that you look through the whole dataset ambitiously just like the greedy algorithm. And I promise you are bound to find surprise. You can have a whole understanding of the domain of this dataset. And you can choose appropriate pre-trained models according to the domain. Also, remember to understand the labels. Once you understand the labels, you can see if the annotation is contradictory. And you can select certain samples to see the annotation and estimate how noisy the whole dataset is.\nYou may also need to think for the machine. Are the sequences easy to understand? If they are easy to understand, then we do not need to apply very complicated models to tackle this problem. Is the local information more important than the global? Your understanding about the dataset can help you figure out some basic modeling problems and offer you intuition about rule-based methods.\nSimple quantification\nYou may need to know the size of the dataset. If the size is small, we can use the simple models such as textCNN or FastText instead of BERT-based models for the complicated models need more data to model the inductive bias. Also, you can write simple code for detecting the duplicate instances and instances which are corrupted (e.g. lack of the label).\nStand on the shoulder of the model\nWhen your model is trained on the dataset, you can see how it performs on the dev/eval set. You need to pay attention to those mis-predicted instances (i.e. bad cases) and think about why the prediction is wrong. Is the label wrong or the way of modeling weak to capture these information.\nAlso, do not forget to filter or clean the dataset based on your thorough observation.\n2. End-to-End Pipeline When you finish observing the dataset, you need to build the simple pipeline to ensure everything goes well.\nFix the random seed. When carrying out the experiments, you had better fix the seed to reduce the influences of randomness on the experiments.\nAs simple as possible. When building the pipeline, you do not have to use very complicated modeling methods, etc. We are just testing. Thus make everything as simple as possible. For instance, you can use the simple classifier such as SVM and MLP (Multi-Layer Perceptron).\nRecord the accuracy and loss. Training accuracy and loss are very useful for you to figure out which difference is beneficial. Also, we do not need complicated tools (e.g. Tensorboard and Wandb) to do so. You can use a list to store things you want and visualize it by matplotlib or write it down in a txt (Sometimes, the data on the terminal can disappear for certain reasons).\nTrack the progress. In python, you can simply use the tqdm to track the progress. And you can also add the immediate accuracy and loss on the progress bar. Believe me, this can reduce your anxiety.\nVerify the init loss. For the multi-label classification problem, its loss should equal \\(-\\log (1/ \\text{num classes})\\) (with a softmax). For instance, if you need to make the true prediction among 4 labels, the init loss should equal \\(1.386\\).\nGood Initialization. For regression problems, if the average of your data is 6, then you can initialize the bias as 6 which can lead to fast and stable convergence. One more example, if you want to initialize the weights and you do not want the weights to be influenced by the output shape, then you may prefer Lecun Normal to Glorot Normal (all initialize with \\(\\text{N(0, scale)}\\)). Also, normal initialization is better than uniform initialization by experience. Last but not least, when facing an imblanced dataset with ratio 1:10 (positive cases V.S. negative cases), set the bias on the logits so that the model can learn the bias with the first few iterations.\n# fan_in, fan_out represent the input and output shape scale = 1. # lecun normal scale_lecun /= max(1., fan_in) scale_lecun = sqrt(scale_lecun) # glorot normal scale_glorot /= max(1., fan_in + fan_out) scale_glorot = sqrt(scale_glorot) Human Baseline. If the dataset is very particular and there are few related evaluation methods, you had better set the human baseline in sampled instances. Compared to the human baseline, you can have an idea that where your model has gone.\nInput-Independent Baseline. You can set the input all zeros and see the performance. And it should be worse than the performance of plugging in your data.\nOverfit a small batch. The model should overfit a batch of few instances (e.g. 10 or 20). Theoretically speaking, you should achieve the least loss. If the model fails to do so, then you can go and find the foxy bug.\nVisualize the input before going into the NN. Take Google\u0026rsquo;s code as example, it shows the input tensors when performing classification problems by BERT. This habit has saved me many times when coming up with a brand-new task because the preparation of data can be hard to some degree.\nVisualize the predictions dynamically over the course of training. By doing so, we can have a direct picture about where the model has gone and how it performs.\nTry Back-Propogation yourself. Gradients can give you information about what the model depends on to make such predictions.\nGeneralize a special case. You should not write the general functions at the beginning because your thoughts can be easy to change, thus these general functions are fragile. You can generalize a special case when you are sure that it won\u0026rsquo;t change a lot.\n3. Overfitting Since we have built a pipeline and tested it, it\u0026rsquo;s time for us to make the model overfit the whole dataset.\nPicking the right model. The selection of models is related to the size of the dataset and complexity of the task. If your dataset is small, you can choose relatively big models to overfit. Borrow experience from the giants. Sometimes you are unfamiliar with the task, you may have no idea about which hyper-parameter to choose (e.g. learning rate). Then you can search some related papers and see the appendix for training details. Carry out many controlled experiments. Deep Learning is a experimental subject. Sometimes observation fails to give you idea about what exactly it will perform. For instance, if you want to know which learning rate is most suitable for this task, try more options to select the best. Remember change a variable once a time to reduce the influence of mixture. Turn to tricks. Tricks are infinite. For instance, you can apply adversarial training like FGM or PGD to improve the model\u0026rsquo;s performance. Also, if permitted, you can use random searching for the best parameters. 4. Regularize More data is better. The most effective way to regularize the model is collecting more real-world data for training. After all, we are using the small set of data to approximate the distribution of the real-world. Data Augmentation. If you lack data, you can apply data augmentation to increase your data. Although this method seems very easy, it does demand your thorough understanding of your data and task. And creative methods can always pay off. For instance, in NLP, you can use back-translation to augment. Cross Validation. You can split the data several times. And use separate data to train some models. Then we can ensemble them to gain the final prediction. 5. Others Always remember to record your results in a good order. For instance, you must record all the parameters and the model\u0026rsquo;s performances at the dev/eval set. You had better record the motivation for you trying out this experiment. Always back up your code and data. When you are trying some new methods, do not just try it on the original code. The same for the data. You need to back up the original pipeline and data for bad things happening. ","permalink":"https://yunpengtai.top/posts/tips-for-training-neural-networks/","summary":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog\u0026rsquo;s interesting part.\nNowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others\u0026rsquo; work (e.","title":"Tips for Training Neural Networks"},{"content":" Life is complex, and it has both real and imaginary parts. — Someone\nBasically, I\u0026rsquo;m not interested in doing research and I never have been\u0026hellip; I\u0026rsquo;m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell\nTo not know maths is a severe limitation to understanding the world. — Richard Feynman\nProblems worthy of attack prove their worth by fighting back. — Piet Hein\nAn expert is a person who has made all the mistakes that can be made in a very narrow field. — Niels Bohr\nThe beauty of mathematics is only shown to its patient followers. — Maryam Mirzakhani\nThe essence of Mathematics lies precisely in its freedom. — Georg Cantor\nWe must know, we will know. — David Hilbert\nIf your tendency is to make sense out of chaos, start chaos. — Carlos Castaneda\nPerhaps I could best describe my experience of doing mathematics in terms of entering a dark mansion. You go into the first room and it\u0026rsquo;s dark, completely dark. You stumble around, bumping into the furniture. Gradually, you learn where each piece of furniture. And finally, after six months or so, you find the light switch and turn it on. Suddenly, it\u0026rsquo;s all illuminated and you can see exactly where you were. Then you enter the next dark room\u0026hellip; — Andrew Wiles\nIf we knew what it is we were doing, it would not be called research. Would it? — Albert Einstein\nIf you can\u0026rsquo;t solve a problem, then there is an easier problem you can solve: find it. — George Pólya\nYoung man, in mathematics you don\u0026rsquo;t understand things. You just get used to them. — John Von Neumann\nMathematics is the art of giving the same name to different things. — Henri Poincare\nMathematics is the subject in which we never know what we\u0026rsquo;re talking about, nor whether what we are saying is true. — Bertrand Russell\nAll mathematics models are wrong. Some of them are useful. — George Box\nWhenever something goes wrong, remember 1+1 is still 2. — Someone\nThe study of mathematics is apt to commence in disappointment\u0026hellip; We are told that by its aid the stars are weighed and the billions of molecules in a drop of water are counted. Yet, like the ghost of Hamlet\u0026rsquo;s father, this great science eludes the efforts of our mental weapons to grasp it. — A. N. Whitehead\nWhen one does a theoretical calculation, there are two ways of doing it. Either one should have a clear physical model in mind or a rigorous mathematical basis. — Enrico Fermi\nLet the formula guide your thinking. — Someone\nA mathematician is a device for turning coffee into theorems. — Alfréd Rényi\nAs long as there\u0026rsquo;s a life, there\u0026rsquo;s hope. — Stephen Hawking\nEvery problem was once \u0026ldquo;unsolvable\u0026rdquo; until it was solved.\nLet us assume we know nothing, which is a reasonable approximation. — D. Kazhdan\nFailure is not an option. Failure is mandatory. The option is whether or not you let failure be the last thing you do. — Howard Tayler\nMathematics is not about numbers, equations, computations, or algorithms: it is about understanding. — William Paul Thurston\nWhat is mathematics? It is only a systematic effort of solving puzzles posed by nature. — Shakuntala Devi\nDoing mathematics should always mean finding patterns and crafting beautiful and meaningful explanations. — Paul Lockhart\nObvious is the most dangerous word in mathematics. — Eric Temple Bell\nMathematics is the poetry of logical ideas. — Albert Einstein\nOnly one thing can comfort your feelings like music, please your heart like paintings, stir your soul like poetry, enhance your wisdom like philosophy, and improve your life like technology — Mathematics. — Klein\n","permalink":"https://yunpengtai.top/posts/quotes-of-mathematicians/","summary":"Life is complex, and it has both real and imaginary parts. — Someone\nBasically, I\u0026rsquo;m not interested in doing research and I never have been\u0026hellip; I\u0026rsquo;m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell\nTo not know maths is a severe limitation to understanding the world. — Richard Feynman","title":"Quotes of Mathematicians"},{"content":"Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training \u0026amp; Evaluation set:\n\\(\\text{MassiveText}\\) for both training \u0026amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \\(128K\\) tokens During training, we retrieving \\(600B\\) tokens from the training The evaluation contains \\(1.75T\\) tokens Test set leakage:\nDue to the huge retrieving database, the test set may have appeared in the training set. Thus, the authors apply 13-gram Jaccard Similarity between the training and test documents to filter those training documents similar to the test documents (i.e., the similarity is \\(\\geq \\textbf{0.80}\\))\nRetrieval Modeling Key-Value Format of the Database:\n\\(\\text{Key} \\Rightarrow\\) frozen BERT Embedding \\(\\text{Value} \\Rightarrow\\) raw chunks of the tokens using the SCaNN library\nthe similarity depends on the \\(\\text{L2 Distance}\\):\n$$ ||x-y||_2 = \\sqrt{\\sum_i (x_i - y_i)^2} $$\npre-compute the frozen BERT Embedding to save the computation and the Embedding is averaged with time.\nretrieving targets are the corresponding chunks and their continuation in the orig document\nThe architecture Assume the input sequence \\(\\text{X}\\) contains \\(9\\) tokens, it can be split into \\(3\\) chunks (i.e., \\(C_1, C_2, C_3\\)) whose sizes are \\(3\\) respectively. Then the chunks are embedded through the frozen BERT embedding. We can retrieve neighbours of those input chunks. We also embed the input sequence and then apply self-attention mechanism on them to get the hidden states \\(H(X)\\) Furthermore, we need to encode the neighbours. Here, the transformer encoder is bi-directional. And it outputs the representations of the neighbours by conditioning on the hidden states of the input chunks. After we get the representations of the neighbours, we let them attend the input chunks as the \\(\\text{K and V}\\) while the input chunk is \\(\\text{Q}\\). The attending network is called CCA(\\(\\textbf{C}\\)hunked \\(\\textbf{C}\\)ross \\(\\textbf{A}\\)ttention). I introduce it in the following part. When the neighbours finish attending the input chunks, the input chunks can be represented by the retrieved neighbours. The representations are going through the FFW(\\(\\textbf{F}\\)eed \\(\\textbf{F}\\)or\\(\\textbf{W}\\)ard). Thus, a Retro-Block contains self-attention mechanism, CCA and FFW. Take the green chunk as the example, we retrieve its neighbours from the database and we let them attend with the concatenation between the green chunk and its next chunk. To put it more precisely, assume we retrieve the neighbours \\(E(mi)\\) for the chunk \\(m_i\\) which contains \\(n\\) tokens: \\({m{i1}, m*{i2}, \\dots, m*{in}}\\), we concatenate the last token of \\(mi\\) with the next chunk \\(m_j\\) except the last token \\(\\Rightarrow \\text{Concatenate}(m{in}, m_{j1, \\dots, jn-1})\\). After the concatenation, we apply CA(\\(\\textbf{C}\\)ross \\(\\textbf{A}\\)ttention). CA is the common attention mechanism. Finally, we concatenate the outputs and pad them. Note, the relative positional encoding is applied.\nExperiment Scaling the Retro The scale of the Retro and the retrieved tokens are proportional to the performance. The number of neighbours has an upped bound: somewhere near \\(40\\). Maybe too many neighbours reduce the retrieval quality. Improvement Comparison Among some tasks, Retro can outperform the models whose parameters are much more than the Retro\u0026rsquo;s.\nPerplexity on Wikitext103 Retro\u0026rsquo;s perplexity can be SOTA on the Wikitext103 Interestingly, the external memory can also have the phenomenon of the underfitting. When using MassiveText(1%), it can underfit the training set. And its performance is worse than the kNN-LM. Retro Finetuning Training from scratch is the most powerful way.\nQuestion Answering Results FID + Distill is the SOTA in the Open-Domain Question Answering when the retrieval involves in the training.\nAblation Studies The continuation of the retrieved chunks do help. CA positions are every 3 from 1 or mid layer. Why work? To summarize, the Retro incorporates the external neighbours of the input sequence into the Large Language Modelling to scale down the model size while maintaining the performance.\nLessons \u0026amp; Imaginations Performance can get improved either by improving the model size or training more data. Huge amount of data don\u0026rsquo;t need too big model to fit in. We can scale down the PLM by attending the external information. CCA is applied because the external knowledge need to be merged. When applying in MRC, the external information can be: the chunked passages the broken passages the past similar to question-passage pairs the knowledge among the input the evidence The BM25, Edit Distance and LDA can also perform not bad in the retieval. ","permalink":"https://yunpengtai.top/posts/retrieval-enhanced-transformer/","summary":"Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training \u0026amp; Evaluation set:\n\\(\\text{MassiveText}\\) for both training \u0026amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of \\(128K\\) tokens During training, we retrieving \\(600B\\) tokens from the training The evaluation contains \\(1.75T\\) tokens Test set leakage:\nDue to the huge retrieving database, the test set may have appeared in the training set.","title":"Retrieval-Enhanced Transformer"},{"content":"Hi! You can check my CV here.\nHere is Yunpeng Tai. I am a geek about mathematics, machine learning and NLP.\nI’m currently trying to get a PhD offer from ML or NLP. If you like my post, you can contact me. I major in Computer Science in Suzhou University of Science and Technology. Currently I am focusing on the In-Context Learning of LLMs. And I am familiar with the topics below:\nOpen-Domained QA Noise Learning Pre-training of LLM Sentence Rephrasing Machine Reading Comprehension For me, learning is a process of getting freedom. I am a perfectionist and hate the feeling of being limited, thus I will do something by myself for maximum freedom. That is why I have some interesting projects.\nI am easily curious about anything interesting, thus always fail to refuse the thoughts of exploring the unknown. I am super passionate about finding real answers for Artificial Intelligence. I am interested in understanding interesting things. And often to understand something you have to work it out yourself. That\u0026rsquo;s why I like research. Here are some \u0026ldquo;side-products\u0026rdquo; which have nothing to do with my research:\nI developed this website\u0026rsquo;s theme based on the original one. I visualized how it comes to the probability density function of Guassian Distribution (here is code). click to see the video I am going to use this blog to record my thoughts. Also, I have been trying really hard to make the knowledge easy to grasp. Possible visualization or examples can be used to explain some topic.\nThere’s lots of ways to be, as a person. And some people express their deep appreciation in different ways. But one of the ways that I believe people express their appreciation to the rest of humanity is to make something wonderful and put it out there. \u0026ndash; Steve Jobs\nPlease feel free to reach out to me. Any interesting topic is welcome.\nWeChat: Yunpengtai Email: yunpengtai.typ@gmail.com PGP Fingerprint: CDC0 3AD7 8045 48A4 43CF 0FA4 9ACD 7CA0 3F31 D24B By the way, if you like to contact me by PGP, the content below is my public key.\nPGP Public Key -----BEGIN PGP PUBLIC KEY BLOCK----- mQGNBGRGPZgBDADHN9SKoELU55fEcc5XKPi4cc9W9ksB99Wha9mH2c3m2nHuRrOj ezEZ0lDLL7/DyJfd796ho4+sE8eZkbO0+UvtVShMElQqVFP87XjdsvKjQgN2urEd pUG0h5Wso5h6FT1G8n3uOWG+5w0T7movAtZ4B7nJKAmpc3rBbMejAkVQU00RoQAO zr2DrbrboHzb+SQyspChIvG34goG4y6ovHpVg8qnsWdZgTWWX7i9nB3ryNVAjo+3 ZFMAAZXqo9anoIqma1OTQineXSU7E/yY5K143UWLAgtEOeC8ZH8R5LJctPTdzfGJ d99ZvSwrSc9qXxeEW0DM9dt/q6IA8pVwz1HEEaj9zQtknZtaQapAzizRadwTvcUS VqrrbG6meE0OrZxAmezKYRdCd+Vcgd6jHUyOtuJEHXJ7fmLnXiylzP0xAMHmtIUG 5+o/ht2R5E68irv60NcdJ16FW50GNOMdsEVo1Oy5r/oRsMziD9lc+NCD3YVDrpGk p++3p7Ip6VGT8wMAEQEAAbRQeXVucGVuZ3RhaSAoUGxlYXNlIHNlbmQgbWUgZW1h aWxzIGJ5IHBncCBwdWJsaWMga2V5LikgPHl1bnBlbmd0YWkudHlwQGdtYWlsLmNv bT6JAdEEEwEIADsWIQTNwDrXgEVIpEPPD6SazXygPzHSSwUCZEY9mAIbAwULCQgH AgIiAgYVCgkICwIEFgIDAQIeBwIXgAAKCRCazXygPzHSS8esC/0WkZs+4PzT1zR7 O7RFI2ebuBwY0wRI30KMXvmhbfw+2rnLPnyMk/tvyq6n+xc/njsMDOQd3fR/c/4W pBZ5v4j9F6OMao5pJg3P3Wbm1hM2hkHT66EPHL24kXZE+yzL5FTLYWJCGFkBEzvI VebeKNFalNd78Sg/LeZvF8uiaR+s0+BU1E3nGu8m8vCnMpOrWKmZVMyn8wWsSHUB vINrltqVHuduyvGauwlU+kngvvNTX8LAOGLhR/Z+5Qi5kDwKue7As9gInDao/gI6 WhsTnl32LKF8wB3EWc1si2u1P6KdnqGjwzA+6wz0/WmPLMnMJ0wklKfHdRRe0XoB qucWjFedXbiPLl8AYRdNHOUre5BDpZryC41Ca/VKaAMMaXy1SEoPmM/qClJrDD2U onGkbOuzi82C70K9uJi4PrCdtt2xi5AfjoeQ/MfqRqx1ohSOqF9CCl8e4pBemEux 2wJYvr/vaSucQbFy0CYKBFgFexpQAZtdH4dwLQ3vmmDIt84axGS5AY0EZEY9mAEM AMOxBjf7xe3NfM5667qp+Cz8hXYHOYKMerOl63WzWopHMUoggsCc1hYC9Vnps0kc ACDofNNEfIU/cSQtNd7tpLckHUAGMXERlmoQnrBe+xLg65kVVBwhZht1ypusc2GT iqSMcqQqavVwnVZ3INgHsKqRDkHG/jkgGuCcV1b4tarpUHRS6xsTYpw1BcH/Jcwj BjEN2UsjNSyYqraP9NRwVLHer7rplNPFG+c+AsUFdSDJIlh5Y9rXpUgwh6dS0GRa hgPJu3Tn1t9KjSmrPd/HY1gi1f0ITrNnJnP6FCI4Pdfcg8ZyAOA/bk6QXFo0q/8r uKm7AFcFnNeD6leIZ8G4/LlovWjO6hfSg2d1NByCUu5hDLq5jLi3AyWckO5jOqn3 dbx7M4/T8bg50sTRkjCoGgMoRBVDqlcFjBtF8GAp3IIAQ2/ZUHYMLaMIzpP1bSLV HADPTSzFYIheyVNVjXk4DL0pVTslAVRowTOWabluecWLo/WXBa5CjDrd9eozz6Yo OQARAQABiQG2BBgBCAAgFiEEzcA614BFSKRDzw+kms18oD8x0ksFAmRGPZgCGwwA CgkQms18oD8x0kt5zAwAsPiHRDtz99w3K3wY8ULbWfBOwTekJ4/RJBGRn6dWJB/e FCxBNu2ug4MJSd4QKecmJ4y5fJaoyToPCzWZ2pZDncnfEd3ji4vYJO8hLdePccys KIK9FHImyv/xe7Vb79Xqpi/NmkxAOkCCwgXt87YiaHlDM3Bo615sOAaMO7dYqyjB RAgEcjhBGxFS8uc0q2Vc0AJfpD7/N0zAYg47DYht2QwzmUUatSgolFOt5M3tLdAh HYFs97+0cmWg/8Xva3ug6GeutXH4Pmr8vAX46wuZeylfdFaUtPpwFBlk3q8pGkrU eV6iIZamBlIPY+eX3FeqMPg0xVCzUGHAAcsZiTQ5OvQ3326AiqLrehqxCP7BRBCv 1ASN0CTINCKn8fj37oKlUrvW9eSd+sO2VR2o6oqmWIOh4ItZPKkqf1lVap+xaBQ0 D2oY+2BYnPMP96IWmjjDlkNW5FaE4FuE2uf3UZebcmRS1w+c04p6PpyIFPJfAMr6 Zhpe5QegrHd27n3hksLP =QsdK -----END PGP PUBLIC KEY BLOCK----- ","permalink":"https://yunpengtai.top/about/","summary":"Hi! You can check my CV here.\nHere is Yunpeng Tai. I am a geek about mathematics, machine learning and NLP.\nI’m currently trying to get a PhD offer from ML or NLP. If you like my post, you can contact me. I major in Computer Science in Suzhou University of Science and Technology. Currently I am focusing on the In-Context Learning of LLMs. And I am familiar with the topics below:","title":"🙋🏻‍♂️About"},{"content":" 科学空间 致力于分享科学之美 林一二的Wiki 林一二的模因和想法 学习者的数字花园 pimgeek的笔记与思考 山茶花舍 吕楪在记录自己的生活 小鹿生活志 一个写故事的博客！ CC康纳百川 CC康纳百川的小窝 一面之猿网 领导难以启齿的过去，同事不堪回首的曾经，产品的噩梦，测试心里永远无法抹去的痛 LICSTAR的博客 NLP和其他有趣的故事 EXEC 嗷呜～～～ 山葵酱 变幻莫测的山葵酱 ","permalink":"https://yunpengtai.top/friends/","summary":"科学空间 致力于分享科学之美 林一二的Wiki 林一二的模因和想法 学习者的数字花园 pimgeek的笔记与思考 山茶花舍 吕楪在记录自己的生活 小鹿生活志","title":"🤝Friends"},{"content":"通常写一篇文章从构思，代码实现，文章排版，方向调研都会花费不少时间，同时网站平时运营开销也有一些，如果你认为有文章对你有帮助，可以打赏我，鼓励我继续产出文章\n","permalink":"https://yunpengtai.top/sponsor/","summary":"通常写一篇文章从构思，代码实现，文章排版，方向调研都会花费不少时间，同时网站平时运营开销也有一些，如果你认为有文章对你有帮助，可以打赏我，鼓","title":"🥂Sponsor"}]