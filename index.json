[{"content":"Cursor 刚火起来的时候自己还是比较保守的，或者说不想费事去搞，毕竟在从业者眼中 Agentic Coding 还多半是一个「半成品」的状态，因为一个半成品而消耗自己的精力，这显然是不值当的。后来，接触了 Cursor 之后，的确是了解了一些高效的用法，比如快速地写函数注释以及参数的 type，抑或是快速了解新 repo 的某些代码段作用。现在，使用了 Claude Code，可以确信地说，Claude Code + IDE (Vscode / Cursor) 才是真正高效的编程，编程不仅可以更快，同时可以更好，而在编程的过程自己也能学的更多\n以下将分为几个大的部分来详细介绍如何更好地使用 Claude Code (CC) 部分内容参考自 Anthropic Best Practices\n安装 首先是在 IDE 终端内完成对 CC 的安装，这里泛指 Vscode 类 IDE，用 IDE 的原因很简单，CC Desktop 的更改是终端式的，不便于查看；而 CC 在 IDE 中可以新建一个窗口，非常方便查看差别（除非该终端是计算节点）（注：如果是 Remote 开发，在 Remote 开发机中装即可）\n# 没有 npm 可以去官网进行安装: https://nodejs.org/en/download npm install -g @anthropic-ai/claude-code 接着安装插件：Claude Code，如果是 Remote 开发，记得装到 Remote 机器上\n运行时 CC 会对「区域」进行检测，有些区域会被 block，可以用这个跳过：\necho \u0026#39;{ \u0026#34;hasCompletedOnboarding\u0026#34;: true }\u0026#39; \u0026gt; ~/.claude.json 接着设置 API Base 和 Key 即可进行玩耍：\nexport ANTHROPIC_BASE_URL=xxx export ANTHROPIC_API_KEY=xxx 最后开启终端输入 claude 即可\n如果你说 Claude 又贵又容易被封，代理站不稳定，那么为什么不试试便宜又同样好用的 kimi/k2 呢？去官网的 API 开通之后，即可使用：\nexport ANTHROPIC_BASE_URL=https://api.moonshot.ai/anthropic export ANTHROPIC_API_KEY=月之暗面的 key 基本元素 CLAUDE.md 如果你需要让 CC 可以在修改 repo 的时候如鱼得水，那么你需要给其一份有关于 repo 的 manual，当然它可以每次都自己探索理解，但这样效率低而且不如你告诉它来的精准。这份 manual 在 CC 中即为「CLAUDE.md」，里面包含 repo 的信息以及「你需要它额外记住」的信息\n那么如何得到 CLAUDE.md 呢？当然你可以纯手写，那样效率不高。我们可以让 CC 自己生成，只需要在终端中输入 /init 命令即可，这是第一步。接着你还需要：\n修改不正确的：毕竟让它直接理解肯定会出现问题，比如 Development Commands（就是运行的命令） 开发示例：有些模块之间会有比较复杂依赖，可以给它一些修改的示例，即 Development Notes，比如你可以教它添加新优化器实现的步骤 工具使用：如果你自己手搓了一些「工具」，也可以单独放在 Tools 章节 除了以上这些，还可以使其原先的表述更加精准，或是加上一些它未曾注意到的，精准的开发需要精确的信息作为辅助 除了 repo 相关的信息可以放在 CLAUDE.md，我们还可以将写代码的注意点放进去，比如 Coding Style，还有 Getting Help 部分。以下是个人示例：\n## Best Practices ### Code Style - Make the code neat and pythonic - Follow existing patterns in the codebase. NEVER write new features into a file without first having a complete understanding of its underlying logic - Make the smallest reasonable changes to get to the desired outcome - DO NOT simply override the current code to implement new feature. Use configuration unless I told you to remove some feature - You MUST ask permission before reimplementing features or systems from scratch instead of updating the existing implementation - Use type hints, but omit the return type for functions that return `None` - Single quotes are preferred over double quotes - Use specific function or variable name ### Getting help - ALWAYS ask for clarification rather than making assumptions. - If you\u0026#39;re having trouble with something, it\u0026#39;s ok to stop and ask for help. Especially if it\u0026#39;s something your human might be better at. 总而言之，CLAUDE.md 应该包括以下部分：\n运行代码的命令（训练，单元测试等） 代码的整体结构（训练，模型，数据，优化器等）以及不同 python 文件所负责的 module 代码的参数配置：参数的含义是什么，这个可以按照 module 来划分 如何修改不同的 module (Development Notes) Coding Style 以及一些额外的信息 另外还可以让 CC 假装不熟悉 repo，让其对 CLAUDE.md 进行补充以便让新手上手并修改 repo\nCommands 一些重复的 prompt 可以收集起来，变成特定的命令，方便直接调用，在项目根目录创建 .claude/commands，将命令以 *.md 的形式添加进该目录即可在 CC 中调用。比如创建 fix-issue.md：\nPlease analyze and fix the GitHub issue: `$ARGUMENTS.$` Follow these steps: 1. Use `gh issue view` to get the issue details 2. Understand the problem described in the issue 3. Search the codebase for relevant files 4. Implement the necessary changes to fix the issue 5. Write and run tests to verify the fix 6. Ensure code passes linting and type checking 7. Create a descriptive commit message 8. Push and create a PR Remember to use the GitHub CLI (`gh`) for all GitHub-related tasks. 使用 / 来唤出命令，即 /fix-issue: ... 即可使用该命令\n工作流 比较直接的就是让 CC 直接给你实现某个 feature，但这样通常不高效，因为这其实涉及了「阅读 -\u0026gt; 思考 -\u0026gt; 写代码」几个流程，一下子完成就容易出错。复杂的任务应该拆分为步骤来完成（除非实现的特别简单），而不同的工作流可以用 command 来表示，这样调用的时候比较方便\nRead-Plan-Code-Submit 四个英文单词分别代表一个「动作」，即将写代码拆分为四个过程：\nRead：你可以指定其阅读特定的内容，比如 model.py，或者 URL，图像等等，让其充分理解相关的内容，并强调该过程只需要理解不需要写代码 Plan: 阅读完之后，让其对需要实现的东西列一个简要的计划，以便你进行检查 Code: 这一步就是让它正式写代码 Submit：让其写完代码后进行 github submit，并且把你的需求和它的实现清单都放上去，方便日后 review 当然如果是方便测试的代码，可以在 Code 之后加上 Test，让其保证本地运行无报错或是通过单元测试才可进行 Submit\nTest-Code-Iterate-Submit 与上方不同，第一个动作变成了「Test」，即单元测试的意思，之所以放在第一位，是因为如果先写代码再写 test，很有可能 test 就比较专注于当前的实现，会忽略一些特殊的 cases（就好像自己检查自己的代码，比较难看出问题）\n其实这个工作流的核心就是「test-iterate」，让模型提前写好单元测试，并且不断其迭代，直到通过测试。这种工作流比较适合于需求非常精确，可用测试具体化的任务，比如后端 API 调用，或者是你让其重构当前代码，将「老代码的运行结果作为对照」\n如果有时候比较模糊，比如 UI 开发，也可以将 test 换成「screenshot」，让其对照截图来不断完善实现\n人为干预 人为干预是和任意工作流一起配合使用，毕竟 CC 是一步步实现需求，如果有时候实现错误，人为进行干预会得到更精确的结果，全自动说实话人也没办法放心。比较常用的是以下几种：\n按 Esc 直接打断 连续按两次 Esc，可以在历史中回跳选择，比如当前实现错误，而之前都是对的情况就可以用 复杂任务的 prompt 对于复杂 \u0026amp; 精确的任务，我们需要好的 prompt 让其完成工作，当然简单的任务有时候也需要更加精确的 prompt，一般情况下怎么写好的 prompt 可以参见我之前的博客 How to Prompt LLMs Better? ，下面针对 CC 开发来说一下好的 prompt 还需要满足什么（老生常谈的 specific 就不聊了）\n强调顺序性 当任务复杂时，尤其是不同步骤看起来有点像的时候，很容易直接进行合并，而合并会带来更大的复杂度，多半结果差强人意。那么，此时就需要一些 prompt 来让其遵循顺序，不要跳步：\n**Objective: Example Command** **Directive:** Your task is to process the request defined by ``$ARGUMENTS`. You MUST follow the five-phase process outlined below in strict sequential order. **Combining phases or proceeding to the next phase before explicitly completing the current one is a critical failure.** For each phase, you will announce its start, perform the required tasks, produce the specified deliverable, and then announce its completion before moving on.$` --- ## Phase 1: This is Phase 1 **Objective:** This is the objective of Phase 1 **Tasks:** 1. xxx 2. xxx **Transition:** After xxx, state: \u0026#34;Phase 1 Complete. Proceeding to Phase 2.\u0026#34; --- ## Phase 2: This is Phase 2 **Objective:** This is the objective of Phase 2 **Tasks:** 1. xxx 2. xxx **Transition:** After xxx, state: \u0026#34;**Phase 2 Complete. Proceeding to Phase 3.**\u0026#34; 上方的 prompt 模板基本上在复杂任务上可以让模型一步一步来进行编程，而不会一下子突然合并，然后暴毙\n将指令编程化 如果你将 Agent 当成一种代码来写，大概率都能得到满意的结果，比如我在做 k2 demo 的时候，最后的 HTML Report 需要加上所有的图片，而有时候随着图片生成的数量一多起来，加上整个过程本来就是非常复杂的连续推理，模型很容易就忘掉。一开始尝试的就是「上强度」，比如加入 **Critical**: You must attach all the figures into HTML report 这种字眼，但其实模型还是做不到，或者骗自己全部加上\n此时正确的做法应该是，将该指令尽可能向编程靠齐，如果你要写代码判断是否全部添加，比较朴素的思维就是，先 list 出图片原来一共有多少，再 grep HTML 中 src 为图片的数量，两者进行对比，如果对不上，则说明有图片被遗漏。那么就可以将上述过程写入 prompt，你还可以将对应的 bash 命令告诉它，这样之后，它多半情况都能正确添加，哪怕图片数量多\n复杂的 prompt 是否合理 你会不会觉得 CLAUDE.md 已经够长了，加上模型还需要游走仓库各个文件，此时如果 prompt 写的复杂，会不会反倒起副作用？ 结论是不必担心，确保 prompt 和完成的任务复杂度成正比即可，你的任务如果无比简单，那没必要用复杂的 prompt；而如果你的任务是多步精确的推理，那么你的 prompt 复杂点反倒会让其执行地更加精确\n让 Gemini 出手 有些时候可以让 Gemini 来修改你的 prompt，这里之所以是 Gemini，是因为其是所有 LLMs 最话痨的一个，会事无巨细地修改你的 prompt，让其更加精确具体，比如：\nMy objective is to xxx. Please re-frame the following prompt to make it easier for LLMs agents to follow. \u0026lt;Your Prompt\u0026gt; 当然，别忘了对其返回的结果进行二次修改，多半情况下 Gemini 还会顺手把不需要改的地方也改了，也可以在一开始输入时限制它不要这么做。总之，检查还是必要的~\n使用场景 Repo QA 这个其实是最靠谱的功能（写代码会出错，这个大概率不会），当上手一个新仓库时，或者阅读一段比较费解的代码，有时候可以让 CC 进行解释，以下列出几种提问方式：\nHow is the logging module implemented? Why does the line 139 in xxx.py call function a()? What is the meaning of parameter p on line 139 of xxx.py? 这个还可以反着用，当你在一个项目上加越来越多 feature，潜在的依赖关系就会越来越复杂，有时候你可能忙着实现一个 feature 而忘记一些依赖关系，也可以先让 CC 进行分析，此时就可以提前规避错误\nLinter 这个是我经常使用的功能，写完代码后，可以让 CC 对代码进行检查，查看其是否符合 pythonic and neat 的要求，常见的修改需求如下：\n将某个大函数拆分为几个原子化的小函数 修改某个变量 / 函数的名字使其更加具体和精确 对复杂的模块及其参数添加注释 对函数的变量加上 type 标识 Git Related 除了 Repo QA，CC 还可以 Git QA，比如你好奇某个模块的功能是在哪个 commit 实现的，或者是想要问责是哪个实现导致了目前的报错\n同时还可以让 CC 帮助你完成 git 的提交，比如写 commit message 以及自动化 push\nTips /clear 可以直接清除所有上下文 Shift+Tab 可以灵活切换状态，比如「自动接受」 修改命令之后得重启 CC 才可以生效 可以向 CC 传数据，比如图片，或者是读取 URL 文件，本地文件可以通过路径来读，还可以使用管道命令，比如 cat foo.txt | claude 多个 Claude 协同，这个适合一个 Claude 解决不了的时候，可以开两个终端，每个 CC 负责不同的功能，比如一个实现，一个检查 使用一些额外的工具，比如 MCP（Model Context Protocol），当然你也可以写一些「工具代码」供其调用 重要的提示词：比如 IMPORTANT，YOU MUST 来强调服从性；思考相关的：think, think hard, think harder（适合 Claude 模型） 有时候我们想一直「自动运行」，但它有时候会弹出一些「命令申请」，此时就可以在 .claude/settings.local.json 内加入一些命令的权限管理，比如 Bash(cat:*) 就是终端执行以 cat 开头的命令 { \u0026#34;env\u0026#34;: { \u0026#34;BASH_DEFAULT_TIMEOUT_MS\u0026#34;: \u0026#34;3000000\u0026#34; // bash command 超时时间 }, \u0026#34;permissions\u0026#34;: { \u0026#34;allow\u0026#34;: [ \u0026#34;Bash(git push:*)\u0026#34;, \u0026#34;Bash(nvidia-smi:*)\u0026#34;, \u0026#34;Bash(cut:*)\u0026#34;, \u0026#34;Bash(cat:*)\u0026#34;, \u0026#34;Bash(ls:*)\u0026#34;, \u0026#34;Bash(awk:*)\u0026#34;, \u0026#34;Bash(find:*)\u0026#34;, \u0026#34;Bash(tail:*)\u0026#34;, \u0026#34;Bash(head:*)\u0026#34;, \u0026#34;Bash(grep:*)\u0026#34; ], \u0026#34;deny\u0026#34;: [] } } ","permalink":"http://yunpengtai.top/posts/agentic-coding/","summary":"Cursor 刚火起来的时候自己还是比较保守的，或者说不想费事去搞，毕竟在从业者眼中 Agentic Coding 还多半是一个「半成品」的状态，因为一个半成品而消耗自己的精力，这","title":"Agentic Coding: 当编程被按下加速键"},{"content":"本文将主要涵盖以下内容：\n从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导的过程跟真正实现上有差异，比如实际是对动量进行正交化，而不是对梯度，但读者无须担心，本文最后还是会回归到具体的实现 介绍 Kimi 团队 https://github.com/MoonshotAI/Moonlight 在 Muon 基础上的改进和代码实现，主要是 weight decay 以及对齐更新量的 RMSNorm 两个方面 逐一实现上面提及的 Muon，然后与原始的 Adam 做对照实验进行验证 最后对 Muon 进行 FLOPS 分析 推导 Muon 度量线性层 给定输入 $\\boldsymbol{x} \\in\\mathbb{R}^{n}$，权重矩阵 $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$，过一层「线性层」（Linear Layers），即 $\\boldsymbol{y} = \\mathbf{W}\\boldsymbol{x}$（这里对 bias 进行忽略）。那么有个有趣的问题，$\\mathbf{W}$ 究竟对输入做了什么？或者如何度量这种线性运算呢？\n此时可以联系一下「算子范数」（Operator Norm）的定义：给定任意两种 norm 方式 $\\| \\cdot\\|_{\\text{F}}$ 和 $\\|\\cdot\\|_{\\text{E}}$，对于任意的 $\\boldsymbol{x}$，算子范数是 $\\mathbf{W}$ 能对 $\\boldsymbol{x}$ 进行的最大拉伸量：\n$$ \\|\\mathbf{W}\\|_{\\text{op}} := \\max_{\\boldsymbol{x}\\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{E}}} $$\n接着让我们看看两种 norm 方式 $\\|\\cdot\\|_{\\text{F}}, \\|\\cdot\\|_{\\text{E}}$ 均为 RMSNorm 时会发生什么，先回顾下 RMSNorm 的定义：\n$$ \\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\sqrt{ \\frac{1}{n} \\sum_{i} \\boldsymbol{x}_{i}^{2}} = \\sqrt{ \\frac{1}{n} } \\|\\boldsymbol{x}\\|_{2} $$\n那么：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} := \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}}}{ \\|\\boldsymbol{x}\\|_{\\text{RMS}}} = \\sqrt{ \\frac{n}{m} }\\underbrace{ {\\color{#08F} \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}}} }_{\\text{L2 operator norm} } \\tag{\\#1} $$\n可以发现 RMSNorm 算子范数是一种归一化后的 L2 算子范数，那么 L2 算子范数究竟是什么呢？我们接着推导：\n首先对 $\\mathbf{W}$ 进行「奇异值分解」（SVD），即 $\\mathbf{W}=\\mathbf{U\\Sigma V^{\\top}}$，其中 $\\mathbf{U}, \\mathbf{V}$ 都是正交矩阵，而 $\\mathbf{\\Sigma}$ 是对角矩阵，对角线的元素为奇异值，不妨设 $\\sigma_{1}\\geq\\sigma_{2}\\geq \\dots \\geq\\sigma_{r}, \\, r=\\min(m,n)$\n先说明一个重要的性质，即「正交变换之后不改变 L2 范数的大小」，证明如下：因为 $\\mathbf{V}$ 是正交矩阵，所以 $\\mathbf{V}^{\\top}\\mathbf{V} = \\mathbf{I}_{n}$\n$$ \\|\\mathbf{V}\\boldsymbol{x}\\|_{2}^{2} = (\\mathbf{V}\\boldsymbol{x})^{\\top}\\mathbf{V}\\boldsymbol{x} = \\boldsymbol{x}^{\\top}\\underbrace{ \\mathbf{V}^{\\top}\\mathbf{V} }_{ \\mathbf{I}_{n} }\\boldsymbol{x} = \\boldsymbol{x}^{\\top}\\boldsymbol{x} = \\|\\boldsymbol{x}\\|_{2}^{2} $$\n接着开始正式推导 L2 算子范数，不妨记 $\\mathbf{V}^{\\top}\\boldsymbol{x} = \\boldsymbol{y}$，蓝色部分的变换都用到了刚刚提及的「正交变换之后不改变 L2 范数的大小」的性质\n$$ \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{U\\Sigma V^{\\top}}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{U\\Sigma}\\boldsymbol{y}\\|_{2}}{{\\color{#08F}\\|\\boldsymbol{y}\\|_{2}}} = \\frac{{\\color{#08F}\\|\\Sigma \\boldsymbol{y}\\|_{2}}}{\\|\\boldsymbol{y}\\|_{2}} $$\n那么：\n$$ \\max_{\\boldsymbol{x}\\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} \\implies \\max \\frac{\\|\\Sigma \\boldsymbol{y}\\|_{2}^{2}}{\\|\\boldsymbol{y}\\|_{2}^{2}}= \\frac{\\sum_{i}\\sigma_{i}^{2}y_{i}^{2}}{\\sum_{i}y_{i}^{2}}\\leq \\frac{\\sigma_{1}^{2}\\sum_{i}y_{i}^{2}}{\\sum_{i}y_{i}^{2}} = \\sigma_{1}^{2} $$\n即：\n$$ \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\sigma_{1} = \\sigma_{\\text{max}} = \\underbrace{ \\|\\mathbf{W}\\|_{2} }_{ \\text{Spectral Norm} } $$\n「谱范数」（Spectral Norm）指的是矩阵的最大奇异值，那么联系式 $\\#1$ 可得：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} = \\sqrt{ \\frac{n}{m} }{ \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}}} = \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2} \\tag{\\#2} $$\nRMSNorm 的算子范数是一种归一化的谱范数\n输出变化量 在训练神经网络时，我们想知道，当权重矩阵更新后，输出会多大程度随之变化（当然这里 $\\boldsymbol{x}$ 也会发生变化，会在后文论述），即：\n$$ \\Delta \\boldsymbol{y} = (\\mathbf{W}+\\Delta \\mathbf{W})\\boldsymbol{x} - \\mathbf{W}\\boldsymbol{x} = \\Delta \\mathbf{W}\\boldsymbol{x} $$\n联系式 $\\#1$ 的定义，可知：\n$$ \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} = \\|\\Delta \\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} \\leq \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\cdot\\|\\boldsymbol{x}\\|_{\\text{RMS}} \\tag{\\#3} $$\n换言之，当权重矩阵更新后，我们找到了输出变化量 RMSNorm 的最大值，这里利用算子范数，巧妙地将矩阵的更新和输出的更新联系了起来\n对偶化梯度 通过「泰勒展开式」（Taylor Expansion），可知：\n$$ \\mathcal{L}(\\mathbf{W} + \\Delta \\mathbf{W}) = \\mathcal{L}(\\mathbf{W})+ \\langle \\nabla_{\\mathbf{W}} \\mathcal{L}, \\Delta \\mathbf{W} \\rangle + \\text{High Order Terms} $$\n由于变化量比较小，不妨对高次项进行省略，即「线性近似」：\n$$ \\Delta \\mathcal{L} = \\mathcal{L}(\\mathbf{W}+\\Delta \\mathbf{W}) - \\mathcal{L}(\\mathbf{W}) \\approx \\langle\\nabla_{\\mathbf{W}}\\mathcal{L}, \\Delta \\mathbf{W}\\rangle $$\n我们肯定想要 loss 变化量越小越好，同时变化的过程中，我们并不希望输出变化量太大，否则就会破坏前面泰勒展开的前提；同时如果输出变化量太大，会让训练过程不稳定。那么我们就对输出变化量加个 bound，即：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} =\\mathcal{O}(1) $$\n笔者以为「对输出变化量进行约束」即为 Muon 的核心 motivation，将式 $\\#3$ 代入，则：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\, \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\cdot\\|\\boldsymbol{x}\\|_{\\text{RMS}} =\\mathcal{O}(1) $$\n假设 $\\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\mathcal{O}(1)$，则：\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\, \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} =\\mathcal{O}(1) \\tag{\\#4} $$\n式 $\\#4$ 即被称为「对偶化梯度」（Dualizing the Gradients），那么这个如何解？\n可以先展开一下约束，使其与谱范数联系起来：\n$$ \\begin{align} \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} = \\sqrt{ \\frac{n}{m} } \\|\\Delta\\mathbf{W}\\|_{2} =\\mathcal{O}(1) \\\\ \\implies \\|\\Delta\\mathbf{W}\\|_{2} =\\mathcal{O}\\left(\\sqrt{\\frac{m}{n}}\\right) \\end{align} $$\n求解约束 接着我们对梯度进行奇异值分解： $\\nabla_{\\mathbf{W}}\\mathcal{L}=\\mathbf{U\\Sigma V^{\\top}}$，这里是「经济型分解」，即 $\\mathbf{U}\\in\\mathbb{R}^{m\\times r}, \\mathbf{V}\\in\\mathbb{R}^{n\\times r}, \\mathbf{\\Sigma} \\in\\mathbb{R}^{r \\times r}$，其中 $r=\\text{rank}(\\nabla_{\\mathbf{W}}\\mathcal{L})\\leq \\min(m, n)$。假设没有约束的情况下，我们想要去 $\\min \\langle \\nabla_{\\mathbf{W}}\\mathcal{L}, \\Delta \\mathbf{W}\\rangle$，我们会直接按照梯度的反方向，即\n$$ \\Delta \\mathbf{W} = - \\nabla_{\\mathbf{W}}\\mathcal{L} $$\n但目前有个约束条件，我们可以对 $\\Delta \\mathbf{W}$ 进行「正交化」（Orthogonalization）， $c \\in\\mathbb{R}$ 是一个系数：\n$$ \\Delta \\mathbf{W} = -c \\cdot\\mathbf{UV}^{\\top} $$\n之所以写成上述形式，主要是因为约束是有关谱范数的，而 $\\|\\mathbf{UV^{\\top}}\\|_{2}=1$，下面来证明这一性质。首先，对于一个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 来说，要求其谱范数，按照如下流程：\n$$ \\|\\mathbf{A}\\|_{2} = \\sqrt{ \\lambda_{\\max}(\\mathbf{A^{\\ast}}\\mathbf{A}) } = \\sigma_{\\max}(\\mathbf{A}) $$\n其中 $\\mathbf{A}^{*}$ 是矩阵 $\\mathbf{A}$ 的「共轭转置」（Conjugate Transpose），对于「实数域」的矩阵来说，共轭等于其自身，故而 $\\mathbf{A}^{*}= \\mathbf{A}^{\\top}$，所以就可以求 $\\|\\mathbf{UV}^{\\top}\\|_{2}$\n$$ (\\mathbf{UV}^{\\top})^{\\top}\\mathbf{UV^{\\top}} = \\mathbf{VU}^{\\top}\\mathbf{UV^{\\top}} = \\mathbf{VV^{\\top}} = \\mathbf{I}_{r}\\implies \\|\\mathbf{UV}^{\\top}\\|_{2} = \\sqrt{ \\lambda_{\\max} } = 1 $$\n接着来计算一下权重更新量的谱范数：\n$$ \\|\\Delta\\mathbf{W}\\|_{2} = \\|-c\\cdot \\mathbf{UV^{\\top}}\\|_{2} = |c| \\cdot \\|\\mathbf{UV^{\\top}}\\|_{2} = |c| $$\n那么：\n$$ \\|\\Delta \\mathbf{W\\|_{2}}=\\mathcal{O} \\left(\\sqrt{ \\frac{m}{n} } \\right)\\implies c =\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n可得：\n$$ \\Delta \\mathbf{W} = - \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} } \\right)\\mathbf{UV^{\\top}} \\tag{\\#5} $$\n即式 $\\#5$ 就是对偶化梯度的解，小结一下，我们将对输出变化量的约束转为对权重变化量的谱范数要求，最后通过「正交化」来求解\n求正交化 那么如何求正交化呢？正交化即让「奇异值全变为 $1$」，而奇异值是正数，就相当于对其使用了 sign 函数\n$$ \\nabla_{\\mathbf{W}}\\mathcal{L} = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{UV^{\\top}} $$\n但是要分解完 SVD，再对 $\\Sigma$ 使用 sign，这个代价有点大，有没有直接在 $\\nabla_{\\mathbf{W}}\\mathcal{L}$ 上作用的办法呢？有一条推论可以借助：「奇次矩阵多项式（Odd Matrix Polynomials）和 SVD 存在可交换性」，即我们对 $\\nabla_{\\mathbf{W}}\\mathcal{L}$ 的操作等于对 $\\Sigma$ 进行操作：\n$$ p(\\nabla_{\\mathbf{W}}\\mathcal{L}) = p(\\mathbf{U\\Sigma V^{\\top}}) = \\mathbf{U}p(\\Sigma)\\mathbf{V^{\\top}} $$\n但问题是 sign 并非是矩阵多项式的形式，奇次矩阵多项式一般长这样：\n$$ p(\\mathbf{X}):= a\\cdot \\mathbf{X} + b \\cdot \\mathbf{XX^{\\top}X} + c\\cdot \\mathbf{XX^{\\top}XX^{\\top}X} + \\dots $$\n此时，就可以用一种 sign 近似 https://epubs.siam.org/doi/10.1137/0707031 来完成，即：\n$$ p(\\Sigma) := \\frac{3}{2}\\Sigma - \\frac{1}{2}\\Sigma\\Sigma^{\\top}\\Sigma $$\n可以用单变量来可视化看一下：\n$$ p(x) = \\frac{3}{2} x - \\frac{1}{2} x^{3}; \\quad p_{n}(x)= \\underbrace{ p \\circ p \\circ \\dots \\circ p }_{ n }(x) $$\n当 $n=14$ 时，可以发现，在 $[0, \\sqrt{ 3 }]$ 范围内可以做到很好地近似 sign，即结果为 $1$\n而上述过程被称为「Newton-Schulz Iteration」，因为我们的目的是想让 $\\mathbf{\\Sigma}$ 变成单位矩阵，所以也可称为矩阵的「零次幂」（Zero Power）求解过程\n让我们简要总结一下推导出的 Muon 公式，其中 $\\eta$ 是学习率\n$$ \\mathbf{W}_{} \\gets \\mathbf{W} - \\eta \\cdot\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right)\\text{NewtonSchulz}(\\nabla_{\\mathbf{W}}\\mathcal{L}), \\mathbf{W} \\in\\mathbb{R}^{m \\times n} $$\n更新规则 实际上 Muon 的更新规则如下：我们并非是对梯度做正交化，而是对 Nesterov Style 的动量做，然后系数是 $1, \\sqrt{ m/n }$ 的最大值\n$$ \\begin{align} \\mathbf{M}_{t} \u0026amp; = \\beta \\, \\mathbf{M}_{t-1} + (1-\\beta) \\nabla_{\\mathbf{W}}\\mathcal{L}\\\\ \\mathbf{O}_{t} \u0026amp; = \\text{NewtonSchulz}(\\beta \\,\\mathbf{M}_{t} + (1-\\beta)\\nabla_{\\mathbf{W}}\\mathcal{L})\\\\ \\mathbf{W}_{t} \u0026amp; = \\mathbf{W}_{t-1} - \\eta \\cdot {\\color{#08F}\\max \\left( 1, \\sqrt{ \\frac{m}{n} } \\right)} \\mathbf{O}_{t} \\end{align} $$\n与谱条件的联系 先联系上述式 $\\#1$ 和式 $\\#2$：\n$$ \\begin{align} \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} \u0026amp; := \\max_{\\boldsymbol{x} \\neq \\boldsymbol{0}} \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}}}{ \\|\\boldsymbol{x}\\|_{\\text{RMS}}} \\\\ \\|\\mathbf{W}\\|_{\\text{RMS} \\to \\text{RMS}} \u0026amp; = \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2} \\end{align} $$\n那么可以导出：\n$$ \\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} \\leq \\|\\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}} \\|\\boldsymbol{x}\\|_{\\text{RMS}}=\\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2}\\|\\boldsymbol{x}\\|_{\\text{RMS}} \\tag{\\#6} $$\n如果想要控制输出的 RMSNorm，即 $\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{RMS}} = \\mathcal{O}(1)$，假设 $\\|\\boldsymbol{x}\\|_{\\text{RMS}}=\\mathcal{O}(1)$，则：\n$$ \\sqrt{ \\frac{n}{m} }\\|\\mathbf{W}\\|_{2}\\|\\boldsymbol{x}\\|_{\\text{RMS}} =\\mathcal{O}(1) \\implies \\|\\mathbf{W}\\|_{2} =\\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n上面推导 Muon 是约束输出变化量来导出对权重的谱范数进行约束，但严格来说，关于输出变化量的推导是不严谨的，这也是 Muon 和谱条件 https://arxiv.org/abs/2310.17813 （Spectral Condition）的不同之处，因为当权重变了之后，输入也会随之改变，比如第二层的输入其实是第一层的输出，这里推导借鉴了苏老师关于谱条件的介绍 https://kexue.fm/archives/10795 ，记 $\\boldsymbol{x}_{k}$ 为第 $k$ 层的输出\n$$ \\begin{align} \\Delta \\boldsymbol{x}_{k} \u0026amp; = (\\boldsymbol{x}_{k-1}+\\Delta \\boldsymbol{x}_{k-1})(\\mathbf{W}_{k}+\\Delta \\mathbf{W}_{k}) - \\boldsymbol{x}_{k-1}\\mathbf{W}_{k} \\\\[5pt] \u0026amp;= \\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k}) + (\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k} + (\\Delta \\boldsymbol{x}_{k-1})(\\Delta\\mathbf{W}_{k}) \\end{align} $$\n那么：\n$$ \\begin{align} \\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} \u0026amp;= \\|\\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k}) + (\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k} + (\\Delta \\boldsymbol{x}_{k-1})(\\Delta\\mathbf{W}_{k})\\|_{\\text{RMS}} \\\\[5pt] \u0026amp;\\leq \\|\\boldsymbol{x}_{k-1}(\\Delta \\mathbf{W}_{k})\\|_{\\text{RMS}} + \\|(\\Delta \\boldsymbol{x}_{k-1})\\mathbf{W}_{k}\\|_{\\text{RMS}} + \\|(\\Delta \\boldsymbol{x}_{k-1})(\\Delta \\mathbf{W}_{k})\\|_{\\text{RMS}} \\\\ \\end{align} $$\n联系式 $\\#6$，将三项分开来看，同时沿用假设：$\\|\\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}=\\mathcal{O}(1), \\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}=\\mathcal{O}(1)$\n$$ \\begin{align} \\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} \u0026amp; \\leq \\sqrt{ \\frac{n}{m}}\\bigg(\\|\\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\Delta \\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\boldsymbol{x}_{k-1}\\|_{\\text{RMS}}\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) \\\\[5pt] \u0026amp;\\leq \\sqrt{ \\frac{n}{m} }\\bigg(\\|\\Delta\\mathbf{W}_{k}\\|_{2}+\\|\\mathbf{W}_{k}\\|_{2}+\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) \\end{align} $$\n若要求 $\\|\\Delta \\boldsymbol{x}_{k}\\|_{\\text{RMS}} =\\mathcal{O}(1)$，则：\n$$ \\sqrt{ \\frac{n}{m}}\\bigg(\\|\\Delta\\mathbf{W}_{k}\\|_{2}+\\underbrace{ \\|\\mathbf{W}_{k}\\|_{2} }_{\\mathcal{O}(\\sqrt{m/n}) }+\\|\\Delta \\mathbf{W}_{k}\\|_{2}\\bigg) =\\mathcal{O}(1) $$\n最后就导出了：\n$$ \\|\\Delta \\mathbf{W}_{k}\\|_{2} = \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n从这个角度来看，其实 Muon 是谱条件的子集，因为谱条件不仅要求控制权重的变化量，还要求控制权重本身\n直觉解释 为什么要正交化 这里给出两个原因：\nJordan（也就是 Muon 的作者）的博客 https://kellerjordan.github.io/posts/muon/ 中是这样说的：\n因为在 Transformer 模型的训练中，梯度矩阵的「条件数」（condition number）通常是非常大的，条件数的一个定义是 $\\sigma_{\\max} / \\sigma_{\\min}$，这个值越大，说明梯度矩阵是由少数主要方向主导的，即 low-rank 的结构，然后正交化可以使得那些本来很弱势的方向被重新关注\n个人认为「高条件数 -\u0026gt; low-rank」是比较牵强的，比如正常满秩的矩阵，最小的奇异值很小，也会让整体的条件数很大。但梯度矩阵是 low-rank 多半是正确的，那么正交化的确会有让弱势方向的比重增大的优势\n然而，如果按照上面的推导就知道，一开始我们是想 bound 住输出变化量的 RMSNorm，进而推导出需要在最速下降的同时控制谱范数\n$$ \\min \\, \\langle \\nabla_{\\mathbf{W}}\\mathcal{L},\\Delta \\mathbf{W}\\rangle, \\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}} =\\mathcal{O}(1) {\\color{#08F}\\implies} \\|\\Delta \\mathbf{W}\\|_{\\text{RMS}\\to \\text{RMS}}=\\mathcal{O}(1) {\\color{#08F}\\implies} \\|\\Delta \\mathbf{W}\\|_{2}= \\mathcal{O}\\left(\\sqrt{ \\frac{m}{n} }\\right) $$\n换句话说，Muon 的本质即是「控制谱范数下的最速下降」\n为什么控制谱范数 那么问题是，为什么要控制谱范数呢？或者为什么控制谱范数下的最速下降收敛更快，泛化更好呢？直观来说，我们是通过 bound 住输出的变化量 $\\|\\Delta \\boldsymbol{y}\\|_{\\text{RMS}}$ 来导出需要控制谱范数，如果更新量过大，会使得训练整体就不太稳定，同时控制住之后还可以使得反传的梯度更加健康，总结就是会使得训练过程中前传和反传更加稳定，这也是为什么有些时候 Muon 可以比 Adam 使用更大学习率的原因\n花开两朵各表一枝，控制谱范数这个推论还可以由谱条件推出来。谱条件其实为了小模型上的最优参数（比如学习率）可以迁移到更大的模型上，即参数迁移不受模型尺度影响。那么如何做到呢？即让不同尺度的模型具有相同的 training dynamics，通过控制模型每一层的输出和输出变化量的 RMSNorm 来做到。换言之，控制谱范数还可以让不同尺度模型的 training dynamics 相同，从而达到迁移参数的目的\nMoonlight 接着来介绍 Kimi 团队在 Muon 上的改进，主要是 weight decay 以及对齐 RMSNorm\nWeight Decay Kimi 团队在进行 scaling 实验时，发现在原始 Muon 的不断更新下，权重的 RMSNorm 会不断变大，可能会超出 bfloat16 的精度，进而有损性能。为了弥补这种情况，加上了权重衰减，即为下式蓝色部分\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta(\\mathbf{O}_{t} + {\\color{#08F}\\lambda \\mathbf{W}_{t-1}}) $$\n加了 weight decay 虽然一开始收敛会慢，但后面就会超过不加 weight decay 的情况；同时，如果不加 weight decay，长时间训练后就会跟 AdamW 很接近\nRMSNorm 对齐 将梯度进行奇异值分解： $\\nabla_{\\mathbf{W}}\\mathcal{L}=\\mathbf{U\\Sigma V^{\\top}}$，即 $\\mathbf{U}\\in\\mathbb{R}^{m\\times r}, \\mathbf{V}\\in\\mathbb{R}^{n\\times r}, \\mathbf{\\Sigma} \\in\\mathbb{R}^{r \\times r}$，其中 $r=\\text{rank}(\\nabla_{\\mathbf{W}}\\mathcal{L})\\leq \\min(m, n)$\n向量的 RMSNorm 计算如下：\n$$ \\|\\boldsymbol{x}\\|_{\\text{RMS}} = \\sqrt{\\frac{1}{n}\\sum_{i} \\boldsymbol{x}_{i}^{2}} $$\n那么同理可得矩阵的 RMSNorm 计算：\n$$ \\|\\mathbf{W}\\|_{\\text{RMS}} = \\sqrt{ \\frac{1}{mn} \\sum_{i}\\sum_{j} w_{ij}^{2} } $$\n下面推导依然按照先前对「梯度正交化」的角度，首先：\n$$ \\|\\mathbf{O}_{t}\\|_{\\text{RMS}} = \\|\\mathbf{UV}^{\\top}\\|_\\text{RMS} = \\sqrt{ \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n}\\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2}} $$\n由于 $\\mathbf{U}, \\mathbf{V}$ 都是正交矩阵，所以其行向量和列向量都是「单位向量」，则：\n$$ mn \\|\\mathbf{O}_{t}\\|_{\\text{RMS}}^{2} = \\sum_{i=1}^{m}\\sum_{j=1}^{n}\\sum_{k=1}^{r} u_{ik}^{2}v_{kj}^{2} = \\sum_{k=1}^{r} \\left(\\sum_{i=1}^{m} u_{ik}^{2}\\right)\\left(\\sum_{j=1}^{n}v_{kj}^{2}\\right) = \\sum_{k=1}^{r} 1 = r $$\n同时因为实际情况下「严格低秩」的概率比较小，所以不妨按满秩来算，即 $r=\\min(m,n)$\n$$ \\|\\mathbf{O}_{t}\\|_{\\text{RMS}} = \\sqrt{ \\frac{r}{mn} } = \\sqrt{ \\frac{1}{\\max(m, n)} } $$\n而实际 LLMs 训练时，不同的权重矩阵 $m,n$ 不同，则导致更新量的 RMSNorm 不均衡，所以可以用一个系数来「归一化更新量的 RMSNorm」，即：\n$$ \\left\\|\\sqrt{ \\max(m,n) } \\cdot \\mathbf{O}_{t}\\right\\|_{\\text{RMS}} = 1 $$\n同时观察到对于 Adam 来说，其 $\\|\\mathbf{O}_{t}\\|_{\\text{RMS}} \\in[0.2, 0.4]$，这里为了可以直接迁移之前 Adam 的学习率，就进一步对齐 Adam 更新量的 RMSNorm，即将 Muon 实际上更新量的 RMSNorm 控制在 $0.2$ 左右：\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta\\left({\\color{#08F}0.2\\sqrt{ \\max(m,n) }}\\mathbf{O}_{t}+\\lambda \\mathbf{W}_{t-1}\\right) $$\n对比一下 Muon 的系数：\n$$ \\mathbf{W}_{t} = \\mathbf{W}_{t-1} - \\eta\\left( {\\color{#08F}\\max\\left( 1,\\sqrt{ \\frac{m}{n} }\\right)} \\mathbf{O}_{t} + \\lambda \\mathbf{W}_{t-1}\\right ) $$\n实现 接下来到了喜闻乐见的上代码环节，具体会实现原始的 Muon 以及 Kimi 在 Muon 上的改进\nNewton Schulz 迭代 Newton Schulz 的实现主要参考官方的源代码 https://github.com/KellerJordan/Muon/blob/master/muon.py ，记 Frobenius Norm $\\|\\mathbf{W}\\|_{\\text{F}}$，其定义如下：\n$$ \\|\\mathbf{W}\\|_{\\text{F}} = \\sqrt{ \\sum_{i}\\sum_{j} w_{ij}^{2}} $$\n接着我们来证明「Frobenius 归一化可以使得矩阵的奇异值缩放到 $[0, 1]$ 之间」，Frobenius Norm 满足下式：\n$$ \\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}} \\leq \\|\\mathbf{W}\\|_{\\text{F}}\\|\\boldsymbol{x}\\|_{\\text{F}} $$\n联想一下，对于向量来说的 L2 norm，不就是 Frobenius Norm 的一种形式嘛：\n$$ \\forall \\boldsymbol{x}, \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\frac{\\|\\mathbf{W}\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{F}}} \\leq \\frac{\\|\\mathbf{W}\\|_{\\text{F}}\\|\\boldsymbol{x}\\|_{\\text{F}}}{\\|\\boldsymbol{x}\\|_{\\text{F}}} = \\|\\mathbf{W}\\|_{\\text{F}} $$\n联系上面我们推导的结果：\n$$ \\max_{\\boldsymbol{x}\\neq\\boldsymbol{0}} \\frac{\\|\\mathbf{A}\\boldsymbol{x}\\|_{2}}{\\|\\boldsymbol{x}\\|_{2}} = \\|\\mathbf{A}\\|_{2} \\leq \\|\\mathbf{A}\\|_{\\text{F}} $$\n也就是说「Frobenius Norm 大于或等于 Spectral Norm」，那么 Spectral Norm 代表的是最大的奇异值，这就可以导出 Frobenius Norm 归一化后的矩阵的奇异值在 $[0,1]$ 之间\nNewton Schulz 实现 def newton_schulz5( gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): \u0026#34;\u0026#34;\u0026#34; 1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出 2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \\Sigma 变成 I, 从而完成对梯度的正交化, 即 UV^T, 但实际上是 US\u0026#39;V^T, S\u0026#39;_ii \\in [1-e, 1+e], 并且不影响实际效果 \u0026#34;\u0026#34;\u0026#34; assert gradient.ndim \u0026gt;= 2 dim1, dim2 = gradient.size(-2), gradient.size(-1) gradient = gradient.bfloat16() # 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1) # 但如果 dim1 过大，运算效率和内存开销都不友好 # 所以可以转置，最后返回前转回来即可 if dim1 \u0026gt; dim2: gradient = gradient.mT # 进行 Frobenius Norm 以确保奇异值在 [0, 1] x = gradient / (gradient.norm() + eps) a, b, c = coefficients for _ in range(steps): xx_T = x @ x.mT xx_Tx = xx_T @ x x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) if dim1 \u0026gt; dim2: x = x.mT return x 下图是 $y=ax + bx^{3}+cx^{5}$ 以及 $y=1$（红线），观察发现等于 $1$ 的情况不多\n换句话说，似乎假设并不成立了\n$$ \\begin{align} \\text{Theory: } \\nabla_{\\mathbf{W}}\\mathcal{L} \u0026amp; = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{UV^{\\top}} \\\\ \\text{Empricial: } \\nabla_{\\mathbf{W}} \\mathcal{L} \u0026amp; = \\mathbf{U\\Sigma V^{\\top}} \\mapsto \\mathbf{U{\\color{#08F}\\Sigma'}V^{\\top}}, \\Sigma_{ii}' \\in [0.6, 1.2] \\end{align} $$\n真实实验结果是我们需要确保在 $[0,1]$ 内收敛到 $[1-\\epsilon, 1+\\epsilon]$ 即可，按照 Jordan 的博客， $\\epsilon$ 可以大到 $0.3$ 而不影响性能，并不需要严格等于 $1$\n还有一个问题，为什么使用 Newton-Schulz 来进行正交化呢？因为正交化还可以用其他方法：\n不使用 SVD 的原因是因为太慢了 不使用 Coupled Newton Iteration 是因为在 bfloat16 上不稳定，至少得需要 float32 才行 再简单说说如何来微调这些系数，下图是两种系数的对比（取自 Jordan 的博客）：\n主要就是 $\\phi'(0)$ 的值，或者说 $0$ 处以及附近点的斜率，这个决定了初始的收敛速度，大些是更好的\n更新 接着来实现 Muon 的更新规则，默认加上了 weight decay，以及通过判断 rms_match 是否为 None 来决定使用 Kimi 的 rms match 还是官方默认的 scale 系数\nMuon 更新代码 class Muon(torch.optim.Optimizer): ... # 先省略其他部分 @torch.no_grad() def step_muon(self, param_groups): def update_momentum(): momentum.lerp_(grad, 1 - beta) momentum_ns = beta * momentum + (1 - beta) * grad if nesterov else momentum return momentum_ns def update(): p.mul_(1 - lr * weight_decay) if rms_match is not None: # 使用 Kimi 提出的 scale scale = rms_match * math.sqrt(max(p.shape[-2:])) else: # original scale by KellerJordan scale = max(1, p.size(-2) / p.size(-1))**0.5 p.add_(delta, alpha=-lr * scale) for group in param_groups: params = group[\u0026#39;params\u0026#39;] lr = group[\u0026#39;lr\u0026#39;] weight_decay = group[\u0026#39;weight_decay\u0026#39;] beta = group[\u0026#39;beta\u0026#39;] nesterov = group[\u0026#39;nesterov\u0026#39;] ns_steps = group[\u0026#39;ns_steps\u0026#39;] rms_match = group[\u0026#39;rms_match\u0026#39;] for p in params: grad = p.grad state = self.state[p] if \u0026#39;momentum\u0026#39; not in state: state[\u0026#39;momentum\u0026#39;] = torch.zeros_like( grad, memory_format=torch.preserve_format ) momentum = state[\u0026#39;momentum\u0026#39;] momentum_ns = update_momentum() if momentum_ns.ndim == 4: # 拉伸 conv filter 参数 momentum_ns = momentum_ns.view(momentum_ns.size(0), -1) delta = self.newton_schulz5(momentum_ns, ns_steps) update() def newton_schulz5( self, gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): \u0026#34;\u0026#34;\u0026#34; 1. 用 Newton Schulz 计算矩阵的零次幂, 只负责二维矩阵, 系数是 Muon 官方搜出 2. 给定 gradient SVD = USV^T, 尽管初衷是通过 NS 来让 \\Sigma 变成 I, 从而完成对梯度的正交化, 即 UV^T, 但实际上是 US\u0026#39;V^T, S\u0026#39;_ii \\in [1-e, 1+e], 并且不影响实际效果 \u0026#34;\u0026#34;\u0026#34; assert gradient.ndim \u0026gt;= 2 dim1, dim2 = gradient.size(-2), gradient.size(-1) gradient = gradient.bfloat16() # 运算时会大量涉及 g @ g^T, 所以形状是 (dim1, dim1) # 但如果 dim1 过大，运算效率和内存开销都不友好 # 所以可以转置，最后返回前转回来即可 if dim1 \u0026gt; dim2: gradient = gradient.mT # 进行 Frobenius Norm 以确保奇异值在 [0, 1] x = gradient / (gradient.norm() + eps) a, b, c = coefficients for _ in range(steps): xx_T = x @ x.mT xx_Tx = xx_T @ x x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) if dim1 \u0026gt; dim2: x = x.mT return x 与 Adam 一道 使用时，对于非矩阵参数（比如 Layer Norm 的 gamma 系数）以及 embed, lm_head 的部分需要使用 Adam 来进行优化，所以还需要加入拆分参数以及 Adam 的使用，这里也列出 Adam 的更新规则来便于读者对照阅读代码：\n$$ \\begin{align} \\boldsymbol{m}_{t} \u0026amp; = \\beta_{1}\\boldsymbol{m}_{t-1} + (1-\\beta_{1})\\nabla_{\\mathbf{W}}\\mathcal{L} \\\\ \\boldsymbol{v}_{t} \u0026amp; = \\beta_{2}\\boldsymbol{v}_{t-1} + (1-\\beta_{2})\\nabla_{\\mathbf{W}}^{2}\\mathcal{L} \\\\ \\boldsymbol{\\hat{m}}_{t} \u0026amp; = \\frac{\\boldsymbol{m}_{t}}{1- \\beta_{1}^{t}}, \\boldsymbol{\\hat{v}}_{t} = \\frac{\\boldsymbol{v}_{t}}{1-\\beta_{2}^{t}} \\\\ \\mathbf{W}_{t} \u0026amp; = \\mathbf{W}_{t-1} - \\eta\\left( \\frac{\\boldsymbol{\\hat{m}}_{t}}{\\sqrt{ \\boldsymbol{\\hat{v}}_{t} }+\\epsilon} + \\lambda \\mathbf{W}_{t-1} \\right) \\end{align} $$\n还有一条实践的 tip，如果是用在 Transformer 中的 Q，K，V 上，最好是单独的权重，而不是整体一个大的权重，然后再 split 出 Q，K，V\nMuon 整体实现 class Muon(torch.optim.Optimizer): def __init__( self, lr: float, # 仅占位符 params: Iterator[Tuple[str, Parameter]], weight_decay: float = 0.1, beta: float = 0.95, nesterov: bool = True, ns_steps: int = 5, rms_match: float = 0.2, adam_betas: list[float] = [0.9, 0.999], adam_eps: float = 1e-8, ): param_groups = self.split_params(params) other_defaults = dict(betas=adam_betas, eps=adam_eps) param_groups = [ dict( params=param_groups[\u0026#39;hidden_matrix_params\u0026#39;], # Muon 去优化的参数 weight_decay=weight_decay, beta=beta, nesterov=nesterov, ns_steps=ns_steps, rms_match=rms_match ), dict( params=param_groups[\u0026#39;non_matrix_params\u0026#39;], weight_decay=0., # 默认非矩阵参数不加 weight decay **other_defaults ), dict( params=param_groups[\u0026#39;embed_lm_head_matrix_params\u0026#39;], weight_decay=weight_decay, **other_defaults, ) ] super().__init__(param_groups, {}) def step(self): muon_param_groups = [group for group in self.param_groups if \u0026#39;ns_steps\u0026#39; in group] other_param_groups = [ group for group in self.param_groups if \u0026#39;ns_steps\u0026#39; not in group ] self.step_muon(muon_param_groups) self.step_adamw(other_param_groups) @torch.no_grad() def step_muon(self, param_groups): ... @torch.no_grad() def step_adamw(self, param_groups): def update_momentum(): momentum1.lerp_(grad, 1 - beta1) momentum2.lerp_(grad.square(), 1 - beta2) def update(): bias_correction1 = 1 - beta1**step bias_correction2 = 1 - beta2**step scale = bias_correction1 / bias_correction2**0.5 delta = momentum1 / (momentum2.sqrt() + eps) p.mul_(1 - lr * weight_decay) p.add_(delta, alpha=-lr / scale) for group in param_groups: params = group[\u0026#39;params\u0026#39;] lr = group[\u0026#39;lr\u0026#39;] weight_decay = group[\u0026#39;weight_decay\u0026#39;] beta1, beta2 = group[\u0026#39;betas\u0026#39;] eps = group[\u0026#39;eps\u0026#39;] for p in params: grad = p.grad state = self.state[p] if \u0026#39;step\u0026#39; not in state: state[\u0026#39;step\u0026#39;] = 0 state[\u0026#39;momentum1\u0026#39;] = torch.zeros_like( grad, memory_format=torch.preserve_format ) state[\u0026#39;momentum2\u0026#39;] = torch.zeros_like( grad, memory_format=torch.preserve_format ) momentum1 = state[\u0026#39;momentum1\u0026#39;] momentum2 = state[\u0026#39;momentum2\u0026#39;] state[\u0026#39;step\u0026#39;] += 1 step = state[\u0026#39;step\u0026#39;] update_momentum() update() def split_params(self, params: Iterator[Tuple[str, Parameter]]): # params: model.named_parameters() # Muon 只负责优化除了 embed, lm_head 之外的「矩阵」参数 param_dict = {pn: p for pn, p in params if p.requires_grad} non_matrix_params = [p for p in param_dict.values() if p.ndim \u0026lt; 2] embed_lm_head_matrix_params = [ p for pn, p in param_dict.items() if p.dim() \u0026gt;= 2 and (\u0026#39;embed\u0026#39; in pn or \u0026#39;lm_head\u0026#39; in pn) ] hidden_matrix_params = [ p for pn, p in param_dict.items() if p.ndim \u0026gt;= 2 and \u0026#39;layers\u0026#39; in pn ] return dict( non_matrix_params=non_matrix_params, embed_lm_head_matrix_params=embed_lm_head_matrix_params, hidden_matrix_params=hidden_matrix_params ) def newton_schulz5( self, gradient: Tensor, steps: int = 5, coefficients: list[float] = [3.4445, -4.7750, 2.0315], eps: float = 1e-7, ): ... 实验 setting 设定 数值 $\\#$params 0.6B $\\#$ train tokens 58B $\\#$ eval tokens 0.2B lr 8e-4 weight decay 0.1 seq length 8192 global batch 384 LR schedule linear warm (0.01) cosine decay (1.) 不同的 scale 系数 首先是对照原始 Muon 和 Kimi Muon 的系数，这里偷懒就不单独为原始的 Muon 像 speedrun 那样为 Adam 和 Muon 调制不同的学习率，统一用一个学习率，可以发现无论是训练还是 eval，Kimi 的系数都会更好\nKimi Muon VS AdamW 接着我们来与 AdamW 对比，可以发现是 Kimi Muon 收敛更快\nFLOPs 分析 我们来分析一下 FLOPs（浮点数运行次数），对于一个 $\\mathbf{A} \\in\\mathbb{R}^{m\\times n},\\mathbf{B}\\in\\mathbb{R}^{n\\times p}$，两者相乘的 FLOPs 是 $mp(2n-1)=2mnp-mp$，$mp$ 是新矩阵的元素个数，然后每个新矩阵的元素需要经过 $n$ 次乘法和 $n-1$ 次加法\n介绍完基本概念，来先分析一下 NS 迭代的 FLOPs，主要的计算量在如下代码处：\nfor _ in range(steps): xx_T = x @ x.mT # 1. m x n, n x m =\u0026gt; m x m xx_Tx = xx_T @ x # 2. m x m, m x n =\u0026gt; m x n # 3. xx_T @ xx_Tx: m x m, m x n =\u0026gt; m x n x = a * x + b * xx_Tx + c * (xx_T @ xx_Tx) # 4 对于 1 的操作，FLOPs 为 $m^{2}(2n-1)=2m^{2}n-m^{2}$；对于 2 的操作，FLOPs 为 $mn(2m-1)=2m^{2}n-mn$；对于 3 的操作，FLOPS 为 $mn(2m-1) = 2m^{2}n-mn$，然后需要将其进行相加：\n$$ \\begin{align} \\underbrace{ 2m^{2}n-m^{2} }_{ \\mathbf{XX^{\\top}} }+\\underbrace{ 2m^{2}n-mn }_{ \\mathbf{XX^{\\top}X} } + \\underbrace{ mn }_{ a\\mathbf{X} } +\\underbrace{ mn }_{ b\\mathbf{XX^{\\top}X} } + \\underbrace{ mn }_{ c \\times \\dots } + \\underbrace{ 2m^{2}n-mn }_{ \\mathbf{XX^{\\top}XX^{\\top}X} } + \\underbrace{ 2mn }_{ \\text{ 两次加法} } \\\\ = 6m^{2}n-m^{2}+3mn \\approx 6m^{2}n \\end{align} $$\n然后我们运行 $T$ 步 NS 迭代，即为 $6Tm^{2}n$\n对于一个线性层来说，我们对其进行前向和反向的计算的 FLOPs 为多少？这里省略对于偏置的计算，因为不是主要计算量，记输入矩阵 $\\mathbf{X} \\in \\mathbb{R}^{B\\times m}$\n$$ \\begin{align} \u0026amp;\\text{Forward: } \\mathbf{Y} = \\mathbf{XW}: B\\times m, m\\times n= B\\times n\\implies Bn(2m-1) \\approx 2Bmn \\\\ \u0026amp;\\text{Backward 1: } \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{X}} = \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{Y}}\\mathbf{W^{\\top}}: B\\times n, n \\times m \\implies Bm(2n-1)\\approx 2Bmn \\\\ \u0026amp;\\text{Backward 2: } \\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{W}} = \\mathbf{X}^{\\top}\\frac{\\text{d}\\mathcal{L}}{\\text{d}\\mathbf{Y}}: m \\times B, B\\times n\\implies mn(2B-1)\\approx 2Bmn \\end{align} $$\n所以整个加起来即为 $6Bmn$，这里计算输入的梯度是因为在网络中，当前的输入其实就是前一层的输出，计算当前输入的梯度是为了 back-propogation 的时候便于计算\n那么使用 Muon 时额外带来的开销是：\n$$ \\frac{6Tm^{2}n}{6Bmn} = \\frac{Tm}{B} $$\n当 $T=5$ 时，对于 nanoGPT 以及 Llama 405B 而言，额外的开销并不算很大：\n$$ \\begin{align} \u0026amp;\\text{nanoGPT: } 5 \\times \\frac{768}{524288} = 0.7\\% \\\\ \u0026amp;\\text{Llama 405B: } 5 \\times \\frac{16384}{16\\times10^{6}} = 0.5\\% \\end{align} $$\n","permalink":"http://yunpengtai.top/posts/muon/","summary":"本文将主要涵盖以下内容： 从理论角度推导 Muon 优化器，介绍其「控制谱范数下的最速下降」的特性，主要在 Bernstein 的博客 https://jeremybernste.in/writing/deriving-muon 的基础上进行延伸。值得注意的是，推导","title":"Muon: 控制谱范数下的最速下降"},{"content":" Mihomo_config 这里存放了我个人的 Mihomo 覆写文件，可以进行参考，请务必读懂本文后使用，不要盲目照抄 YAML 引子 代理，对于很多人来说并不陌生，尤其是在科研领域，例如在谷歌学术上查找论文的 BibTeX 时。然而，我们常常对网络代理的原理、功能以及如何更好地配置等问题缺乏深入了解。这其实是不可取的，因为网络代理已经深入到我们生活的方方面面，因此，我们更应该去理解它，并让它更好地为我们服务。\n然而，网上关于代理具体原理的解释却相对匮乏，大多是零星片段，甚至存在互相矛盾之处。官方文档也往往仅限于对某些具体配置的说明。为此，笔者投入了大量精力，广泛阅读了各类文献和相关博客，力求去芜存菁，汇聚成本文。对于部分文献未涉及之处，笔者结合自身理解进行了补充，但囿于水平，恐有疏漏。若读者能及时指出，笔者将不胜感激！\n请注意，本文只讨论计算机网络中的各种原理，若读者将此用于违法之事，读者应个人承担相应责任，与本文和作者无关！\n客户端 在展开对具体技术细节的讨论之前，我们先来聊聊「客户端」的选择，因为客户端的些许不同，会导致支持的功能也会存在差异。这里我主要介绍我正在用的几款开源软件，读者可以根据需要进行取舍\n本章只会介绍电脑端以及移动端，至于其他的如路由器等，请读者自行查阅资料\n电脑端 我个人是在使用 Windows 11 23H2，不过读者不必担心自己是 MacOS 抑或是 Linux，我用的软件是多端适配~\n我早期用的软件是 Clash for Windows（CFW），后面 CFW 因为一些不可抗力原因呢就停止了维护，我就去寻找了其他的软件，用过 Clash Verge 一段时间，后面这个也停止维护了，然后目前是有人接力了 Verge 继续开发，叫 Clash Verge Rev，目前我也一直在用，软件也在积极更新中~\nUI 还不错，而且最关键的是内置了 Clash Meta（Mihomo）内核，对于小白来说，可以这样理解：内核实现了诸多复杂的功能，比如自定义一些域名的代理，比如浏览 google.com 用香港的节点，目前 Mihomo 也是社区中维护最为频繁而且功能也好用的内核 附上社区中支持 Mihomo 的软件清单供读者参考，本文只会介绍我用过的软件\n有些读者可能在用，或者考虑用 Mihomo Party，我个人使用过一段时间，并不推荐，尤其是对于自己写覆写配置的人来说，因为它这个软件很多地方压根不遵循你的配置，比如关于 geodata 的地址，更新频率，甚至你明明在配置里写明 DNS 支持 IPv6，然而你点开正在运行的配置，IPv6 还是 false，有很多地方你都得手动去 check，不建议使用，而上面的 clash verge rev 基本都遵循你的覆写配置，我目前检查下来只有总的 IPv6 控制需要额外去软件设置\n移动端 我个人用的是 Android 手机，安卓端一开始我用的是 Clash Meta for Android，也是支持 Mihomo 内核的。然而发现这款软件用途有些局限，手机流量连接时开代理没法访问我的纯 IPv6 服务器，即使配置和软件覆写中 IPv6 全开的情况（解决方案可以看本文的开代理访问IPv6），寻找解决方案时发现另一款软件叫 FlClash， 不仅能够解决我这个问题，UI 支持 material you 以及配置中的图标，就很好看；同时可定义的配置更多\n说完了 Android，我还有一台 iPad，目前是在用 sing-box和 Shadowsocket，后者自不必多说，比较简单的代理软件，前者也在用的原因是其免费，同时也支持比较复杂的功能，比如 AdGuard DNS Filter，当然读者如果已经购买了 Surge 等软件，可以继续沿用~\n当你访问时 接下来讨论一些更有趣的，本章会详细介绍客户端向你的代理软件发起请求时，Mihomo 内核的 DNS 工作流程，比如浏览器访问某个网页\n如果一开始客户端访问的就是 IP，就不用费很多事，直接到关于 IP 的规则，比如是直连还是走代理等，不需要域名解析。下面是一个简单的 IP 规则举例，CIDR 表示 IP 的范围\nrules: # 以 192.168.1.0 开始的 IP 段 - IP-CIDR,192.168.1.0/23,DIRECT # 走直连 - IP-CIDR,103.151.150.0/23,香港-代理 # 走香港-代理 走代理的意思是说，代理软件将访问的请求发送给你的代理节点（这里就是香港的服务器），然后代理节点将访问接收到的 IP 或者域名，最后代理节点会把响应返回给代理软件；走直连的意思是直接和该 IP 进行连接\n而对于域名的访问就要复杂一些，故而接下来将详细讨论客户端向某个域名发起请求会具体发生什么，为了让概念更加清晰易懂，下面的流程图中省略了 IP 规则的部分 部分参考自 Mihomo 官方文档\nflowchart TB Start[客户端发起请求] --\u0026gt; fake[fake-ip 反查] fake[fake-ip 反查] --\u0026gt; Domain[基于域名匹配规则] fake --\u0026gt; |fakeip-filter|system[系统解析 DNS] Domain --\u0026gt; |匹配过程中|IP[遇到 IP 规则] Domain --\u0026gt; reject[匹配到 Reject 规则] Domain --\u0026gt; |匹配到直连规则|Cache IP --\u0026gt; Cache Domain --\u0026gt; |匹配到代理规则|Remote[通过代理服务器解析域名并建立连接] Cache --\u0026gt; |Cache 未命中|NS[匹配 nameserver-policy 并查询 ] Cache --\u0026gt; |Cache 命中|Get NS --\u0026gt; |匹配成功| Get[将查询到的 IP 用于匹配 IP 规则] NS --\u0026gt; |没匹配到| NF[nameserver/fallback 并发查询] NF --\u0026gt; Get[查询得到 IP] Get --\u0026gt; |缓存 DNS 结果|Cache[(查询 DNS 缓存)] 确定域名前 在讲具体的域名规则之前，我们还得确保一件事：就是你的代理软件得知道你访问的「目标域名」，听起来是不是有点意外。有时候代理软件不知道你访问的目标域名是什么，比如以下情况：\n假设你的浏览器没有直接通过代理协议（如 socks5 或 http 代理）将请求发送到代理软件，而是依赖系统的普通网络栈。在这种情况下，浏览器会向系统发起域名解析，操作系统会通过本地的 DNS 服务器将域名解析为 IP 地址。然后，浏览器使用这个 IP 地址建立连接。这种场景下，浏览器完全没有直接传递域名给代理，代理只知道「一个 IP 地址」\n那这个时候很有可能就会有问题，因为代理压根不知道你访问的是哪个域名：故而需要引入一个新的机制：fake-ip 因为 redir-host 已经过时了，故本文并不会进行讨论\nfake-ip 的定义出自 RFC3089，关于 fake-ip 的介绍，Clash 知识库介绍的很好，我这里摘抄一下：\nfake-ip 池的默认 CIDR 是 198.18.0.1/16，一个保留的 IPv4 地址空间（CIDR 表示的是一个 IP 范围）\n当 DNS 请求被发送到 Clash DNS 时，Clash 内核会通过管理内部的域名和其 fake-ip 地址的映射，从池中分配一个空闲的 fake-ip 地址\n以使用浏览器访问 https://google.com 为例\n浏览器向 Clash DNS 请求 google.com 的 IP 地址 Clash 检查内部映射并返回 198.18.1.5 浏览器向 198.18.1.5 的 80/tcp 端口发送 HTTP 请求 当收到 198.18.1.5 的入站数据包时，Clash 查询内部映射，发现客户端实际上是在向 google.com 发送数据包 a. Clash 可能仅将域名发送到 SOCKS5 或 shadowsocks 等出站代理，并与代理服务器建立连接；b. 或者 Clash 可能会基于 SCRIPT、GEOIP、IP-CIDR 规则或者使用 DIRECT 直连出口查询 google.com 的真实 IP 地址 我总结一下：简而言之，fake-ip 通过拦截客户端的解析请求从而知道真正需要访问的域名是什么，进而可以使用提前设置的分流规则\n当然，你可能会疑惑，我明明都输入了 google.com 了，为什么还要「多此一举」呢？尽管大多数时候客户端的请求中都指明了访问的域名，但有些时候并不能直接获得域名，所以得都拦截下来，以确保代理知道你要访问的域名\n刚刚我们也说过会有系统接管 DNS 解析的情况，有些我们并不需要通过代理软件进行解析，比如访问 localhost:8080 这种本地的情况，抑或是压根不需要代理的 Windows 软件等，这个时候利用 fakeip-filter 来做到：\ndns: # 配置不使用 fake-ip 的域名，代理跳过解析，让系统自己解析 fake-ip-filter: - \u0026#34;*\u0026#34; # 只匹配 localhost 等没有. 的主机名 - \u0026#34;*.lan\u0026#34; - \u0026#34;xbox.*.microsoft.com\u0026#34; - \u0026#34;+.xboxlive.com\u0026#34; - \u0026#34;localhost.ptlogin2.qq.com\u0026#34; 确定域名后 代理软件通过 fake-ip 确认了客户端想要访问的域名之后，就要开始匹配域名的相关规则，以下是一个域名规则示例：\nrules: - DOMAIN-SUFFIX,baidu.com,REJECT # 直接拒绝 - DOMAIN-SUFFIX,google.com,美国-代理 # 走代理 - DOMAIN-SUFFIX,baidu.com,DIRECT # 直连 示例说的是以下情况：\n当你访问的域名以 baidu.com 结尾时，直接拒绝该请求 当你访问的域名以 google.com 结尾时，走美国代理，此时就由你的美国服务器来负责解析域名对应的 DNS，访问后把响应返回给你 或者是以 baidu.com 结尾，走直连 域名直连 走代理和拒绝没啥多说的，继续讲直连的过程，会进一步查询 DNS 缓存，当发现需要访问的域名在缓存内存在时，就直接获得了其 IP\n当 cache 命中不了的时候，接着会到 nameserver-policy 来解析，nameserver 中文是「域名服务器」，具体负责将域名转成 IP，policy 代表着某种解析的策略，我们可以设置一些规则让某些域名让 A 域名服务器来解析，而另一些让 B 来解析。设置正确的好处是可以更准确更快地获得 IP，比如你访问国内，用阿里和腾讯的域名服务器来解析；访问国外的，就用谷歌和 Cloudflare 的来解析\n再具体介绍 nameserver-policy 之前，先得判断域名是否是国内或国外，这里需要借助 geosite 和 geoip 的帮助，并成为 geodata。这两个资源文件将一些域名和 IP 对应的地区进行配对，当然，有些情况里面还会有自定义的分组，比如所有 Steam 相关的域名和 IP 等\n我们可以配置 geodata 的加载地址，我这里用的是 Mihomo 社区中的配置，有些读者 可能在用 Loyalsoldier 的 v2ray-rules-dat 和 geoip，其实 Mihomo 社区的跟前面两个基本上是一致的，比较大的不同是社区的删掉了 geosite 中大部分的广告域名，因为仅靠 geosite 来屏蔽广告是很有限的。我本人是用了其他的来规避广告（下文会具体展开），所以就选择社区的版本。这里还用 cdn 了进行加速，原生的 GitHub release 有时候可能访问不上\n# 用 dat 格式文件来加载 geoip，默认为 false，用 mmdb 格式文件 geodata-mode: true geo-auto-update: true # 自动更新 geodata geo-update-interval: 24 # 24 小时更新一次 # 自定义 geodata 的加载地址 geox-url: # geoip，geosite 存储的是 IP 和域名对应的分组，可能是国家，也可能是某些分组，比如广告 geoip: \u0026#34;https://cdn.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/geoip.dat\u0026#34; geosite: \u0026#34;https://cdn.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/geosite.dat\u0026#34; # 这里用 dat 就没必要配置 mmdb # ASN 指的是一些注册的企业，机构和个人，参见 https://github.com/VirgilClyne/GetSomeFries/wiki/%F0%9F%8C%90-ASN#%E7%AE%80%E4%BB%8B asn: \u0026#34;https://cdn.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/GeoLite2-ASN.mmdb\u0026#34; 接着我们来设置 nameserver-policy，国内的全送给阿里和腾讯的域名服务器进行解析，而国外的由谷歌和 Cloudflare 来负责解析\ndns: nameserver-policy: # private 其实指的是 Reserved IP（保留地址），比如 192.168.0.0 \u0026#34;geosite:cn,private\u0026#34;: - https://dns.alidns.com/dns-query # 阿里 - https://doh.pub/dns-query # 腾讯 \u0026#34;geosite:geolocation-!cn\u0026#34;: # 已经包含了 gfw 类 - \u0026#34;https://dns.cloudflare.com/dns-query\u0026#34; - \u0026#34;https://dns.google/dns-query\u0026#34; 如果设置了 nameserver-policy 并且匹配成功，就会通过指定的 nameserver 来进行解析，如果解析成功，就直接获得 IP；而如果解析失败，也就失败了，并没有兜底规则！\n解析失败并没有兜底规则这一点提醒我们用起来需要注意风险，网上有一些配置预先设置了很多的策略，比如阿里的服务，腾讯的服务，B 站的服务等等全部进行分开解析，放到策略里。我个人并不推荐，因为有时候解析会错误，我之前是模仿网上的配置将 B 站的解析送到腾讯的域名服务器，然后就遇到过解析不出来的情况。反正多一次并发 DNS 查询并不会很大程度影响性能，何必要大费周章地预置呢，还会增加风险 :doge\n当没有设置 nameserver-policy 时，就走全局的 nameserver 来解析。读到这里读者可能会疑惑，上述的 nameserver-policy 不是已经涵盖了所有的网站嘛，国内和国外，这里的 nameserver 设置是否不必要了呢？答案是肯定有必要，因为 geosite 是一个规则集，有些网站并未包含在这些规则内，比如常见的个人小型博客，所以还需要默认的 nameserver 的规则\ndns: nameserver: # 不要用 IP - https://dns.alidns.com/dns-query # 阿里 - https://doh.pub/dns-query # 腾讯 fallback: - \u0026#34;https://dns.cloudflare.com/dns-query\u0026#34; - \u0026#34;https://dns.google/dns-query\u0026#34; # 如果满足了 filter 的条件，就用 nameserver 的解析结果 # 否则就用 fallback 的解析结果 fallback-filter: geoip-code: CN 这里一旦设置了 fallback 项，nameserver 并发解析的同时，还会向 fallback 里的域名服务器并发请求，然后根据设置的 fallback-filter 来决定最终用谁的解析结果。这里是说如果是国内就用阿里和腾讯的解析结果，而国外就用谷歌和 Cloudflare 的解析结果，避免 DNS 被污染\n多余的 DNS 解析 有些时候当你的配置文件写的不对时，代理软件会进行多余的 DNS 解析，比如说当你访问 google.com，明明你的规则集里写了遇到谷歌走代理，但你打开代理软件 log 时发现谷歌域名对应的 IP 还是被解析好了，这就是上图中画的另外一种情况：你的 IP 规则写到了域名规则中，比如这种情况：\nrules: - IP-CIDR,103.151.150.0/23,香港-代理 - DOMAIN-SUFFIX,google.com,美国-代理 Mihomo 内核是从上往下的顺序进行检查的，也就是说当代理软件发现有 IP 相关的规则，它会首先将 google.com 转换成 IP，然后判断是否满足，将 IP 规则混在域名规则中造成了多余的 DNS 解析，因为对于 google.com 的访问我们本来就是直接送给代理节点来进行访问，所以在写配置时要注意域名和 IP 规则进行分开，通常域名规则在上面，而 IP 规则在下\n写好配置 在本章我们将写一个 Mihomo 内核的覆写（override）文件，无论以后用何种机场，都可以用覆写文件自动转换，而不必依靠于机场自带的各种落后的分流规则，形同虚设的节点分组等\n注意：这里是 Mihomo 内核的覆写文件，若是 sing-box 等，请进行修改转换\n语法 Mihomo 内核的覆写文件有两种方式，一种是 yaml，另一种是 JavaScript，这里介绍更为简单的 yaml 形式，yaml 语法主要就如下几种：\n# 字典：a = {\u0026#39;b\u0026#39;: 1} a: b: 1 # 数组：a = [b, c] a: - b - c 稍微复杂一点的是「锚点」的应用，这里直接摘自 Mihomo 官方对 yaml 语法的介绍\n\u0026amp; 锚点和 * 别名，可以用来引用，\u0026amp; 用来建立锚点，\u0026laquo; 表示合并到当前数据，* 用来引用锚点\n如下示例中，因 p 这个键在 mihomo 内置中不存在，所以在 mihomo 解析配置会被忽视\n如合并时有重复的项，则不会去合并\n锚点的使用示例 p: \u0026amp;p type: http interval: 3600 health-check: enable: true url: https://www.gstatic.com/generate_204 interval: 300 proxy-providers: provider1: \u0026lt;\u0026lt;: *p url: \u0026#34;\u0026#34; path: ./proxy_providers/provider1.yaml provider2: \u0026lt;\u0026lt;: *p type: file path: ./proxy_providers/provider2.yaml 等同于\n不用锚点的内容 proxy-providers: provider1: type: http interval: 3600 health-check: enable: true url: https://www.gstatic.com/generate_204 interval: 300 url: \u0026#34;\u0026#34; path: ./proxy_providers/provider1.yaml provider2: interval: 3600 health-check: enable: true url: https://www.gstatic.com/generate_204 interval: 300 type: file path: ./proxy_providers/provider2.yaml 节点组和策略组 接着我们对节点来进行分组，下面用了正则的写法，HK 节点的意思是说，当发现以香港，或 HK，或 Hong 以及 🇭🇰 开头，并且[网站, 地址, 剩余, ..., 节点]等词没有出现过，一旦出现，就放弃匹配，关于正则表达式，不确定的读者可以去该网站进行查看\nFilterHK: \u0026amp;FilterHK \u0026#34;^(?=.*(香港|HK|Hong|🇭🇰))^(?!.*(网站|地址|剩余|过期|时间|有效|网址|禁止|邮箱|发布|客服|订阅|节点)).*$\u0026#34; FilterJP: \u0026amp;FilterJP \u0026#34;^(?=.*(日本|JP|Japan|🇯🇵))^(?!.*(网站|地址|剩余|过期|时间|有效|网址|禁止|邮箱|发布|客服|订阅|节点)).*$\u0026#34; 对节点按照地区分好组之后，这里建立选择节点的方式：自动测速和手动选择\n自动选择和手动选择 # 策略组参数锚点 # 手动选择参数 Select: \u0026amp;Select { type: select, url: \u0026#34;https://cp.cloudflare.com\u0026#34;, disable-udp: false, hidden: false, include-all: true, } # 自动测速参数 Auto: \u0026amp;Auto { type: url-test, url: \u0026#34;https://cp.cloudflare.com\u0026#34;, interval: 300, tolerance: 50, disable-udp: false, hidden: true, include-all: true, } 然后就有我们的节点策略组了，filter 的意思是针对所有的代理节点留下满足条件的\nproxy-groups: # 自动选择，会自动测速，选择最快的 - { name: 🇭🇰 - 自动选择, \u0026lt;\u0026lt;: *Auto, filter: *FilterHK } # 手动选择 - { name: 🇭🇰 - 手动选择, \u0026lt;\u0026lt;: *Select, filter: *FilterHK } 其他地区相关的节点组和策略组按照如上方式编写即可，编写好之后我们就开始进行按照不同的访问目的来选择不同地区的节点，下面是编了三组，一组是节点选择，包含了自动，手动和 DIRECT；第二组是自动选择，包含所有的自动选择，以及 telegram 的节点策略组，而且默认是用香港的节点（第一个出现的即为默认）\nproxy-groups 示例 proxy-groups: - name: 节点选择 type: select proxies: [自动选择, 手动选择, DIRECT] url: https://cp.cloudflare.com icon: https://raw.githubusercontent.com/Orz-3/mini/master/Color/Static.png - name: 自动选择 type: select proxies: [🇭🇰 - 自动选择, 🇯🇵 - 自动选择, 🇰🇷 - 自动选择, 🇸🇬 - 自动选择, 🇺🇸 - 自动选择, 🇬🇧 - 自动选择, 🇫🇷 - 自动选择, 🇩🇪 - 自动选择, 🇹🇼 - 自动选择, 🇲🇾 - 自动选择, 🇹🇭 - 自动选择, 🇻🇳 - 自动选择, 🇹🇷 - 自动选择, 🇦🇷 - 自动选择] url: https://cp.cloudflare.com icon: https://raw.githubusercontent.com/Orz-3/mini/master/Color/Urltest.png - name: Telegram type: select proxies: [🇭🇰 - 自动选择, 🇯🇵 - 自动选择, 🇰🇷 - 自动选择, 🇸🇬 - 自动选择, 🇺🇸 - 自动选择, 🇬🇧 - 自动选择, 🇫🇷 - 自动选择, 🇩🇪 - 自动选择, 🇹🇼 - 自动选择, 🇲🇾 - 自动选择, 🇹🇭 - 自动选择, 🇻🇳 - 自动选择, 🇹🇷 - 自动选择, 🇦🇷 - 自动选择] icon: https://raw.githubusercontent.com/Orz-3/mini/master/Color/Telegram.png 分流规则 讲完了基础语法，终于来到了最激动的分流规则部分了，本节的大部分规则来自于 Sukka Surge，感谢 Sukka 的贡献~\n首先我们得加载识别访问域名或者 IP 是否是广告的一些规则集，然后我们才能去选择策略，比如 REJECT\n规则集很多配置有重复的地方，这里就先建立锚点，然后来引用锚点使得配置文件更整洁\n规则参数 # 规则参数 [每12小时更新一次订阅规则，更新规则时使用（节点选择）策略] RuleSet_classical: \u0026amp;RuleSet_classical { type: http, behavior: classical, interval: 43200, format: text, proxy: 节点选择, } RuleSet_domain: \u0026amp;RuleSet_domain { type: http, behavior: domain, interval: 43200, format: text, proxy: 节点选择, } RuleSet_ipcidr: \u0026amp;RuleSet_ipcidr { type: http, behavior: ipcidr, interval: 43200, format: text, proxy: 节点选择, } 然后就可以编写规则集，这里不一一列出琐碎的细节，用广告的例子来具体讲一下怎么写，读者应能举一反三\n广告规则集 rule-providers: reject_non_ip_no_drop: \u0026lt;\u0026lt;: *RuleSet_classical url: https://ruleset.skk.moe/Clash/non_ip/reject-no-drop.txt path: ./rule_set/sukkaw_ruleset/reject_non_ip_no_drop.txt reject_non_ip_drop: \u0026lt;\u0026lt;: *RuleSet_classical url: https://ruleset.skk.moe/Clash/non_ip/reject-drop.txt path: ./rule_set/sukkaw_ruleset/reject_non_ip_drop.txt reject_non_ip: \u0026lt;\u0026lt;: *RuleSet_classical url: https://ruleset.skk.moe/Clash/non_ip/reject.txt path: ./rule_set/sukkaw_ruleset/reject_non_ip.txt reject_domainset: \u0026lt;\u0026lt;: *RuleSet_domain url: https://ruleset.skk.moe/Clash/domainset/reject.txt path: ./rule_set/sukkaw_ruleset/reject_domainset.txt reject_extra_domainset: \u0026lt;\u0026lt;: *RuleSet_domain url: https://ruleset.skk.moe/Clash/domainset/reject_extra.txt path: ./sukkaw_ruleset/reject_domainset_extra.txt reject_ip: \u0026lt;\u0026lt;: *RuleSet_classical url: https://ruleset.skk.moe/Clash/ip/reject.txt path: ./rule_set/sukkaw_ruleset/reject_ip.txt 上面的几个规则集一共有两个方面，域名和 IP 来拦截广告，我们配置分流规则务必按照域名和 IP 规则分开，而且是 IP 规则在下，从而避免不必要的 DNS 解析。下面的规则中有一个是 REJECT-DROP，跟直接 REJECT 不同的是该策略收到请求后，直接静默直到超时为止，适合那些被拒绝就立马再发起请求的情况\n分流规则示例 # 分流规则 rules: # 非 IP 类规则必须与 IP 类规则分开，避免不必要的 DNS 解析 # 先是拒绝类，优先级最高，引自 Sukka，包含 uBlock Origin、AdGuard、EasyList 等 - RULE-SET,reject_non_ip,REJECT - RULE-SET,reject_domainset,REJECT - RULE-SET,reject_extra_domainset,REJECT # 可能开启后对性能有影响 - RULE-SET,reject_non_ip_drop,REJECT-DROP - RULE-SET,reject_non_ip_no_drop,REJECT ... # 这里是另外其他的域名规则 # IP 类规则 - RULE-SET,reject_ip,REJECT # 兜底规则 - MATCH,节点选择 这样就写好了一个分流规则，同时分流规则的最后应该是兜底规则，MATCH 的意思是无论什么都会去匹配\n再补充一下常见的几种域名规则，方便读者自定义：\nrules: - DOMAIN,ad.com,REJECT # 完整的域名 - DOMAIN-SUFFIX,google.com,auto # 域名以 google.com 结尾 - DOMAIN-KEYWORD,google,auto # 域名包含关键词 google - DOMAIN-REGEX,^abc.*com,PROXY # 利用正则来匹配，以 abc 开头，包含 com 接着讲讲另一种规则集的常用写法，就是从上文说到的 geodata 来进行配置，也就是从 geosite 和 geoip 来匹配某种特殊用途，前者是域名，后者是 IP。这里屏蔽了 Windows 的一些脏东西，geosite 合并了 crazy-max 的规则\nrules: # 引自 crazy-max 的规则，WIN-SPY 和 WIN-EXTRA 是 Windows 的附加的隐私跟踪域名 # WIN-UPDATE 是 Windows 的自动更新域名 - GEOSITE,WIN-SPY,REJECT - GEOSITE,WIN-EXTRA,REJECT - GEOSITE,WIN-UPDATE,REJECT 但同时也要注意，当你相关规则多了之后，可能会有误杀的情况，比如如上的三条规则就导致你登不上 outlook，所以可以通过代理软件的 log 查看有哪些是必须的，然后救回来即可，不要盲目抄规则\nrule: # 引自 crazy-max 的规则，WIN-SPY 和 WIN-EXTRA 是 Windows 的附加的隐私跟踪域名 # WIN-UPDATE 是 Windows 的自动更新域名 # 有些域名可能被误杀，可以通过 log 来进行补救 - DOMAIN-REGEX,outlook.office.*.com,DIRECT - DOMAIN-SUFFIX,login.microsoftonline.com,DIRECT - GEOSITE,WIN-SPY,REJECT - GEOSITE,WIN-EXTRA,REJECT - GEOSITE,WIN-UPDATE,REJECT 那么如果你想从 geosite 中找到自己想要的类，比如 telegram 呢？可以本地下载 geosite.dat，然后直接记事本打开，然后去搜你想要找的类别，必须要大写搜索，小写会搜到很多域名部分，geosite 中类是大写的，比如 TELEGRAM。另外，有些类是比较杂，会以 \u0026ldquo;CATEGORY-\u0026rdquo; 开头，可以搜搜看是否有想要的\n至此你应该能够自己写配置了，接着讲一下我的配置中关于 Sukka 规则的取舍\n广告部分，我全部拿过来，并且启用 CDN 部分，我是专门定制了新的 CDN 节点策略组，专门用自建的节点或者低倍率节点，因为 CDN 通常流量较大 对于苹果和微软可以直连的，直接 DIRECT；不能的则分别建立苹果和微软策略组 对于流媒体部分，我并没有做过多区分，因为我也不咋用，所以就直接放到节点选择了 关于 AIGC的，我专门建立了策略组，默认是美国的节点，港澳台用不了 global 的就直接走节点选择，domestic 就走 DIRECT 另外一点就是移动端如何运用覆写配置呢？我是将电脑端正在运行的配置保存为文件，然后手机导入文件即可\n巧用配置 终于来到了配置的应用环节， 这里分享一些日常可以用到的配置，方便读者使用\n内网开梯子打不开 很多读者发现开着梯子访问公司内网或者校园网打不开，当你读完了上文就会明白，因为当你开了系统代理之后，代理软件接管了 DNS 解析的过程，也就是说代理软件会向常用的域名服务器去查找你的公司或者校园网的域名，这当然是找不到的，内网的域名在公网肯定是解析不到的。那么就可以用到我们之前说的 nameserver-policy 来做到，当我们匹配到特定域名，就让内网的域名服务器来解析，而不是用公网的\n那么如何找到内网的 nameserver 呢？找到 WiFi，然后点开属性，找到以下的即可：\n然后配置好就可以开着系统代理正常访问内网了，下面说的是任何以 xxx.edu.cn 结尾的都可以走我们预设的 DNS 服务器\ndns: nameserver-policy: \u0026#34;+.xxx.edu.cn\u0026#34;: 10.20.220.50 解锁流媒体 不少机场不同的节点对不同流媒体的解锁情况不一样，那么就可以将可以解锁的代理节点放在一个策略组内，然后将所有的流媒体的分流规则都导到这个策略组即可~\n想要直连 tracker 有些读者可能像我一样日常生活中会用到一些 public 或是 private 的 tracker，有些服务器可能是国外，但是 tracker 一般都能直连，我们不想要代理去连接 tracker，就可以利用分流规则\n幸好先前引入的 geosite 中有一部分是融合了TrackersList以及blackmatrix7 / ios_rule_script，包含了public 和 private 的 tracker\nrules: - GEOSITE,CATEGORY-PUBLIC-TRACKER,DIRECT # 公共 tracker - GEOSITE,CATEGORY-PT,DIRECT # 私有 tracker 使用学校学术资源 有些时候开着代理软件会不可避免的用代理节点来访问学术网站，导致没办法用到学校购买的资源，下载不了论文等，此时我们就可以针对学术网站来制定一个策略组，节点配置就只有一个直连，这样就可以使得即使代理开着，也能用到学校资源\n下列域名规则取自于 hexiao，另外注意，该域名规则应该加在 global 之前，因为优先级是从上而下的，如果一开始匹配到 global，就直接结束了\nrules: # Academic - DOMAIN,tuchong.com,DIRECT - DOMAIN-SUFFIX,taylorandfrancis.com,DIRECT - DOMAIN,dl.acm.org,DIRECT - DOMAIN,acm-prod.disqus.com,DIRECT - DOMAIN-SUFFIX,sciencedirectassets.com,DIRECT - DOMAIN-SUFFIX,readspeaker.com,DIRECT - DOMAIN-SUFFIX,webofknowledge.com,DIRECT - DOMAIN-KEYWORD,pubmed,DIRECT - DOMAIN-KEYWORD,springer,DIRECT - DOMAIN-KEYWORD,ieee,DIRECT - DOMAIN-KEYWORD,elsevier,DIRECT - DOMAIN-KEYWORD,sciencedirect,DIRECT - DOMAIN-KEYWORD,nature,DIRECT - DOMAIN-KEYWORD,tandfonline,DIRECT - DOMAIN-SUFFIX,elsevier.com,DIRECT - DOMAIN-SUFFIX,edu.cn,DIRECT - DOMAIN-SUFFIX,webofscience.com,DIRECT - DOMAIN-SUFFIX,tandfonline.com,DIRECT - DOMAIN-SUFFIX,link.springer.com,DIRECT - DOMAIN-SUFFIX,onlinelibrary.wiley.com,DIRECT - DOMAIN-SUFFIX,sciencedirect.com,DIRECT - DOMAIN-SUFFIX,taylorfrancis.com,DIRECT 开代理访问 IPv6 无论是电脑还是手机端，如果开了代理的情况下还想要访问纯 IPv6 的站点，需要开启 IPv6 相关的设置，一般来说会有总的 IPv6 开关以及 DNS 中的 IPv6 开关。这里有个误区就是你以为你的覆写文件中已经写过开启 IPv6 就可以了，实际上软件层面还要同时去打开设置\n电脑端的 Clash-verge-rev：设置 -\u0026gt; Clash 设置 -\u0026gt; IPv6 手机端的 FlClash：工具 -\u0026gt; 覆写 -\u0026gt; DNS -\u0026gt; 开启覆写 DNS 以及 IPv6；以及工具 -\u0026gt; 覆写 -\u0026gt; 基础 -\u0026gt; IPv6 开关 需要注意的是，当你用 FlClash 开启了 DNS 覆写，那么软件中的设置项就接管了覆写配置中提前设置好的信息，就需要你手动将覆写配置中的信息输入到软件里\n注意，有些情况如果域名服务器设置不正确，也会影响开代理加载 IPv6。尽量以域名而非 IP 来设置域名服务器！\n致谢 尽管上文均已提及本文所参考的来源，但有必要再一次列出以示感谢，排名不分先后~\nSukka 关于规则集的相关配置 yyhhyy 在 Mihomo party 的配置教程 Mihomo 官方文档 Mihomo 官方的规则 geodata 集合 Loyalsoldier 的 geodata 集合: geosite和 geoip VirgilClyne 关于 ASN 的介绍 crazy-max 提供屏蔽 Windows 隐私以及更新的域名 学术规则来源自 hexiao 的博客 策略组的图标来自于 Orz-3 geodata 中的 tracker 来自于 XIU2 以及 blackmatrix7 Sing-Box 软件 Clash Verge Rev FlClash ","permalink":"http://yunpengtai.top/posts/know-mihomo/","summary":"Mihomo_config 这里存放了我个人的 Mihomo 覆写文件，可以进行参考，请务必读懂本文后使用，不要盲目照抄 YAML 引子 代理，对于很多人来说并不陌生，尤其是在科研领域，例如在","title":"「网络代理」这件小事"},{"content":"引子 前段时间对博客进行了整理和翻新，趁着记忆还没完全模糊，将搭建博客的细节记录下来。个人而言，对目前博客的各项功能以及美观度还是比较满意的，而这些背后也付出了一定的努力，希望也对其他想折腾博客的人有些许帮助，那样就更好了！也作为博客分类「折腾」的第一篇，送给自己~\n注意：博客的主题是基于 Hugo 的 PaperMod 进行魔改，故而诸多相关细节未必适用于其他博客框架，请阅读理解后进行使用。另外，本博客相关源码也放在此，方便进行查阅\nMyPaperMod This is the demo of my improved PaperMod theme. You can visit the introduction: https://yunpengtai.top/posts/hello-world/ HTML 关于本文涉及的一些用法示例，可以参见本篇文章的源码\n基础知识 这里主要介绍 Hugo 主题相关的基础知识，比如文件夹代表的意思，我使用的 Hugo 版本以及常用命令等，有基础的读者应直接跳至下一节进行阅读\n组成 开始之前，先说明一个比较重要的事情，你会发现 layouts 和 assets 在 themes 下某个主题里也有，不要在主题里进行修改，否则当你主题更新后，就比较麻烦，正确做法应该是在 themes 同级目录创建\n以下即为一个 Hugo 主题常见的组成结构：\n|—— assets # 放置 css 和 js |—— css |—— js |—— content # 放置网站内容，比如 posts，或者 friends.md |—— posts |—— hello-world.md friends.md # 友链内容 |—— data # 我是只放了 SVG.toml 文件 |—— layouts # 控制网站相关的布局 |—— _default # 主题内置的布局，如 single.html 代表一个帖子的布局 |—— partials # 放置你个人魔改的部件 |—— extend_footer.html # 代码会添加到原网站的 footer 里 |—— extend_head.html # 代码会添加到原网站的 head 里 |—— shortcodes # 一些好玩的 shortcodes 命令 |—— public # 渲染后的纯 HTML 代码，刚下载未渲染不会有该目录 |—— resources # 一般不用管 |—— static # 网站的字体以及 icon 放置目录 |—— fonts favicon.ico |—— themes |—— PaperMod config.yaml # 网站的配置文件 content 目录下的路径关系，即为网站上的链接顺序，比如 hello-world.md 访问链接便为：域名/posts/hello-world/，而 friends.md 便为 域名/friends BTW，如果想在文章中引用博客内容，可以省去域名[hello-world](/posts/hello-world)\n另外，Hugo 新手可能不知道 shortcodes 是啥意思，可以理解为一种快捷指令，具体的意思也可去 Hugo 官网 查看\nconfig.yaml 里面可以放置全局参数以及 menu 等信息，根据你所使用的主题文档进行修改即可。这里举个简单的例子，假如你希望你的网站遵循浏览器的亮暗偏好来加载，在 PaperMod 里就可以这样设置：\nparams: ... # 其他参数 defaultTheme: auto 调试和发布 我的 Hugo 一直是使用老版本：\nhugo version # hugo v0.117.0-b2f0696cad918fb61420a6aff173eb36662b406e linux/amd64 BuildDate=2023-08-07T12:49:48Z VendorInfo=gohugoio Hugo 常用的也就两个命令：\nhugo server # 进行本地调试 hugo # 正式渲染，结果在 public，将 public 上传至支持静态站的地方即可发布 另外关于 Hugo 中常见的变量以及支持的方法，也应该去官网查看更详细的记录，还有时常在 Hugo HTML 中出现的，类似如下内容是属于「Go」的模板函数，不只是 Go，如果有任何疑问，查看官网 doc 永远是第一选择\n{{ if or .Params.math .Site.Params.math }} ... {{ end }} 有了如上的基础知识后，我们可以来愉快的进行精装修了\n数学公式 $a=b$ 这是行内公式\n这是行间公式 $$ e=mc^2 $$\n对于写技术 blog 的同志们来说，数学公式的适配几乎是必须的，然而很多主题未对数学公式进行适配，或者只是比较随意的适配（有些情况还是不会 work），我使用的这个主题作者就在 GitHub issues 里贴了所谓的实现方案，内容如下：\n{{ if or .Params.math .Site.Params.math }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} 看似基本的公式可以正常显示，然而，当公式复杂起来，上述方案就不会 work 了，比如：\n$$ \\frac{\\partial E(\\boldsymbol{w})}{\\partial z_j} = \\sum\\limits_{k}\\frac{\\partial E(\\boldsymbol{w})}{\\partial y_{k}}\\frac{\\partial y_k}{\\partial z_{j}}= \\sum\\limits_{k} (y_{k}- \\hat{y}_{k}) w_{kj}^{(2)} \\tag{5.11} $$ 有了可以渲染数学元素的工具还不够，因为你还需要保证公式的内容并没有进行修改。举个例子，当你想要渲染 a_{1} + b_{1}，而 Hugo 可能将下划线渲染成 markdown 的模式：a\u0026lt;em\u0026gt;1 + b\u0026lt;/em\u0026gt;1，导致 mathjax 去渲染的时候找不到你原来的公式，进而导致了渲染的失败。解决方案是将带有公式的部分先用代码 block 装饰起来，避免内容被修改，接着再将代码 block 去掉，完整的送给渲染工具 借鉴了谢益辉的相关实现\n在 layouts/partials/extend_footer.html 中复制以下 js 代码：\n去掉数学公式的代码框 js \u0026lt;script\u0026gt; (function () { var i, text, code, codes = document.getElementsByTagName(\u0026#34;code\u0026#34;); for (i = 0; i \u0026lt; codes.length; ) { code = codes[i]; if (code.parentNode.tagName !== \u0026#34;PRE\u0026#34; \u0026amp;\u0026amp; code.childElementCount === 0) { text = code.textContent; if (/^\\$[^$]/.test(text) \u0026amp;\u0026amp; /[^$]\\$$/.test(text)) { text = text.replace(/^\\$/, \u0026#34;\\\\(\u0026#34;).replace(/\\$$/, \u0026#34;\\\\)\u0026#34;); code.textContent = text; } if ( /^\\\\\\((.|\\s)+\\\\\\)$/.test(text) || /^\\\\\\[(.|\\s)+\\\\\\]$/.test(text) || /^\\$(.|\\s)+\\$$/.test(text) || /^\\\\begin\\{([^}]+)\\}(.|\\s)+\\\\end\\{[^}]+\\}$/.test(text) ) { code.outerHTML = code.innerHTML; // remove \u0026lt;code\u0026gt;\u0026lt;/code\u0026gt; continue; } } i++; } })(); \u0026lt;/script\u0026gt; 接着在 layouts/partials/extend_head.html 中引用 layouts/partials/mathjax.html：由全局和网站变量来共同决定是否进行数学公式渲染 上述文件若是不存在则自己创建\n{{ if or .Params.math .Site.Params.math }} {{- partial \u0026#34;mathjax.html\u0026#34; .}} {{ end }} mathjax.html 的内容如下，当然我这里还加了额外的 boldsymbol 包，没有需求的可以去掉\nmathjax.html \u0026lt;script\u0026gt; MathJax = { loader: { load: [\u0026#34;[tex]/boldsymbol\u0026#34;] }, tex: { inlineMath: [ [\u0026#34;`$\u0026#34;, \u0026#34;$`\u0026#34;], [\u0026#34;\\\\(\u0026#34;, \u0026#34;\\\\)\u0026#34;], ], displayMath: [ [\u0026#34;`$$`\u0026#34;, \u0026#34;`$$`\u0026#34;], [\u0026#34;\\\\[\u0026#34;, \u0026#34;\\\\]\u0026#34;], ], processEscapes: true, processEnvironments: true, tags: \u0026#34;all\u0026#34;, packages: { \u0026#34;[+]\u0026#34;: [\u0026#34;boldsymbol\u0026#34;] }, }, }; \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js\u0026#34; integrity=\u0026#34;sha256-kbAFUDxdHwlYv01zraGjvjNZayxKtdoiJ38bDTFJtaQ=\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; 至此，数学公式的渲染问题就解决好了，使用时把原本的公式放入 code block 即可：\n`$a=b$` 行内公式 以下是行间公式 `$$ e=mc^2 $$` 评论系统 评论系统用过很多，比如用 GitHub 来驱动，或者自部署类比如 Waline，之前一直是用 Waline，有表情包，也支持邮箱通知等。但是，技术 blog 的评论系统怎么能没有「公式的集成」呢？所以我选择了Artalk，支持公式，表情，邮箱通知，自动亮暗模式等，功能很全，作者也在积极更新~\n概念上 Artalk 分为前后端以及存储所需要的数据库，先讲前端的配置，创建 layouts/partials/artalk.html，内容如下，因为我需要用到 katex，故而引入了其 css 和 js 相关的文件，读者应根据自己需要进行取舍\n最重要的是根据浏览器的偏好以及读者对网站的偏好来设置亮暗，首先第一次加载时网站和 Artalk 会按照浏览器的偏好来加载。接着，如果读者点击了网站的「亮暗切换按钮」，那么以后的加载就遵循网站的亮暗偏好，而非浏览器的偏好。实现的逻辑大概就是通过一些 element 来获取当前的偏好，不同主题的 element id 势必会有些许不同，故而不可照抄~\nartalk.html \u0026lt;!-- Artalk Doc 默认是 unpkg 的 CDN，尽量不要用，国内连通性不好 --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; /\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/artalk@2.8.6/dist/Artalk.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@artalk/plugin-katex@0.2.4/dist/artalk-plugin-katex.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;div id=\u0026#34;Comments\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; const savedTheme = localStorage.getItem(\u0026#34;pref-theme\u0026#34;); let darkMode = \u0026#34;auto\u0026#34;; // 查看网站是否已经设置了亮暗偏好 if (savedTheme !== null) { darkMode = savedTheme === \u0026#34;dark\u0026#34; ? true : false; } const artalk = Artalk.init({ el: \u0026#34;#Comments\u0026#34;, // 绑定元素的 Selector pageKey: \u0026#34;\u0026#34;, pageTitle: \u0026#34;{{ .Title }}\u0026#34;, server: \u0026#34;{{ site.Params.artalk.server }}\u0026#34;, // 后端地址 site: \u0026#34;{{ site.Params.artalk.site }}\u0026#34;, // 你的站点名 darkMode: darkMode, // 首次打开时自动亮暗模式 versionCheck: false, // 不提醒需要更新，还需要后端也设置，后端 \u0026gt; 前端 }); document.getElementById(\u0026#34;theme-toggle\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { document.body.className.includes(\u0026#34;dark\u0026#34;) ? artalk.setDarkMode(!1) : artalk.setDarkMode(!0); }); \u0026lt;/script\u0026gt; 另外，关于停用版本检测，不仅要设置前端的参数，后端的参数也需要设置为 false，具体即为 render 上的 conf.yaml\n接着在同级目录中创建 comments.html 来引用即可：\n{{ if or .Params.comments .Site.Params.comments }} {{- partial \u0026#34;artalk.html\u0026#34; .}} {{ end }} 关于数据库以及后端部署的部署，很大程度得益于这位网友的帖子，重复内容就不多赘述，我选择的是 Neon+Render 来分别部署数据库和后端，关于 Render 部署的详细文件，可以参考这里。这里为表情包多做一条说明，如果是希望网站支持表情包，则在 conf.yaml 中的 emoticons 项中加入表情包的远程链接\nemoticons: link_to_artalk.json 关于此文件的具体格式，可以参考我的表情包配置仓库，至此，评论系统集成也已经完毕。当然，本网站的 artalk 看上去可能与你们的有些许不同，比如表情包的大小以及没有头像，我自己又改了一些 CSS 来完成上述目标，创建 assets/css/extended/artalk.css，将 artalk 对应的 CSS 内容复制进去，然后修改为以下内容，如果没搜到对应的 CSS 项，就直接新建即可（关于字体的导入，在字体设置）\nArtalk css /* 改变表情包大小 */ img[atk-emoticon] { width: 5em; height: auto; display: initial; } /* 下列内容可能存在于原来的 CSS 文件内，请查找后进行修改 */ .atk-comment \u0026gt; .atk-avatar img { width: 50px; height: 50px; border-radius: 3px; display: none; /* 移除头像 */ } /* artalk 代码相关的字体与正文对齐 */ .artalk code, .atk-layer-wrap code { font-family: \u0026#34;Consolas\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;; margin: 0 0.05em; padding: 0 0.4em; display: inline-block; vertical-align: middle; font-size: 0.9em; background-color: var(--at-color-bg-grey); color: var(--at-color-font); border-radius: 2px; } .artalk pre code *, .atk-layer-wrap pre code * { font-family: \u0026#34;Consolas\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;; } 至于字体方面，等后面讲网站字体介绍即可，并未加其他特殊设置。关于邮箱通知，你需要去专门进行相关设置，请根据官方文档进行，比较简单，这里进行省略\n更好看 这一节主要讲为了博客变得更好看做的改变\n字体 我对博客的字体向来是比较挑剔，而且这很影响读者的观感，我比较喜欢霞鹜文楷，这款字体好看而且是开源的。不过该字体可不小，对于个人的轻量级 blog 来说，还是存在着优化的可能性，故而，我在博客上使用的是 woff2 格式文件，大小只有 2 M，直接放到了 static/fonts 目录，有需要可以去我的仓库里下载。同时英文使用 Apple 的字体 SF Pro Text Regular 来渲染，然后用 CSS 来控制字体加载即可：\n字体 CSS 设置 @font-face { font-family: \u0026#34;LXGWWenKaiScreenR\u0026#34;; src: url(\u0026#34;/fonts/lxgwwenkaiscreen.subset.v1.235.standard.woff2\u0026#34;); } /* https://www.webfontfree.com/cn/download/SFProText-Regular */ @font-face { font-family: \u0026#34;SFProText-Regular\u0026#34;; src: url(\u0026#34;/fonts/SFProText-Regular.woff2\u0026#34;); } body { font-family: \u0026#34;SFProText-Regular\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;; font-size: 16px; line-height: 1.6; word-break: break-word; background: var(--theme); font-display: swap; } 代码渲染 代码渲染的主题其实也是见仁见智吧，我个人是选择了atom-one-dark/light\n这里提供一下修改方法，因为我们需要覆盖掉原先主题对于代码渲染的设置，所以在 assets/css/hljs 中创建 an-old-hope.min.css ，注意，必须为该名字，当然如果是 Hugo PaperMod 主题肯定另有不同，然后直接复制相关的 CSS 进去。\n这里唯一需要注意的是亮暗模式的设置，对于亮色模式，直接拷贝 light 即可，但对于暗色来说，则需要一些特定的限定：\nbody.dark { .hljs { color: #abb2bf; background: #282c34; } ... } 同时，需要修改亮暗模式下代码框的背景颜色，这里是直接用变量来进行替代。然后关于代码的字体设置，笔者使用 Consolas 和霞鹜文楷（注释的中文字体）。为了防止有些读者并没有 Consolas 字体，这里保险起见还是下载下来。另外注意，因为我之前的 CSS 中加载了霞鹜文楷，如果你没有加载，还是需要多写一个 font-face\n代码 CSS 设置 @font-face { font-family: \u0026#34;Consolas\u0026#34;; src: url(\u0026#34;/fonts/Consolas.woff2\u0026#34;); } code { font-family: \u0026#34;Consolas\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;; } .post-content code { margin: auto 4px; padding: 4px 6px; font-size: 0.8em; line-height: 1.5; background: var(--code-bg); } .post-content pre code { display: block; margin: auto 0; padding: 10px; background: var(--hljs-bg) !important; color: var(--content); border-radius: var(--radius); overflow-x: auto; word-break: break-all; font-family: \u0026#34;Consolas\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;; font-size: 15px; } 亮暗相关的两个变量进行设置的地方在 assets/css/core/theme-vars.css 这里：\ntheme-vars.css /* 省略的内容请拷贝原先主题对应的文件 */ :root { ... --hljs-bg: #f7f7f7; --code-bg: rgb(245, 245, 245); } .dark { ... --hljs-bg: rgb(46, 46, 51); --code-bg: rgb(55, 56, 62); } 表格 对主题自带的表格渲染也进行修改，同时适配亮暗模式，还是需要注意暗色模式下的设置\n姓名 年龄 职业 张三 30 工程师 李四 25 设计师 王五 35 医生 表格 CSS table { border-collapse: collapse; display: table; margin-bottom: 1rem; width: 100%; overflow-x: auto; -webkit-overflow-scrolling: touch; \u0026amp; thead th { vertical-align: bottom; border-bottom: 2px solid #dee2e6; } \u0026amp; td, \u0026amp; th { vertical-align: top; border-top: 1px solid #dee2e6; border-bottom: 1px solid #dee2e6; } \u0026amp; tbody tr:hover { background-color: rgba(0, 0, 0, 0.075); } \u0026amp; tbody tr:nth-of-type(2n + 1) { background-color: rgba(0, 0, 0, 0.05); } \u0026amp; tr:last-of-type { vertical-align: bottom; border-bottom: 2px solid #dee2e6; } } .dark table { border-collapse: collapse; display: table; margin-bottom: 1rem; width: 100%; overflow-x: auto; -webkit-overflow-scrolling: touch; \u0026amp; thead th { vertical-align: bottom; border-bottom: 2px solid var(--code-bg); } \u0026amp; td, \u0026amp; th { vertical-align: top; border-top: 1px solid var(--code-bg); border-bottom: 1px solid var(--code-bg); } \u0026amp; tbody tr:hover { background-color: var(--code-bg); } \u0026amp; tbody tr:nth-of-type(2n + 1) { background-color: var(--code-bg); } \u0026amp; tr:last-of-type { vertical-align: bottom; border-bottom: 2px solid var(--code-bg); } } svg icon 觉得主页上的 icon 不太好看也可以进行修改，修改的方法就是将原来主题中的 layouts/partials/svg.html 拷贝到我们的 partials 目录，然后就选择自己想要的 icon 进行修改，这样就会覆盖了\n更便于阅读 这一节主要讲博客为了更方便读者阅读做出的努力\nMermaid 图 flowchart TB Start[客户端发起请求] --\u0026gt; fake[fake-ip 反查] fake[fake-ip 反查] --\u0026gt; Domain[基于域名匹配规则] fake --\u0026gt; |fakeip-filter|system[系统解析 DNS] Domain --\u0026gt; |匹配过程中|IP[遇到 IP 规则] Domain --\u0026gt; reject[匹配到 Reject 规则] Domain --\u0026gt; |匹配到直连规则|Cache IP --\u0026gt; Cache Domain --\u0026gt; |匹配到代理规则|Remote[通过代理服务器解析域名并建立连接] Cache --\u0026gt; |Cache 未命中|NS[匹配 nameserver-policy 并查询 ] Cache --\u0026gt; |Cache 命中|Get NS --\u0026gt; |匹配成功| Get[将查询到的 IP 用于匹配 IP 规则] NS --\u0026gt; |没匹配到| NF[nameserver/fallback 并发查询] NF --\u0026gt; Get[查询得到 IP] Get --\u0026gt; |缓存 DNS 结果|Cache[(查询 DNS 缓存)] Mermaid js 可以可以让我们用代码的方式画流程图（如上图），在文章的概念比较多或者关系复杂时，流程图就可以让读者更容易看懂，故而也引入了 mermaid 的实现\n首先创建 layouts/_default/_markup/render-codeblock-mermaid.html，写入以下内容：\n\u0026lt;!-- 因为正常写会有 ```meraid ... ``` --\u0026gt; \u0026lt;pre class=\u0026#34;mermaid\u0026#34;\u0026gt; {{- .Inner | htmlEscape | safeHTML }} \u0026lt;/pre\u0026gt; {{ .Page.Store.Set \u0026#34;hasMermaid\u0026#34; true }} 这样就可以将 mermaid 这种特殊的 codeblock 加入渲染机制里，同时设置 hasMermaid 为 true，方便后面判断是否加载 mermaid js。接着我们创建 layouts/partials/mermaid.html，来让 mermaid js 对我们写的代码进行渲染\n同时支持亮暗自动切换，大部分代码片段取自于 mermaid-js社区的讨论，然而默认的代码是初次渲染是查看 localStorage 是否包含 pref-theme，很多时候用户并未手动点击切换是不会有这个值，即为 null。我这里是判断 document.body.className 是否包含 dark 来判断，更为准确\nmermaid 的字体设置依然是对齐正文，使用mermaid.init()设置即可：\nmermaid.html 文件 {{ if .Page.Store.Get \u0026#34;hasMermaid\u0026#34; }} \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const elementCode = \u0026#34;.mermaid\u0026#34;; const loadMermaid = function (theme) { mermaid.initialize({ theme }); mermaid.init({ theme, themeVariables: { // 这里设置字体跟正文一致 fontFamily: [\u0026#34;SFProText-Regular\u0026#34;, \u0026#34;LXGWWenKaiScreenR\u0026#34;] }}, document.querySelectorAll(elementCode)); }; const saveOriginalData = function () { return new Promise((resolve, reject) =\u0026gt; { try { var els = document.querySelectorAll(elementCode), count = els.length; els.forEach((element) =\u0026gt; { element.setAttribute(\u0026#34;data-original-code\u0026#34;, element.innerHTML); count--; if (count == 0) { resolve(); } }); } catch (error) { reject(error); } }); }; const resetProcessed = function () { return new Promise((resolve, reject) =\u0026gt; { try { var els = document.querySelectorAll(elementCode), count = els.length; els.forEach((element) =\u0026gt; { if (element.getAttribute(\u0026#34;data-original-code\u0026#34;) != null) { element.removeAttribute(\u0026#34;data-processed\u0026#34;); element.innerHTML = element.getAttribute(\u0026#34;data-original-code\u0026#34;); } count--; if (count == 0) { resolve(); } }); } catch (error) { reject(error); } }); }; saveOriginalData().catch(console.error); // 不要用 localStorage.getItem(\u0026#34;pref-theme\u0026#34;)，因为有些时候会为 null let isdark = document.body.className.includes(\u0026#34;dark\u0026#34;); if (isdark) { resetProcessed().then(loadMermaid(\u0026#34;dark\u0026#34;)).catch(console.error); } else { resetProcessed().then(loadMermaid(\u0026#34;neutral\u0026#34;)).catch(console.error); } document.getElementById(\u0026#34;theme-toggle\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { resetProcessed(); document.body.className.includes(\u0026#34;dark\u0026#34;) ? loadMermaid(\u0026#34;neutral\u0026#34;) : loadMermaid(\u0026#34;dark\u0026#34;).catch(console.error); }); \u0026lt;/script\u0026gt; {{ end }} 最后再在 layouts/_default/single.html 中加入引用 mermaid.html 的部分，注意，single.html 你如果一开始没有，需要先去主题 themes/PaperMod/layouts/_default/single.html 那里拷贝原来的 single.html 到上面这个地址\n\u0026lt;article\u0026gt; \u0026lt;!-- 省略上面的 --\u0026gt; {{- partial \u0026#34;mermaid.html\u0026#34; . }} \u0026lt;/article\u0026gt; 侧边悬浮目录 悬浮目录是比较重要的，原始的目录是固定在文章的顶部，这样不便于读者阅读时对目录有直观的把握，也不利于读者进行快速跳转。\n在 assets/css/extended/toc.css 中添加代码即可 借鉴了 sulvblog 的实现\n悬浮目录 CSS :root { --article-width: 650px; --toc-width: 230px; } .toc { margin: 0 2px 40px 2px; border: 1px solid var(--border); background: var(--entry); border-radius: var(--radius); padding: 0.4em; } .toc-container.wide { position: absolute; height: 100%; border-right: 1px solid var(--border); left: calc((var(--toc-width) * 0.9 + var(--gap)) * -1); top: calc(var(--gap) * 2); width: var(--toc-width); } .wide .toc { position: sticky; top: var(--gap); border: unset; background: unset; border-radius: unset; width: 100%; margin: 0 2px 40px 2px; } .toc details summary { cursor: zoom-in; margin-inline-start: 20px; padding: 12px 0; } .toc details[open] summary { font-weight: 500; } .toc-container.wide .toc .inner { margin: 0; } .toc .active { font-size: 110%; font-weight: 600; color: #614a85; text-decoration: underline; } .toc ul { list-style-type: circle; } .toc .inner { margin: 0 0 0 20px; padding: 0px 15px 15px 20px; font-size: 16px; /*目录显示高度*/ max-height: 83vh; overflow-y: auto; } .toc .inner::-webkit-scrollbar-thumb { /*滚动条*/ background: var(--border); border: 7px solid var(--theme); border-radius: var(--radius); } .toc li ul { margin-inline-start: calc(var(--gap) * 0.5); list-style-type: none; } .toc li { list-style: none; font-size: 0.95rem; padding-bottom: 5px; } .toc li a:hover { color: var(--secondary); } 添加修改时间 原先的主题并没有显示「修改时间」的功能，这对于读者阅读起来并不是好事情，像我阅读时就会关注文章最后一次的修改日期，否则可能会过时\n在 layouts/partials/post_meta.html 中加入以下内容即可：\n{{- if (.Param \u0026#34;ShowLastMod\u0026#34;) -}} {{ if ne (.Lastmod.Format \u0026#34;2006-01-02\u0026#34;) (.Date.Format \u0026#34;2006-01-02\u0026#34;) }} {{- `$scratch.Add \u0026#34;meta\u0026#34; (slice (printf \u0026#34;Updated:\u0026amp;nbsp;%s\u0026#34; (.Lastmod.Format (.Site.Params.dateFormat | default \u0026#34;January 2, 2006\u0026#34;)))) }}$` {{- end -}} {{- end -}} 然后在具体的帖子里加入 lastmod 和 showLastMod 即可显示出修改的时间了\n--- title: 新的主题 date: 2022-06-19 11:10:00 +0800 lastmod: 2024-11-20 18:00:00 +0800 showLastMod: true ... --- MarginNote 这是示例 这是示例的侧边注解\n很多时候想补充说明，或者引用某些内容时，常见的脚注就必须跳转到文章末尾进行阅读，然后读者还需要跳回来，这就十分不方便，而 MarginNote 则并不会有这些问题，借由 MarginNote， 就可以在文章的侧边来显示信息，读者阅读起来也会方便很多 借鉴了 kennethfriedman 和 scripter\n首先来说相关的 CSS，主题的实现也是通过 CSS 来进行实现，以及 sidenote number 的增减，不过有些时候会有 bug，故而，为了方便起见，我索性将 number 变为固定的「#」，显示起来也比较美观\nMarginNote CSS .sidenote { float: right; position: relative; margin-right: -18vw; width: 40%; max-width: 200px; } body { counter-reset: sidenote-counter; } .sidenote-number { counter-increment: sidenote-counter; } .sidenote::before { content: \u0026#34;# \u0026#34;; position: relative; font-size: 0.9em; font-weight: 700; color: red; } .sidenote-number::after { content: \u0026#34;#\u0026#34;; vertical-align: super; font-size: 0.8em; font-weight: 700; color: #409dff; } .sidenote-number:hover .sidenote { background-color: var(--sidenote-bg); } 光有 CSS 还不够，我们上面定义的这些特殊的 HTML 元素，都得创建才行，在 layouts/shortcodes/sidenote.html 中写入以下内容\n\u0026lt;span class=\u0026#34;sidenote-number\u0026#34;\u0026gt;\u0026lt;small class=\u0026#34;sidenote\u0026#34;\u0026gt;{{ .Inner | markdownify }}\u0026lt;/small\u0026gt;\u0026lt;/span\u0026gt; .Inner 就代表是输入的内容，而 | markdownify 是为了支持 markdown 渲染，比如超链接等语法\n图片点击放大 Black Holes: Monsters in Space\n当图片细节很多或图片很大时，放在博客上就难免会进行大比例缩放，此时读者若是不能放大查看该图片，想必会十分苦恼，故而图片点击放大的功能也是必不可少的，具体是通过引入 fancybox 来实现：\n在 layouts/shortcodes/figure.html 中填入以下内容即可\nfigure.html \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css\u0026#34; /\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;a data-fancybox=\u0026#34;gallery\u0026#34; href=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}\u0026#34;\u0026gt; \u0026lt;figure{{ if or (.Get \u0026#34;class\u0026#34;) (eq (.Get \u0026#34;align\u0026#34;) \u0026#34;center\u0026#34;) }} class=\u0026#34; {{- if eq (.Get \u0026#34;align\u0026#34;) \u0026#34;center\u0026#34; }}align-center {{ end }} {{- with .Get \u0026#34;class\u0026#34; }}{{ . }}{{- end }}\u0026#34; {{- end -}}\u0026gt; {{- if .Get \u0026#34;link\u0026#34; -}} \u0026lt;a href=\u0026#34;{{ .Get \u0026#34;link\u0026#34; }}\u0026#34;{{ with .Get \u0026#34;target\u0026#34; }} target=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ with .Get \u0026#34;rel\u0026#34; }} rel=\u0026#34;{{ . }}\u0026#34;{{ end }}\u0026gt; {{- end }} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}{{- if eq (.Get \u0026#34;align\u0026#34;) \u0026#34;center\u0026#34; }}#center{{- end }}\u0026#34; {{- if or (.Get \u0026#34;alt\u0026#34;) (.Get \u0026#34;caption\u0026#34;) }} alt=\u0026#34;{{ with .Get \u0026#34;alt\u0026#34; }}{{ . }}{{ else }}{{ .Get \u0026#34;caption\u0026#34; | markdownify| plainify }}{{ end }}\u0026#34; {{- end -}} {{- with .Get \u0026#34;width\u0026#34; }} width=\u0026#34;{{ . }}\u0026#34;{{ end -}} {{- with .Get \u0026#34;height\u0026#34; }} height=\u0026#34;{{ . }}\u0026#34;{{ end -}} /\u0026gt; \u0026lt;!-- Closing img tag --\u0026gt; {{- if .Get \u0026#34;link\u0026#34; }}\u0026lt;/a\u0026gt;{{ end -}} {{- if or (or (.Get \u0026#34;title\u0026#34;) (.Get \u0026#34;caption\u0026#34;)) (.Get \u0026#34;attr\u0026#34;) -}} \u0026lt;figcaption\u0026gt; {{ with (.Get \u0026#34;title\u0026#34;) -}} {{ . }} {{- end -}} {{- if or (.Get \u0026#34;caption\u0026#34;) (.Get \u0026#34;attr\u0026#34;) -}}\u0026lt;p\u0026gt; {{- .Get \u0026#34;caption\u0026#34; | markdownify -}} {{- with .Get \u0026#34;attrlink\u0026#34; }} \u0026lt;a href=\u0026#34;{{ . }}\u0026#34;\u0026gt; {{- end -}} {{- .Get \u0026#34;attr\u0026#34; | markdownify -}} {{- if .Get \u0026#34;attrlink\u0026#34; }}\u0026lt;/a\u0026gt;{{ end }}\u0026lt;/p\u0026gt; {{- end }} \u0026lt;/figcaption\u0026gt; {{- end }} \u0026lt;/figure\u0026gt; \u0026lt;/a\u0026gt; 当然上面还不止是增加了图片点击放大的功能，还加了 align，caption 等功能\n盘古之白 个人认为，中文和英文以及数字之间有空格会更加便于阅读，这个空格也被称为「盘古之白」，像是劈开了这几者之间的混沌， 实现起来也十分简便：\n在 layouts/partials/extend_footer.html 中加入以下内容：\n盘古之白配置 {{- `$highlight := resources.Get \u0026#34;js/pangu.min.js\u0026#34; }}$` \u0026lt;script\u0026gt; (function (u, c) { var d = document, t = \u0026#34;script\u0026#34;, o = d.createElement(t), s = d.getElementsByTagName(t)[0]; o.src = u; if (c) { o.addEventListener(\u0026#34;load\u0026#34;, function (e) { c(e); }); } s.parentNode.insertBefore(o, s); })(\u0026#34;{{ $highlight.RelPermalink }}\u0026#34;, function () { pangu.spacingPage(); }); \u0026lt;/script\u0026gt; 我这里是将 pangu.min.js 下载到了网站本地，具体是在 assets/js/pangu.min.js，当网站加载时，盘古之白会自动进行渲染\n代码折叠 示例代码 print(\u0026#39;Acc: 100%\u0026#39;) 对于代码比较多的 blog，比如这篇，很多代码较长会影响阅读，占用很多篇幅，如果可以折叠，需要的时候点击展开就会很方便，折叠功能实现起来也比较方便，在 layouts/shortcodes/collapse.html 中加入以下内容：\n{{ if .Get \u0026#34;summary\u0026#34; }} {{ else }} {{ warnf \u0026#34;missing value for param \u0026#39;summary\u0026#39;: %s\u0026#34; .Position }} {{ end }} \u0026lt;p\u0026gt;\u0026lt;details {{ if (eq (.Get \u0026#34;openByDefault\u0026#34;) true) }} open=true {{ end }}\u0026gt; \u0026lt;summary markdown=\u0026#34;span\u0026#34;\u0026gt;{{ .Get \u0026#34;summary\u0026#34; | markdownify }}\u0026lt;/summary\u0026gt; {{ .Inner | markdownify }} \u0026lt;/details\u0026gt;\u0026lt;/p\u0026gt; 版权声明 也许你也需要修改文章末尾的版权说明，那么就在 layouts/partials/post_copyright.html 中加入自己的版权声明即可\n文章分类 有时候我们想要发表截然不同类型的文章，比如我主要会发深度学习以及 AI 相关的，但我也会写「折腾」相关的，就需要有不同的分类\n先在 config.yaml 中加入以下内容：\nparams: taxonomies: category: categories tag: tags 然后在写文章的时候，可以编辑文章的元信息来进行分类，同时还可以不在主页显示\n--- title: 新的主题 date: 2022-06-19 11:10:00 +0800 categories: [折腾] hiddenInHomeList: true ... --- 但是不在主页显示后读者就不容易找到，此时我们在 config.yaml 中额外加个 menu 即可：\nmenu: main: ... - identifier: categories name: 折腾 url: /categories/折腾 weight: 20 接着再寻找到 layouts/_default/single.html 中的 post-meta 类，在里面加入代码来让文章的元信息栏（显示时间，有多少词的地方）显示分类\n修改 single.html 加入分类信息 \u0026lt;div class=\u0026#34;post-meta\u0026#34;\u0026gt; {{- partial \u0026#34;post_meta.html\u0026#34; . -}} {{- partial \u0026#34;translation_list.html\u0026#34; . -}} {{- partial \u0026#34;edit_post.html\u0026#34; . -}} {{- partial \u0026#34;post_canonical.html\u0026#34; . -}} \u0026lt;!-- 在元数据中显示分类信息 --\u0026gt; {{- $categories := .Language.Params.Taxonomies.category | default \u0026#34;categories\u0026#34;}} \u0026lt;!-- 统计分类个数 --\u0026gt; {{- $cnt := 0 }} {{- range ($.GetTerms $categories) }} {{- $cnt = add $cnt 1 }} {{- end }} \u0026lt;!-- 只有文章有分类信息时才显示 --\u0026gt; {{- if gt $cnt 0 }} {{- $i := 0 }} \u0026lt;div class=\u0026#34;meta-item\u0026#34;\u0026gt;\u0026amp;nbsp·\u0026amp;nbsp {{- range ($.GetTerms $categories) }} \u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .LinkTitle }}\u0026lt;/a\u0026gt; {{ $i = add $i 1 }} \u0026lt;!-- 不是最后一个类别时，添加逗号分割类别 --\u0026gt; {{- if lt $i $cnt}} \u0026lt;span\u0026gt;,\u0026lt;/span\u0026gt; {{- end }} {{- end }} \u0026lt;/div\u0026gt; {{- end }} \u0026lt;/div\u0026gt; {{- end }} Shortcodes 大赏 这里分享一些我比较常用的shortcode，也欢迎评论区分享你觉得有趣的\n旋转的友链 Aaron Swartz 用以缅怀自由斗士 Aaron 当鼠标悬浮至友链所对应的头像时，头像便会开始进行旋转，这个还挺有趣的，首先在 assets/css/extended/friends.css 中加入以下内容： 借鉴了 sulvblog\n旋转友链的 CSS 设置 .friendurl { text-decoration: none !important; color: black; box-shadow: none !important; } .myfriend { width: 56px !important; height: 56px !important; border-radius: 50% !important; padding: 2px; margin-top: 20px !important; margin-left: 14px !important; background-color: #fff; } .frienddiv { overflow: auto; height: 100px; width: 49%; display: inline-block !important; border-radius: 5px; background: none; -webkit-transition: all ease-out 0.3s; -moz-transition: all ease-out 0.3s; -o-transition: all ease-out 0.3s; transition: all ease-out 0.3s; } .dark .frienddiv:hover { background: var(--code-bg); } .frienddiv:hover { background: var(--theme); transition: transform 1s; webkit-transform: scale(1.1); -moz-transform: scale(1.2); -ms-transform: scale(1.2); -o-transform: scale(1.2); transform: scale(1.1); } .frienddiv:hover .frienddivleft img { transition: 0.9s !important; -webkit-transition: 0.9s !important; -moz-transition: 0.9s !important; -o-transition: 0.9s !important; -ms-transition: 0.9s !important; transform: rotate(360deg) !important; -webkit-transform: rotate(360deg) !important; -moz-transform: rotate(360deg) !important; -o-transform: rotate(360deg) !important; -ms-transform: rotate(360deg) !important; } .frienddivleft { width: 92px; float: left; margin-right: -5px; } .frienddivright { margin-top: 18px; margin-right: 18px; } .friendname { text-overflow: ellipsis; font-size: 100%; margin-bottom: 5px; color: var(--primary); } .friendinfo { text-overflow: ellipsis; font-size: 70%; color: var(--primary); } @media screen and (max-width: 600px) { .friendinfo { display: none; } .frienddivleft { width: 84px; margin: auto; } .frienddivright { height: 100%; margin: auto; display: flex; align-items: center; justify-content: center; } .friendname { font-size: 18px; } } 接着在 layouts/shortcodes/friend.html 中加入以下内容：\nfriend.html 内容 {{- if .IsNamedParams -}} \u0026lt;a target=\u0026#34;_blank\u0026#34; href={{ .Get \u0026#34;url\u0026#34; }} title={{ .Get \u0026#34;name\u0026#34; }} class=\u0026#34;friendurl\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;frienddiv\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;frienddivleft\u0026#34;\u0026gt; \u0026lt;img class=\u0026#34;myfriend\u0026#34; src={{ .Get \u0026#34;logo\u0026#34; }} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;frienddivright\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;friendname\u0026#34;\u0026gt;{{- .Get \u0026#34;name\u0026#34; -}}\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;friendinfo\u0026#34;\u0026gt;{{- .Get \u0026#34;word\u0026#34; -}}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; {{- end }} Blockquote Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell\n这个主题原来的 blockquote 较为丑，在 layouts/shortcodes/quote.html 加入以下内容 借鉴了 Guan Qirui\nBlockquote html 设置 \u0026lt;blockquote class=\u0026#34;quote{{ range .Params }} {{ . }}{{ end }}\u0026#34;\u0026gt; {{- `$content := .Inner | markdownify -}}$` {{- if not (strings.HasPrefix `$content \u0026#34;\u0026lt;p\u0026gt;\u0026#34;) }}$` {{ printf `\u0026lt;p\u0026gt;%s\u0026lt;/p\u0026gt;` `$content | safeHTML }}$` {{- else }} {{- `$content }}$` {{- end -}} \u0026lt;/blockquote\u0026gt; 然后创建 assets/css/extended/quote.css 并加入以下内容\nBlockquote 的 css 配置 blockquote.quote { position: relative; margin: 1em auto; padding-left: 3em; border: none; } blockquote.quote::before { position: absolute; left: 0; content: \u0026#34;“\u0026#34;; font-size: 3em; font-weight: bold; line-height: 1; } blockquote.quote-copyright { position: relative; margin: 2em auto; padding-left: 3em; border: none; background-color: aliceblue; } blockquote.quote-copyright::before { position: absolute; left: 0; content: \u0026#34;“\u0026#34;; font-size: 3em; font-weight: bold; line-height: 1; } Github 小卡片 Github 仓库小卡片对我来说还是比较重要的，因为我一般代码都会进行开源，如果可以用一种卡片的方式提醒读者开源代码所在处，就省了读者去查找的功夫了，首先在 assets/css/extended/github.css 中填入以下内容：\nGithub 卡片相关 CSS 设置 .github { border: 0px solid; border-radius: 5px; width: 95%; margin-bottom: 1em; margin-top: 1em; padding: 1em; background-color: var(--code-bg); .github_bar { margin-top: -0.6em; margin-left: 0; } .github_name { font-weight: bold; text-decoration: none; font-size: 24px; position: relative; top: -0.6em; left: 0.3em; } .github_description { margin-top: -0.3em; margin-bottom: 1em; color: var(--color-contrast-high); text-align: justify; font-size: 90%; width: 95%; transition: all 0.5s; } .github_language { margin-top: -0.6em; } .github_language_name { color: var(--color-contrast-high); font-size: 90%; margin-left: 0.5em; transition: all 0.5s; } } 接着，在 layouts/shortcodes/github.html 中加入以下内容：\ngithub.html 内容 \u0026lt;div class=\u0026#34;github\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;github_bar\u0026#34;\u0026gt; {{ replace `$.Site.Data.SVG.repository \u0026#34;icon\u0026#34; \u0026#34;icon github-icon\u0026#34; | safeHTML }}$` \u0026lt;a class=\u0026#34;github_name\u0026#34; href={{ .Get \u0026#34;link\u0026#34; }} target=\u0026#34;_blank\u0026#34;\u0026gt;{{ .Get \u0026#34;name\u0026#34; }}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;github_description\u0026#34;\u0026gt;{{ .Get \u0026#34;description\u0026#34; }}\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;github_language\u0026#34;\u0026gt; {{ .Get \u0026#34;language\u0026#34; }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 最后在 data 中添加 SVG.toml 并加入以下内容\nrepository = \u0026#39;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;50\u0026#34; height=\u0026#34;50\u0026#34; viewBox=\u0026#34;0 0 50 50\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39; 各种 notice 一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n在 layouts/shortcodes 中创建 notice.html，然后复制以下内容： 借鉴了 Hugo-notice\nnotice.html 内容 {{- `$noticeType := .Get 0 -}}$` {{- `$raw := (markdownify .Inner | chomp) -}}$` {{- `$block := findRE \u0026#34;(?is)^\u0026lt;(?:address|article|aside|blockquote|canvas|dd|div|dl|dt|fieldset|figcaption|figure|footer|form|h(?:1|2|3|4|5|6)|header|hgroup|hr|li|main|nav|noscript|ol|output|p|pre|section|table|tfoot|ul|video)\\\\b\u0026#34; $`raw 1 -}} {{ `$icon := (replace (index site.Data.SVG $`noticeType) \u0026#34;icon\u0026#34; \u0026#34;icon notice-icon\u0026#34;) }} \u0026lt;div class=\u0026#34;notice {{ `$noticeType }}\u0026#34; {{ if len .Params | eq 2 }} id=\u0026#34;{{ .Get 1 }}\u0026#34; {{ end }}\u0026gt;$` \u0026lt;div class=\u0026#34;notice-title\u0026#34;\u0026gt;{{ `$icon | safeHTML }}\u0026lt;/div\u0026gt;$` {{- if or `$block (not $`raw) }}{{ `$raw }}{{ else }}\u0026lt;p\u0026gt;{{ $`raw }}\u0026lt;/p\u0026gt;{{ end -}} \u0026lt;/div\u0026gt; 接着在 assets/extended 中创建 notice.css 并填入以下内容：\nnotice.css 内容 .notice { display: flex; align-items: center; position: relative; padding: 0.6em; margin-bottom: 1em; border-radius: 4px; p:last-child { margin-bottom: 0; } .notice-title { margin-right: 0.5em; margin-top: 0.5em; .notice-icon { width: 1.2em; height: 1.2em; } } \u0026amp;.notice-warning { background: hsla(0, 65%, 65%, 0.15); .notice-title { color: hsl(0, 65%, 65%); } } \u0026amp;.notice-info { background: hsla(30, 80%, 70%, 0.15); .notice-title { color: hsl(30, 80%, 70%); } } \u0026amp;.notice-note { background: hsla(200, 65%, 65%, 0.15); .notice-title { color: hsl(200, 65%, 65%); } } \u0026amp;.notice-tip { background: hsla(140, 65%, 65%, 0.15); .notice-title { color: hsl(140, 65%, 65%); } } } 最后在 data/SVG.toml 中加入以下内容：\nnotice-warning = \u0026#39;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon\u0026#34; viewBox=\u0026#34;0 0 576 512\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M570 440c18 32-5 72-42 72H48c-37 0-60-40-42-72L246 24c19-32 65-32 84 0l240 416zm-282-86a46 46 0 100 92 46 46 0 000-92zm-44-165l8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39; notice-info = \u0026#39;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon\u0026#34; viewBox=\u0026#34;0 0 512 512\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M256 8a248 248 0 100 496 248 248 0 000-496zm0 110a42 42 0 110 84 42 42 0 010-84zm56 254c0 7-5 12-12 12h-88c-7 0-12-5-12-12v-24c0-7 5-12 12-12h12v-64h-12c-7 0-12-5-12-12v-24c0-7 5-12 12-12h64c7 0 12 5 12 12v100h12c7 0 12 5 12 12v24z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39; notice-note = \u0026#39;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon\u0026#34; viewBox=\u0026#34;0 0 512 512\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M504 256a248 248 0 11-496 0 248 248 0 01496 0zm-248 50a46 46 0 100 92 46 46 0 000-92zm-44-165l8 136c0 6 5 11 12 11h48c7 0 12-5 12-11l8-136c0-7-5-13-12-13h-64c-7 0-12 6-12 13z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39; notice-tip = \u0026#39;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;icon\u0026#34; viewBox=\u0026#34;0 0 512 512\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M504 256a248 248 0 11-496 0 248 248 0 01496 0zM227 387l184-184c7-6 7-16 0-22l-22-23c-7-6-17-6-23 0L216 308l-70-70c-6-6-16-6-23 0l-22 23c-7 6-7 16 0 22l104 104c6 7 16 7 22 0z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39; ","permalink":"http://yunpengtai.top/posts/hugo-journey/","summary":"引子 前段时间对博客进行了整理和翻新，趁着记忆还没完全模糊，将搭建博客的细节记录下来。个人而言，对目前博客的各项功能以及美观度还是比较满意的，","title":"Hugo PaperMod 主题精装修"},{"content":"prompt 在人与 LLM 的互动中起着关键的作用，好的 prompt 可以让 LLM「思考」更多一些，那么如何更好地理解 prompt 的组成，以及设计 prompt 来完成想要的任务便成了主要的目标\n注意，这里讨论的 prompt 是指系统 prompt（system prompt）\n系统 prompt 和对话 prompt 最大不同的地方在于，系统 prompt 是你希望 LLM 一直记住的，每次对话 LLM 会阅读系统 prompt；而对话 prompt 的作用域更多强调单轮以及少量多轮对话（前提是 LLM 还能记住的情况下）\n单位 「单位」指的即为 prompt 中比较重要的组成单位，先理解 prompt 的重要单位才能更好地去构建 prompt，这里介绍一下由 Government Technology Agency of Singapore (GovTech)设计的 CO-STAR 框架，依照1的介绍，一共分为几个部分：\n背景信息（Context），即解决目标任务所需的前置信息 目标（Objective），即希望 LLM 最终为你做的事情 风格（Style），即 LLM 生成回复文本的风格，比如某个心理分析专家 语气（Tone），即 LLM 回应时所具备的情感，比如幽默 受众（Audience），即 LLM 生成回复时的受众 回应（Response），即对 LLM 生成回应的限制，比如格式上的 这些单位是 prompt 基本的组成结构，能正确的设定有助于帮助 LLM 更好地理解 prompt\n举个例子，当你想要 LLM 为你解释一些深度学习有关于 NLP 的名词时\n背景信息：你是一个深度学习，尤其是自然语言处理方面的专家。自然语言处理（Natural Language Processing）指的是机器能和人一样对语言进行一定程度的理解和应用 目标：你需要对输入的自然语言处理相关名词进行准确的解释 语气和风格：其实差不多意思，可以是「解释时需要具体，最好能结合相关示例进行阐释。同时解释时最好清晰易懂，就像对七岁小孩解释那样」 受众：因为我们之前已经说到了「七岁小孩」，就不用额外指定受众了 回应的格式：无 用以对比的 prompt 即为只有目标的 prompt，以下是对比，如果对于一个不了解编程和 AI 的人而言，第一种的解释比第二种要容易理解\n构建 完成了对 prompt 单位的理解，算是初步完成了对 prompt 的构建，然而，对于比较复杂或者人为要求较多的任务而言，这样的 prompt 还不够\n结构化 第一个好用的技巧是「结构化」你的 prompt，LLM 对于 markdown 形式有比较好的理解，可以使用 markdown 相关语法来进行格式化，以我们之前构建的 prompt 来举例，结构化之后即为：\n## Context 你是一个深度学习，尤其是自然语言处理方面的专家。自然语言处理（Natural Language Processing）指的是机器能和人一样对语言进行一定程度的理解和应用。 ## Objective 你需要对输入的自然语言处理相关名词进行准确的解释 ## Style 解释时需要具体，最好能结合相关示例进行阐释。同时解释时最好清晰易懂，就像对七岁小孩解释那样。 可以看到效果比之前还要好上一点：\n结构化可以更好地帮助 LLM 来捕捉到重点，这一点其实跟人类的沟通无异，一连串的文本和结构分明的文本，势必后者更容易理解\n变量 当你需要让 LLM 在聊天时对它的输出进行处理时，比如在一次回答中进行「反思」等操作，抑或是 LLM 需要多步骤处理你的任务\n那么，像写代码一样设计变量是很不错的技巧\n举个例子，翻译分为直译和意译，意译就相当于在直译的基础上进行反思，首先按照「单位」和「结构化」的技巧，构建出初始的 prompt：\n## Context 翻译指的是将源语言翻译至目标语言的过程，你需要处理的文本大多来自科学技术领域，包括但不限于「软件」，「深度学习」等 ## Objective 你需要将输入的英文翻译成连贯的中文 ## Style 翻译的过程中需要尽可能忠于原文，同时在不违背原文的前提下进行更连贯的转换 ## Tone 简洁，明了 ## Response 将翻译的结果用 **markdown code block** 进行包裹，即 ```text \u0026lt;\u0026lt;\u0026lt;翻译结果\u0026gt;\u0026gt;\u0026gt; ``` 接着我们给 prompt 加入意译反思的部分，这里主要是利用 Process 关键词进行完成\n## Context 翻译指的是将源语言翻译至目标语言的过程，你需要处理的文本大多来自科学技术领域，包括但不限于「软件」，「深度学习」等 ## Objective 你需要将输入的英文翻译成连贯的中文 ## Style 翻译的过程中需要尽可能忠于原文，同时在不违背原文的前提下进行更连贯的转换 ## Tone 简洁，明了 ## Process 1. 首先，你需要依靠直觉和常识对源文本进行直译，结果为 \u0026lt;\u0026lt;\u0026lt;original_result\u0026gt;\u0026gt;\u0026gt; 2. 接着，你需要对 \u0026lt;\u0026lt;\u0026lt;original_result\u0026gt;\u0026gt;\u0026gt; 进行意译，在理解的基础上对直译结果进行修改，变为 \u0026lt;\u0026lt;\u0026lt;final_result\u0026gt;\u0026gt;\u0026gt;，以下是需要注意的地方： - 对于语法，语序等表达习惯不符合中文的予以修改 - 让文本整体更具备连贯性，考虑整个段落的意思 - 用有序列表来分点记录你修改的具体原因，即为 \u0026lt;\u0026lt;\u0026lt;modify_reason\u0026gt;\u0026gt;\u0026gt; ## Response 将所有翻译的结果用 **markdown code block** 进行包裹，即 ```text \u0026lt;\u0026lt;\u0026lt;original_result\u0026gt;\u0026gt;\u0026gt; ```，```text \u0026lt;\u0026lt;\u0026lt;final_result\u0026gt;\u0026gt;\u0026gt; ``` 另外，你对直译结果进行修改的具体原因也需要输出，即 ```text \u0026lt;\u0026lt;\u0026lt;modify_reason\u0026gt;\u0026gt;\u0026gt; ``` 对比上面两个翻译结果，可以很明显的感知到翻译的差距，模型和数据固然重要，如何释放出模型具备的能力也同样重要，就像同一个运动员面对不同教练的嘱托时可能发挥不同，有的教练让运动员不必太在乎输赢，可能运动员心态好一些；而有些教练可能就比较喜欢争输赢，若是运动员心理素质一般，可能发挥就会差\n变量的好处是在问题具有多个步骤的时候，让 LLM 不会迷失方向，更好地解决复杂性的问题。这里设计变量用的是 \u0026laquo;\u0026lt;变量名\u0026raquo;\u0026gt;，连续的特殊分隔符更容易让 LLM 记住。同时，因为 LLM 更多接触的是英文的变量名，变量名最好采用「英文」\n示例 对于更为复杂的任务，光靠一些指示性的话语往往不太容易做到，此时则需要添加「相关示例」来帮助 LLM 理解任务，省略的 prompt 就是上面我们构建的那种\n... [省略的 prompt] ## Examples \u0026lt;examples\u0026gt; \u0026lt;example1\u0026gt; \u0026lt;question1\u0026gt; 示例问题1 \u0026lt;/question1\u0026gt; \u0026lt;answer1\u0026gt; 示例回答1 \u0026lt;/answer1\u0026gt; \u0026lt;classes1\u0026gt; 示例标签1 \u0026lt;/classes1\u0026gt; \u0026lt;/example1\u0026gt; \u0026lt;example2\u0026gt; \u0026lt;question2\u0026gt; 示例问题2 \u0026lt;/question2\u0026gt; \u0026lt;answer2\u0026gt; 示例回答2 \u0026lt;/answer2\u0026gt; \u0026lt;classes2\u0026gt; 示例标签2 \u0026lt;/classes2\u0026gt; \u0026lt;/example2\u0026gt; \u0026lt;example3\u0026gt; ... \u0026lt;/example3\u0026gt; \u0026lt;/examples\u0026gt; 对于示例的样式，最好采用 XML 格式进行书写，好处是这种格式 LLM 在训练时已见过许多，并且 XML tag 就具有比较好的语义信息，能更容易让 LLM 捕捉到 prompt 中的结构信息，比如上面的就能清洗捕捉到问题，回答以及可能标签。Anthropic 同时还提到在长上下文窗口的情况下用 XML 能获得更好的结果2\n在让 LLM 理解论文时，我们怕它乱想，所以如果能够让它输出思考结果的同时，输出参考的原文依据就更好了，考虑到上下文长度以及直接阅读网上文献的功能，这里选用 Claude-3-Opus，这样的任务是比较复杂的，纯靠 prompt 无法让模型有比较好的感知，因而这里便需要示例来进一步阐述任务信息，这里的示例是用「雅思阅读题」\n## Context 往往研究人员之间的沟通交流能迸发出新的创意，目前你是自然语言处理方面的研究者，我和你正在合作解决一系列问题。自然语言处理（Natural Language Processing）指的是机器能和人一样对语言进行一定程度的理解和应用。 ## Objective 你需要阅读我提供的 PDF 论文文件，然后回答我的问题，或者提出你的假设和疑问 ## Style 你思考时必须严格参照原文，不能回答与原文不一致的答案 ## Tone 严谨 ## Process 1. 首先，你记住阅读论文的内容，记为 \u0026lt;\u0026lt;\u0026lt;paper_read\u0026gt;\u0026gt;\u0026gt; 2. 接着，你需要结合 \u0026lt;\u0026lt;\u0026lt;paper_read\u0026gt;\u0026gt;\u0026gt; 来回答我的问题，你的答案记为 \u0026lt;\u0026lt;\u0026lt;initial_answer\u0026gt;\u0026gt;\u0026gt;，你参考的部分原文记为 \u0026lt;\u0026lt;\u0026lt;references\u0026gt;\u0026gt;\u0026gt;。 3. 最后，你需要对照 \u0026lt;\u0026lt;\u0026lt;references\u0026gt;\u0026gt;\u0026gt;，来修改 \u0026lt;\u0026lt;\u0026lt;initial_answer\u0026gt;\u0026gt;\u0026gt; 中不符合的地方，记为 \u0026lt;\u0026lt;\u0026lt;final_answer\u0026gt;\u0026gt;\u0026gt; 4. 在上述过程中，有些必须注意的地方： - 参考的原文必须与原论文中的部分完全一致，不能进行任何修改和思考 - 答案若涉及到多个要点，请分点给出答案 - 如果原文中没有能够回答我的问题的，就直接返回 \u0026lt;\u0026lt;\u0026lt;Not Given\u0026gt;\u0026gt;\u0026gt; ## Response 将回答用 markdown block 进行包裹，即 ```text \u0026lt;\u0026lt;\u0026lt;initial_answer\u0026gt;\u0026gt;\u0026gt; ```，```text \u0026lt;\u0026lt;\u0026lt;final_answer\u0026gt;\u0026gt;\u0026gt; ``` 另外，对于参考原文的地方，也需要进行输出，即 ```text \u0026lt;\u0026lt;\u0026lt;references\u0026gt;\u0026gt;\u0026gt; ``` ## Examples \u0026lt;Examples\u0026gt; \u0026lt;Paper\u0026gt; B First, implicit theories of intelligence drive the way in which people perceive and evaluate their own intelligence and that of others. To better understand the judgments people make about their own and others\u0026#39; abilities, it is useful to learn about people\u0026#39;s implicit theories. For example, parents\u0026#39; implicit theories of their children\u0026#39;s language development will determine at what ages they will be willing to make various corrections in their children\u0026#39;s speech. More generally, parents\u0026#39; implicit theories of intelligence will determine at what ages they believe their children are ready to perform various cognitive tasks. Job interviewers will make hiring decisions on the basis of their implicit theories of intelligence. People will decide who to be friends with on the basis of such theories. In sum, knowledge about implicit theories of intelligence is important because this knowledge is so often used by people to make judgments in the course of their everyday lives. C Second, the implicit theories of scientific investigators ultimately give rise to their explicit theories. Thus it is useful to find out what these implicit theories are. Implicit theories provide a framework that is useful in defining the general scope of a phenomenon - especially a not-well-understood phenomenon. These implicit theories can suggest what aspects of the phenomenon have been more or less attended to in previous investigations. I The Jacksonian view is that all people are equal, not only as human beings but in terms of their competencies - that one person would serve as well as another in government or on a jury or in almost any position of responsibility. In this view of democracy, people are essentially intersubstitutable except for specialized skills, all of which can be learned. In this view, we do not need or want any institutions that might lead to favoring one group over another. J Implicit theories of intelligence and of the relationship of intelligence to society perhaps need to be considered more carefully than they have been because they often serve as underlying presuppositions for explicit theories and even experimental designs that are then taken as scientific contributions. Until scholars are able to discuss their implicit theories and thus their assumptions, they are likely to miss the point of what others are saying when discussing their explicit theories and their data. \u0026lt;/Paper\u0026gt; \u0026lt;Example1\u0026gt; \u0026lt;Question1\u0026gt; 慢语言发展会让父母失望吗？ \u0026lt;/Question1\u0026gt; \u0026lt;Response1\u0026gt; ```text Not Given ``` 参考如下： ```text B First, implicit theories of intelligence drive the way in which people perceive and evaluate their own intelligence and that of others. To better understand the judgments people make about their own and others\u0026#39; abilities, it is useful to learn about people\u0026#39;s implicit theories. For example, parents\u0026#39; implicit theories of their children\u0026#39;s language development will determine at what ages they will be willing to make various corrections in their children\u0026#39;s speech. More generally, parents\u0026#39; implicit theories of intelligence will determine at what ages they believe their children are ready to perform various cognitive tasks. Job interviewers will make hiring decisions on the basis of their implicit theories of intelligence. People will decide who to be friends with on the basis of such theories. In sum, knowledge about implicit theories of intelligence is important because this knowledge is so often used by people to make judgments in the course of their everyday lives. ``` \u0026lt;/Response1\u0026gt; \u0026lt;/Example1\u0026gt; \u0026lt;Example2\u0026gt; \u0026lt;Question2\u0026gt; 为什么有些观点人为人人平等 \u0026lt;/Question2\u0026gt; \u0026lt;Response2\u0026gt; 初步答案如下： ```text 因为大家同为人，所以从民主的角度上说，大家应该人人平等。 ``` 最终答案如下： ```text 1. 因为大家同为人，从民主角度而言，大家应该人人平等 2. 每个人的能力也是可以通过学习来进行获得，因而能力也是一样的 ``` 参考如下： ```text I The Jacksonian view is that all people are equal, not only as human beings but in terms of their competencies - that one person would serve as well as another in government or on a jury or in almost any position of responsibility. In this view of democracy, people are essentially intersubstitutable except for specialized skills, all of which can be learned. In this view, we do not need or want any institutions that might lead to favoring one group over another. ``` \u0026lt;/Response2\u0026gt; \u0026lt;/Example2\u0026gt; \u0026lt;/Examples\u0026gt; 接着我们进行尝试：\n再试一个例子：\n我们来换个复杂点的问题，上 GPT-2 的论文，尽管尝试了两次，依然不是原文\n猜想可能跟论文的长度有关，利用工具将论文的正文全部摘抄，手动输入之后，再次尝试，这次参考的几乎是原文中出现过的，说明整个的 PDF 对模型来说还是比较困难\n我需要你仔细阅读论文并回答我的问题 \u0026lt;paper\u0026gt; ... \u0026lt;/paper\u0026gt; \u0026lt;question\u0026gt; 为什么 GPT-2 通过无监督训练 （unsupervised learning）这种方式可以让其性能变好？ \u0026lt;/question\u0026gt; 其他小技巧 详细化 首先介绍的几个技巧来自于 OpenAI 官方3，第一个便是在询问时使用更加具体的信息，更加丰富化整个过程，比较简单的做法就是，把 AI 当做一个你不认识的人，然后去阐述你的需求\n拆分复杂任务 其实这点重点讲的例子中也已经运用到，LLM 不擅长一口气解决复杂任务，你需要将复杂任务拆分成具体的可以一步一步执行的简单任务\n第二种即为 Anthropic 提到的 prompt chaining，这个的意思是当一个复杂任务哪怕在一次对话中拆分为多个小步骤仍然解决不了，那就可以将多个小步骤多次对话完成，有需要时便可以利用上一轮对话的输出\n利用外部工具 当任务特点比较明显，比如知识问答，或者回答代码运行结果时，就需要特定的外部工具来得到结果，在代码相关的问题上，调用代码解析器往往比普通对话效果要好\n监控思考过程 如果对 LLM 的输出不放心，或者需要进行检查，可以让 LLM 来输出思考过程，比如 Anthropic 提到的：\nBefore answering the question, please think about it step-by-step within \u0026lt;thinking\u0026gt;\u0026lt;/thinking\u0026gt; tags. Then, provide your final answer within \u0026lt;answer\u0026gt;\u0026lt;/answer\u0026gt; tags. 引用 https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.anthropic.com/en/docs/long-context-window-tips\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://platform.openai.com/docs/guides/prompt-engineering/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://yunpengtai.top/posts/better-prompt/","summary":"prompt 在人与 LLM 的互动中起着关键的作用，好的 prompt 可以让 LLM「思考」更多一些，那么如何更好地理解 prompt 的组成，以及设计 prompt 来完成想要的任务便成了主要的目标","title":"How to prompt LLM better?"},{"content":" 问题 LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇：\nLLM 目前在数学问题上取得的进展 有哪些技术路线在未来可能会更进一步提升 LLM 解决数学问题的能力？ 在以下部分不妨来讨论上面两个问题\n技术路线 那么在目前的模型中，分别有哪些方案呢？这里只会介绍各种路线关于数学的 key points，不会特别关注其他细节。另外 MathGPT 和 Abel 目前没有详细的 report，不会涉及\nInference Prompts LLM 直接输出答案经常会出错，第一条路线是通过 prompt 的不同方法来让模型不断「shift the distribution」，让模型不断调整输出分布，让结果更稳定和准确\nCoT 众所周知，CoT（Chain of Thought）是一种通过让模型一步步思考来提高模型推理能力的，下图右侧是 CoT\n下图横轴代表参数规模，当时通过 CoT 在 PaLM 上甚至超过了监督训练的 SOTA，CoT 这种方法就感觉像是「refine 模型输出的分布」，一步步思考也就意味着输出尽可能往正确的上面靠，相比之下，直接输出就像是「一锤定音」\nSelf-Verification 这篇工作就是进行反向验证，这篇工作看到还是比较开心的，刚好验证了自己之前的猜想\n举个例子来说，我们问模型一个问题，模型回答了两种答案：\n接下来我们将这个答案作为已知量，将题干中的变量当做未知，去反推，发现不一致，则说明答案有误\nFORBAR FORBAR 代指的是 Forward-Backward Reasoning，跟上面几乎一致，可以看个例子：\nForward 就是我们之前让模型直接算的，而 Backward 的意思就是我把题干中的一个变量给遮盖掉，变成 x，而我告诉你答案，我要反问你 x 是啥\n不同的是 Self-Verification 是用以验证，不能扩充成为 Examples，而 FORBAR 就可以作为 Examples\n效果是相当不错的：\nPoT Program of Thought 是不同于 CoT 的工作，出发点也很简单，比较复杂的数值推理任务，靠一步步推理，其实 LLM 还是会搞错\n那就有人想了，能不能不让 LLM 来计算，让 LLM 学会调用 Python 程序来搞呢？程序计算肯定比 LLM 自己来靠谱，这就是 PoT 干的事情：\n效果也是会比 CoT 好上一些，下图是 Few-Shot 的结果：\nMajority Voting 多数投票法（或称之为 self-consistency）是在 CoT 的基础上，对结果进行投票，在推理时，我们采样多个可能的解决方案，然后看最后那个预测结果出现最多，就选哪个\n结果也是比较显著的，比单独的 CoT 要好：\nWork Memory Galactica 提出了「工作记忆」的概念，先引入人类思考的习惯，当我让你求几个数字的平均值时，比如 43, 29, 51, 13，你会写一些过程在纸上，这种写出来的称之为 External Work Memory，即为外在工作记忆\n仔细看图中倒数两行的位置，有个 thinking，有些人算 136/4 是可以在脑中完成计算的，这种被称为 Internal Work Memory，而 Galatica 正是将两种记忆方式结合在一起解决问题。让我们看个 Work Memory 的例子：\n有人说，乍一看，这不就是 CoT 吗？一步步展开思考，没毛病吧，但你看 calculate 那里，这个过程是可以不需要 LLM 自己得出答案的，它可以只写个代码让计算机执行 Python 程序，而 LLM 做的是下达指令，读取输出即可\n具体而言，它跟 CoT 的区别或者说 CoT 的不足之处在于：\nCoT 相当于上面求平均值一直写在纸上的过程，但是有些 low-level 的思考人类是不需要写下来的，而 LLM 做不到这一点，这也是为什么 CoT 会产生看似正确但又模糊的答案，而 Work Memory 是将 internal 的思考交由更准确的工具去做，比如上述算力大小的例子，写程序，然后执行代码，最后 LLM 只需要读取就行了\n这个方法的难点在于数据集的准备，而 Galatica 是通过以下方法来创建：\n通过程序来控制一些变量，来生成例子（OneSmallStep） 从现成的数据集中拿（Workout, Khan Problems） 另一种就是直接用\u0026lt;work\u0026gt;这种模版转换（GSM8k train） mCoT 即代表 Majority Voting + CoT 一起的，可以看到 mCoT 还是效果优于工作记忆的，不过工作记忆并没有经过很好的养成，数据集既不大也不多样，后面可以优化的空间肯定更多\n需要注意的是，不能单纯的拿\u0026lt;work\u0026gt;和 mCoT 对比，因为上面 Galatica 的 prompt 数据集是预训练过的\nOPRO 这篇文章我也比较喜欢，目前 LLM 对于 prompt 不稳定，我猜测可能是不一致的原因：用人工优化的 prompt 来调整 LLM 内在输出分布，那很自然就有一个想法，能不能让 LLM 自己来优化使用的 prompt 呢？\n这就是 OPRO 做的事情，将 LLM 当做是优化器，不同于以往传统的优化任务，有各种公式做约束，OPRO 是用「自然语言描述」做约束，比如 Find the most effective prompt for this problem，具体框架如下：\n首先是用最开始 prompt，接着让负责优化 LLM 产生出多个 prompt，再根据目标用 负责打分 LLM 进行打分，将打分后的 prompt 和分数一起放进下一次优化中，最后输出分数最高的 prompt\n下图是 GMS8k 在优化 prompt 过程中的准确度，这里图可能有误解，其实没有 train，只是在优化过程中找到 prompt，然后在 eval set 进行评测准确率\nReward Models 第二条路线是改进 RLHF（Reinforcement Learning from Human Feedback）\nProcess Supervision We\u0026rsquo;ve trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”).\n目前的奖励模型大多基于「结果监督」，追求让模型产出正确的结果，而这样的弊端在于两点：\n模型中间的思考过程是未知的，解释性和可靠性不高 无法真正实现 alignment with humans，只是去对齐结果 In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.\n那么 OpenAI 的解决方案即引入「过程监督」，让模型每一步思考都和人类的进行对齐，这样就可以较好地解决以上的两个问题。作者还发现，尽管是过程监督，但只对比结果依然比结果监督要好\n同时 OpenAI 开源了过程监督的数据集 PRM800K，下图中 ORM 代表「Outcome Rewarded Model」，PRM 代表「Process Rewarded Model」\n当问题越复杂时，过程监督的优点就逐渐显露出来，结果好的同时更具解释性，可谓是一举两得\nReinforcement Learning from Evol-Instruct Feedback Reinforcement Learning from Evol-Instruct Feedback（RLEIF）是 WizardMath 提出的技术，设计比较巧妙，很 nice 的工作，一共分为三步：\n第一步：是先用指令微调获得模型 Wizard-E\n第二步：接着模仿 Evol-Instruct 利用 Wizard-E 和 ChatGPT 来扩充训练集，主要是分为两个方面，让问题变得更简单（Downward）和让问题更复杂（Upward），比如加更多限制，复杂化原来的问题。然后用 Wizard-E 进行排序，获得训练 Instruction Reward Model（IRM）的数据集\n再让 Wizard-E 模型来生成解决问题的步骤，用 ChatGPT 来评判每步的正确性，以此来生成训练 Process Supervision 的数据集\n第三步便是用 PPO 联合训练两个奖励模型，具体是将两个获得的奖励相乘，模型的效果也是比较惊艳的，不过在 MATH 数据集上还差点意思（可能是参数规模不够）\n数据入手 Galactica Galactica 用于预训练的数据量比同等大模型用的要小得多，整个数据集是经过清洗的「科学领域」数据集，包括论文，参考资料，蛋白质序列，化学公式等等，大小为仅有 106 billion tokens\n从上述表格可以看出，论文占了大头（83%），令人惊讶的是 CommonCrawl 仅仅占了 1%\n还将prompt数据集加入了预训练之中\n作者还提到 prompt prertraining 可以增强模型 general 的能力：\nMinerva 无独有偶，Minerva 同样收集了来自 arxiv 上的论文，有趣的是 General 的语料也是占比很少\n另外还收集了带有数学公式的网站：\n不一样的是利用方式，Minerva 采用的模型基座是 PaLM，然后在收集的数据上进行 Auto-regressive loss 微调\nRejection Sampling FineTuning 上面的 Galatica 和 Minvera 都是收集数据，是否可以让 LLM 生成数据呢？\nRFT（Rejection Sampling FineTuning）是阿里的工作，方法比较符合直觉，直接让微调后的模型来生成不同的例子用以扩充数据集，然后根据一些准则来筛选：\n答案不对的 计算结果和 Python 结果不一致的 可以看到效果比单纯的 SFT 要好上不少：\nAugGSM8K 还是那句话，我们可以用 LLM 来进行数据的扩充，问题是怎么扩充呢？这篇工作针对问题和回复分别进行了增强：\n关于问题，分别采用了五种增强策略（见下图左列），这些原则的设计还是比较符合直觉的\n对于回复，作者是采用不同的温度来生成多样化的推理路径，当然也会对于明显错误的进行过滤\n数据集概览如下：\n生成的数据集证明也是有效果的：\nMetaMath 这篇文章跟上面的 AugGSM8K 算是同期工作，也是利用 LLM 来对数据集进行增广，然后进行微调\n它主要是对问题进行增广，分为以下几种：\nRephrase Question Self-Verification FORBAR 效果的话基本和 AugGSM8K 也差不多，除了 70B 是 QLoRA，其他的参数是 LLaMA 的\nMAmmoTH 这篇工作是我数据分支线中很喜欢的一篇，其实 RFT 类通过 Bootstrap 来搞数据集的路子，会丧失「通用性」，在 GMS8K 上效果好，而在 MMLU-Math 上就有可能掉很多点；而 Galatica 和 Minvera 付出的代价要很多\n那么 MAmmoTH 就构建了一个数据集：MathInstruct：\n下图是其与其他数据集的对比：\n特点有三个：\n量大，一共有 200K 覆盖比较多的数学领域 Prompt 同时包括了 CoT 和 PoT 其实有些时候我觉得搞数据集是在水文章，但是有些时候是真的很有意义，比如 ImageNet，以及这一篇工作，方法再 fancy，数据缺少带来的问题是很难解决的\n效果也很顶，比 RFT 以及 Wizard 更具备通用性，是 Llama 就能有如此效果，相信换更大的模型肯定会有不错的提升\n架构 MathGLM 具备算数能力 这篇工作是相当有趣，它也解决了我的困惑，若 LLM 从一开始就学数值推理，能不能做好？同时，这篇工作给了一个全新的视角来做一个「专家模型」\n首先是其数据构造环节，很有启发性，无论是 CoT，还是工作区记忆，都强调需要把细节尽可能写下来，LLM 不能像人一样跳跃，这篇工作直接将推理的结果改成一步步得出\n接下来选用的主力 backbone 是 2B 的 Transformer-Decoder，你没有看错，这篇文章并没有使用 LLM Backbone，而是用 AutoRegressive loss 直接用上面数据集去训练\n下图的测试例子一共有 9592 条，直接碾压 GPT-4。当然，我认为这里的是裸模型，没加任何操作，如果用好的操作，GPT-4 应该可以做得更好。因为你拿一个垂域和一个通用模型比，至少也应该给 GPT-4 一些更好的 Prompt Method，或者一些上下文学习的例子\n不过这个实验结果证明了以下两点：\nDecoder-Only 架构的确可以学习到算数规则 参数的规模不需要那么大，2B 就能有很好的能力 通用一点的专家模型 为了让 MathGLM 可以解决文本描述的问题，MathGLM 还需要变得更通用，于是把目光放到了带有描述的数据集，同样的，将原来的解题步骤进行了细化，这样的好处是，既可以学习到数学知识，又可以建模文本，相当于比之前的专家模型更具通用性\n路在何方 那么理想化 LLM 能解决数学问题的标准是什么呢？\n题目的解答得正确 解题过程正确可解释 这两点是有可能做到的。综合以上的论文，我大胆预测一下未来的 Math LLM 可能的发展趋势，思路肯定是做一个较为通用的「专家模型」，具体怎么做呢？\n数据的收集和处理 数据的收集前面的工作可谓是百花齐放，核心思路就三个：\n尽可能人工去搞高质量的数学推理数据集，你像 LLM 在文本领域能成功，肯定离不开大量的高质量数据集 如果特别强领域特性的没有，就找更接近的，比如论文这种科学领域的语料 实在不行，就想办法让 LLM 来自我产生数据集（Bootstrap），但这种很依赖于模型，且会引入模型的内在 Bias，但不失为提升模型的手段 数据的处理这块看着比较简单，其实不然\n举个例子，MetaMath 就将答案改写成一步步过程，引入第一个数据原则：detailed，把 LLM 当成弱智，越详细越好\n第二个是 Minerva 的处理，它将公式单独处理成 LaTeX 中见到的样子，就相当于你用特殊的 token 来包裹公式，用某种方式来提示 LLM，第二个原则：保留不同于文本的模态特征\n专家模型的路线 这一点其实 MathGLM 和 Galatica 给了我不少启发，但目前有一个问题尚未解决：\n如果先预训练 Math LLM，后期去建模文本，究竟能保留多少通用性，或者反过来，如何保留数学的能力，说到底就是 how to be general and specific，如何衡量「通用性」和「专用型」，是值得考量的\n决定之后，其实就可以借鉴这两个模型的路子，比如将 Prompt 数据集直接放入预训练中，以及用 AutoRegressive loss 去建模数值推理的例子等\n预训练完毕，再利用 SFT 或是 RLHF 类方法去进一步微调\n参数的规模我觉得应该不会需要很大，相反，对于数据如何利用是值得考量的，正如在 Galatica 中提到的：作者们发现当重复训练时，性能也会稳步上升，作者将此归因于高质量的数据\n更强的推理方法 从简单的 Zero-Shot 再到 CoT，到 OPRO，经历了太多 Prompt 方法的变迁，我想未来应该很会有，但趋势应该是如何将 LLM 本身的知识引入其中来选择或构造 Prompt，这种一致性带来的提升会更稳定和持久\n最后一个问题 How to leverage LLM\u0026rsquo;s intrinsic ability to do reasoning?\n这也是我最近一直在想的问题，你看在数据那块，你会发现让模型生成一些例子再放进去推理会比直接推理要好一些，这都是模型自己的能力，有没有什么更优雅的方法可以将这种能力抽离出来\n换句话说，我们对 LLM 本身能力的压榨是不是还有上升的空间？\nReferences https://openai.com/research/improving-mathematical-reasoning-with-process-supervision https://galactica.org/static/paper.pdf https://arxiv.org/abs/2203.11171 https://arxiv.org/abs/2201.11903 https://arxiv.org/abs/2206.14858 http://arxiv.org/abs/2309.03409 https://arxiv.org/pdf/2308.09583.pdf https://arxiv.org/abs/2304.12244 http://arxiv.org/abs/2308.01825 https://arxiv.org/abs/2309.05653 https://arxiv.org/abs/2211.12588 http://arxiv.org/abs/2310.05506 http://arxiv.org/abs/2309.03241 https://arxiv.org/abs/2309.12284 https://arxiv.org/abs/2308.07758 https://arxiv.org/abs/2212.09561 ","permalink":"http://yunpengtai.top/posts/llm-math/","summary":"问题 LLM 通过大量的语料来建模下一个 token 的概率，这种训练方式促成 LLM 成为一个「文科生」，那么我们不禁对以下几个问题好奇： LLM 目前在数学问题上取得的进展","title":"大模型的数学之路"},{"content":"如何高效训练或推理大模型一般在两点：如何装得下以及如何更快\n这里讲一些主要的并行概念，不会深挖原理，只会介绍 key points，看它们分别为加速和适配显存做了什么贡献\nData Parallelism 对于常规的单机多卡用户而言，数据并行（DP）应该不陌生，在 PyTorch 中就有 DistributedDataParallel，主要原理就是将模型复制到每个 GPU 上，并将数据进行切分，然后在每个 GPU 上单独进行前向传播，接着在主进程上将输出聚集，将输出进行分发，在主进程上聚集梯度，然后进行更新，再将更新好的模型复制到其他 GPU 上\n当我们说 DP Degree 为 2 时，则需要两张 GPU\n得益于并行计算，相比于单卡速度会有不错的提升，需要注意的是，速度的增长很难是线性的，因为中间各种分发和聚集的过程会有通信的损失，这一点在高效训练中经常需要考虑\nData Parallel 过程\nZeRO Data Parallelism ZeRO 是DeepSpeed开发的，下面来简要讲讲：\n可以看到 Baseline 代表的就是上面原始的 DP，可以看到蓝色的代表模型参数，绿色代表的是优化器状态，剩下的是梯度\n$os, g, p$ 分别代表的是优化器状态，梯度以及参数。ZeRO 有三个阶段：$P_{os}, P_{os+g}, P_{os+g+p}$，$P$的意思是切分\nStage 1：我们只对优化器状态进行切分 Stage 2：同时对优化器状态和梯度进行切分 Stage 3：对优化器状态，梯度和参数进行切分\nDeepSpeed 三个阶段\n其实不用想的很复杂，跟原始的 DP 相比，你可以理解为只是原来在每张 GPU 上的优化器状态，梯度和参数变成了一部分而已\n同时，需要注意，ZeRO 需要在 FP16 进行计算\n因而，ZeRO 主要是为训练而生的，只有 Stage 3 和推理相关\nTensor Parallelism 张量并行就是原来一张卡上进行的张量操作在多张 GPU 上进行，可以看到原来的$\\mathbf{XA}$可以被拆分为以下过程：\nTensor Parallelism\n本质上就是将张量按照行或列进行拆分，分别计算，然后聚集得到结果，这样的好处是可以将很大的张量放到几个 GPU 上来适配显存\n由于张量计算是比较普遍的，因而若想进行 TP，对于设备间的通信要求比较高，当你有好几个节点的时候，不建议节点之间使用 TP，通信将是负累，在Bloom里作者说PCIe速度比节点间要快得多\nPipeline Parallelism 流水线并行的操作主要就是为了适配显存的，当你的显卡不足以放下模型的时候，你可以将模型的组成部分进行切分然后放到不同的 GPU 上\n=================== =================== | 0 | 1 | 2 | 3 | | 4 | 5 | 6 | 7 | =================== =================== GPU0 GPU1 比方说上图，模型一共有 8 层，我们将模型一分为二，放到不同的 GPU 上面，当我们涉及到那一层，就把相关的中间输出和数据转移到哪张卡上\n原始的 PP 效率问题很严重，当你在某张卡上进行运算时，其他卡都是空闲状态\n在下图中，上图是原始的 PP，从上往下分别代表四个 GPU，F 是代表 Forward，而 B 代表 Backward\nPipeline Parallelism\n下面的即为改进方案，将每个 Batch 切分为 micro-batches，拿灰色的举例，被切分为 4 个 micro-batches\n在 Bloom 作者的实践中，当一个词表很大（250K）时，word embedding 会比 transformer 的 block 占用更大显存，此时就需要将 word embedding 层也看做是一个 transformer block\nDP+PP 当然，不同的并行方法也可以组合，DP+PP 就是一个例子。假设我们目前有 4 张卡，我们可以将 2 张卡就看做是一张卡，那我们就可以进行 DP 为 2 的并行，而两张卡里又可以进行 PP 为 2 的并行\nDP + PP\nDP+PP+TP 当然，这两者还可以和 TP 结合在一起，DP=PP=TP=2，这时候就需要八张卡了，这种并行方式叫做 3D Parallel\nDP + PP + TP\n异构空间管理 上面无论是 PP 还是 TP 都在说，放另一张卡上，假设我没有那么多卡咋办，此时就引入了异构操作了，既然放不下，就将模型的 block 进行切分，切分之后尽可能塞满 GPU，放不下的就放在 CPU 和硬盘上\nGemini Gemini 是 Colossal AI 开发的功能，比 ZeRO 多了动态异构空间管理，同时引入了 chunk 机制，可以让通信比原先更加高效\n当用 ZeRO 来切分参数时，每次通信都得申请一块内存，通信完毕再释放，这样就会有内存碎片化的情况；同时，通信是以 Tensor 为粒度进行通信，会导致网络带宽无法充分利用，一般而言，传输消息越长带宽利用率越高\n对于以上两个问题，Gemini 采用以下方式解决：\n将计算顺序连续的一组参数直接存入一个 chunk，避免了内存碎片的问题； 如果一个参数多次发生计算，即会发生多次通信，效率不高，就会将和它有关的一组参数全部放进 chunk 里，降低了通信消耗； 同时，小 Tensor 无法充分利用带宽，Gemini 会将小 Tensor 聚集起来放在 chunk 里变成大 Tensor，然后一次性计算 ZeRO DP+PP+TP 当 ZeRO DP 和 PP（或是 TP）结合在一起时，一般 Stage 1 是可以激活的，然而 Stage 2 和 3 是否被激活，可以看看激活后是否真正达成了自己的目的，因为激活后通信带来的损失会越来越多\nBF16 Bloom 的作者不建议训练 LLM 用 FP16，因为在训练过程中很容易遇到数值溢出问题（loss 很不稳定，如下图），而 BF16 不会遭遇这种问题\n然而，BF16 带来的代价就是精度会降低，不过只要多训一点，精度就会慢慢上来，所以也可以接受\n然而用 BF16 之后 Loss 就平滑不少了\nFused CUDA Kernels GPUs 一般做两件事：计算以及拷贝数据，当 GPU 拷贝数据时，其计算单元是空闲的，比如以下例子：\nc = torch.add(a, b) e = torch.max([c,d]) 原始的做法就是首先从内存读入 a, b 的值，然后进行运算，将 c 的值返回至内存；接下来再从内存读入 c 和 d 的值，进行运算\n而融合的 kernel 就不必返回 c 的值到内存，只需要放到 GPU registers，只需要取出 d 的值进行计算即可，比原来更高效\nReferences https://colossalai.org/zh-Hans/docs/features/zero_with_chunk https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism https://arxiv.org/abs/1910.02054 https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/ https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/ ","permalink":"http://yunpengtai.top/posts/efficient-tricks-for-llms/","summary":"如何高效训练或推理大模型一般在两点：如何装得下以及如何更快 这里讲一些主要的并行概念，不会深挖原理，只会介绍 key points，看它们分别为加速和","title":"Efficient Tricks for LLMs"},{"content":"区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，直接采用自归一化，即：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{1} $$\n首先对于每个训练样本来说，我们再从噪声分布$q(x)$中采样$k-1$个样本，我们的目的即是区分出真实样本：\n$$ \\begin{align} L(\\boldsymbol{\\theta}) \u0026amp; = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w}|\\boldsymbol{c})+\\sum_{j=1}^{k-1}q(\\boldsymbol{w}_{j})}\\\\ \u0026amp; = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j} \\sim q} \\log \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}) + \\sum_{j=1}^{k-1} q(\\boldsymbol{w}_{j})} \\end{align} $$\n最小化上述 loss 到底有什么好处呢？\n互信息来访 对于训练样本$i$来说，它是真实样本的概率为：\n$$ \\begin{align} p(d=i) \u0026amp; = \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})\\prod_{j=1, i\\neq j}^{k}q(\\boldsymbol{w}_{j})}{\\sum_{j=1}^{k}p(\\boldsymbol{w}_{j}|\\boldsymbol{c})\\prod_{l=1,l\\neq j}^{k}q(\\boldsymbol{w}_{l})} \\\\ \u0026amp;= \\frac{\\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{i})}}{\\sum_{j=1}^{k} \\frac{p(\\boldsymbol{w}_{j}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{j})}} \\end{align} $$\n转换的方法即是上下同除以$\\prod_{i=1}^{k} q(\\boldsymbol{w}_{i})$\nInfoNCE 的目的即为正确区分出真实样本，而真实样本都来自训练集（与噪声集区分），也就是说对于 loss 而言，最优解即为$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$正比于 density ratio：\n$$ u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{q(\\boldsymbol{w}_{i})} $$\n我们知道，互信息的计算如下：\n$$ I(\\boldsymbol{w}, \\boldsymbol{c}) = \\text{KL}(p(\\boldsymbol{w}, \\boldsymbol{c})\\|p(\\boldsymbol{w})p(\\boldsymbol{c})) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}, \\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})} $$\n为了和上述 loss 有关，很自然做出以下变化：\n$$ I(\\boldsymbol{w}, \\boldsymbol{c}) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log \\frac{p(\\boldsymbol{w}|\\boldsymbol{c})p(\\boldsymbol{c})}{p(\\boldsymbol{w})p(\\boldsymbol{c})} = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}} \\log {\\color{#337dff}\\frac{p(\\boldsymbol{w}|\\boldsymbol{c})}{p(\\boldsymbol{w})}} $$\n也就是说如果我们要$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$来表示互信息，还得$q(x) = p(x)$，也就是噪声分布即为模型分布（unconditioned）\n那么：\n$$ u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i}, \\boldsymbol{c}) \\propto \\frac{p(\\boldsymbol{w}_{i}|\\boldsymbol{c})}{{\\color{#337dff}p(\\boldsymbol{w}_{i})}} $$\n此时完整的 loss 即为：\n$$ L(\\boldsymbol{\\theta }) = -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{j=1}^{k} u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{j}, \\boldsymbol{c})} $$\n当我们要使得训练样本$\\boldsymbol{w}$是真实样本的概率变得最大，即一定程度最大化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})$，然后最小化$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{j}, \\boldsymbol{c})$，即最大化$\\boldsymbol{w}, \\boldsymbol{c}$之间的互信息，最小化负样本与$\\boldsymbol{c}$之间的互信息\n注意：现在用的时候并非像一开始通过真实样本的概念，而是可以将$\\boldsymbol{w}, \\boldsymbol{c}$看成是一对正样本，噪声样本看作负样本\nHardness-Aware 同时 InfoNCE 会着重关照那些跟真实样本$\\boldsymbol{c}$更相似的负样本，这类也叫做「Hard-Negative」，而相应地会轻视一些与$\\boldsymbol{c}$本来就没有那么相似的负样本，即「Hardness-Aware」\n$$ \\begin{align} L(\\boldsymbol{\\theta }) \u0026amp; = - \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j}\\sim q} \\log \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})+\\sum_{j=1}^{k-1} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{j}, \\boldsymbol{c})} \\\\ \u0026amp;= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j}\\sim q} \\log \\frac{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}} )}{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}})} \\\\ \u0026amp;= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D};\\boldsymbol{w}_{j} \\sim q} \\left[s_{\\boldsymbol{w}, \\boldsymbol{c}} - \\log\\left( \\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}})+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j},\\boldsymbol{c}}) \\right)\\right] \\end{align} $$\n对于$\\boldsymbol{w}_{j}$而言，与$\\boldsymbol{c}$的相似性越大，则在损失函数中占比越大，可以看成是惩罚越大，然而仅凭这个没办法做到上述性质，比如下面的 loss 也可以：\n$$ L_{\\text{simple}}(\\boldsymbol{\\theta }) = \\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\left[-s_{\\boldsymbol{w}, \\boldsymbol{c}} + \\sum_{j=1}^{k-1} s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}\\right] $$\n然而实验证明，这个 loss 效果并不是很好\n接着，我们来求下损失函数相对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的偏导：\n不妨就对单个样本$\\boldsymbol{w}, \\boldsymbol{c}$来看：\n$$ \\frac{ \\partial L_{\\text{simple}} }{ \\partial s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}} } = 1 $$\n也就是说 loss 对于所有$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化一样敏感，即「一视同仁」，没有给损失函数的优化注入其他信息\n$$ \\frac{ \\partial L }{ \\partial s_{\\boldsymbol{w}_{j},\\boldsymbol{c}} } = \\frac{\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}})}{\\exp(s_{\\boldsymbol{w},\\boldsymbol{c}} )+ \\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, c})} $$\n而对于 InfoNCE 而言，对于所有负样本而言，分母是固定的，而「分子越大」，即$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$越大，则此时 loss 对于$s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}$的变化则越敏感\n总结说就是，InfoNCE 这种 loss 在优化时会注入一种偏好，更偏向于把与真实样本相似的噪声样本给区分开来\n温度系数 一般现在用的时候，还会加入一个调节因子：$\\tau$\n$$ L(\\boldsymbol{\\theta})= -\\mathbb{E}_{\\boldsymbol{w}, \\boldsymbol{c} \\sim \\mathcal{D}; \\boldsymbol{w}_{j} \\sim q} \\log \\frac{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau )}{\\exp(s_{\\boldsymbol{w}, \\boldsymbol{c}}/\\tau)+\\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}/\\tau)} $$\n那么：\n$$ \\frac{ \\partial L }{ \\partial s_{\\boldsymbol{w}_{j},\\boldsymbol{c}} } = \\frac{1}{\\tau}\\frac{\\exp(s_{\\boldsymbol{w}_{j}, \\boldsymbol{c}}/\\tau)}{\\exp(s_{\\boldsymbol{w},\\boldsymbol{c}}/\\tau )+ \\sum_{j=1}^{k-1}\\exp(s_{\\boldsymbol{w}_{j}, c}/\\tau)} $$\n推论 1：当$\\tau$很小时，比如$0.02$那么由上式可得 loss 会格外关注于将与真实样本相似的噪声样本给区分开。换句话说，$\\tau$越小，对 hard-negative 的惩罚越大\n下面我们引入一个 kernel 来衡量一个分布的均匀性（uniform），$f(\\boldsymbol{x})$代表模型输出的表示\n$$ L_{\\text{uniformity}}(f;t) = \\log \\mathbb{E}_{\\boldsymbol{x}, \\boldsymbol{y} \\sim \\mathcal{D}} \\left[e^{-t\\|f(\\boldsymbol{x}) - f(\\boldsymbol{y})\\|_{2}^{2}}\\right](t \\in \\mathbb{R}_{+}) $$\n作者采用不同的$\\tau$来对比模型产生表示的均匀性\nWang et al., 2020\n当$\\tau$比较小时，模型产生的表征空间更加均匀，随着$\\tau$的增大，均匀性被破坏\n而对于一个好的对比学习的表征空间应该满足「locally clustered，globally uniform」，当表征均匀分布在球面上时，此时用一个超平面（线性分类器）便可分开，比如下图\nWang et al., 2020\n推论 2：当$\\tau$较小时，用 infoNCE loss 训出的模型表征空间比较均匀，当$\\tau$增大时，表征空间的均匀性被逐步破坏\n上代码 当下的算力已经足够支撑直接用概率而非自归一化了，其实会发现 InfoNCE loss 跟多分类交叉熵损失一样：\nimport torch.nn.functional as F def infoNCE(q_vectors, c_vectors, labels, tau=1): \u0026#34;\u0026#34;\u0026#34;Compute the InfoNCE Loss in http://arxiv.org/abs/1807.03748. Params: - q_vectors: Tensor. Shape: (bs, d_model). Vector presentation of query. - c_vectors: Tensor. Shape: (num_pos_neg_contexts, d_model). Vector presentation of context. - labels: Tensor. Shape: (bs, ). Indices for the positive contexts. - tau: Scalar. The parameter to control penalty on hard negatives (smaller =\u0026gt; harder). A good initialization value can be 0.02 or 0.05. Examples: \u0026gt;\u0026gt;\u0026gt; q_vectors = torch.randn((2, 4)) \u0026gt;\u0026gt;\u0026gt; c_vectors = torch.randn((4, 4)) # each query has 4 contexts \u0026gt;\u0026gt;\u0026gt; labels = torch.tensor([1, 3]) \u0026gt;\u0026gt;\u0026gt; print(infoNCE(q_vectors, c_vectors, labels)) \u0026#34;\u0026#34;\u0026#34; scores = (q_vectors @ c_vectors.T) / tau return F.cross_entropy(scores, labels) ","permalink":"http://yunpengtai.top/posts/infonce/","summary":"区分真实样本 前面的两种是为了去估计配分函数，接下来要介绍的 InfoNCE 虽然带个 NCE，但这个的目的不是要预估配分函数，他是直接像上篇应用 NCE 的方法一样，","title":"放大镜下的 InfoNCE"},{"content":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbol{c}$，要去词表$\\mathcal{V}$挑选$\\boldsymbol{w}$来生成：\nNegative Sampling 其实要说 NCE 的缺点，就是需要花时间调参数，比如说$k$和噪声分布$p_{n}$的选择，而负采样则固定下来这两个参数，不同的负采样对于这两个参数的选择也不尽相同，这里介绍比较简单的一种\n我们规定，当$\\mathcal{D}=1$时代表从训练集采样，而$\\mathcal{D}=0$则代表从噪声中采样，为了表达简便，对$\\boldsymbol{c}$进行省略，即$p(\\mathcal{D}=1|\\boldsymbol{c}, \\boldsymbol{w}) = p(\\mathcal{D}=1|\\boldsymbol{w})$\nNCE 会这样求样本来自哪个采样的概率：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n而负采样说，如果词表大小是$|\\mathcal{V}|$，不妨就采样$|\\mathcal{V}|$个噪声，而且每个噪声是等概率出现的，那么：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{1}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+1} \\end{align} $$\n综合上面的式子来看，其实负采样是 NCE 的一种\nImportance Sampling 我们记需要计算的分母（配分函数）为$Z(\\boldsymbol{\\theta })$，注意：$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})=\\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}))$，$s_{\\boldsymbol{\\theta}}$为打分函数\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\underbrace{ \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) }_{ Z(\\boldsymbol{\\theta }) }} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} $$\n那么：\n$$ \\begin{align} Z(\\boldsymbol{\\theta }) \u0026amp; = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) = \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c}) {\\color{#337dff}\\frac{p_{n}(\\boldsymbol{w}')}{p_{n}(\\boldsymbol{w}')}} \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} p_{n}(\\boldsymbol{w}') \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\\\ \u0026amp;= \\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})}{p_{n}(\\boldsymbol{w}')} \\end{align} $$\n跟 NCE 不同的是，重要性采样并没有把配分函数当成参数，而是利用噪声数据去拟合它：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\mathbb{E}_{\\boldsymbol{w}' \\sim p_{n}} u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}', \\boldsymbol{c})/p_{n}(\\boldsymbol{w}')} $$\n同样地操作，我们可以采样$k$个噪声样本来去近似期望，即：\n$$ p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c}) \\approx \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}p_{n}(\\boldsymbol{w}_{i})u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{i}, \\boldsymbol{c})/p_{n}(\\boldsymbol{w}_{i})} = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\sum_{i=1}^{k}u_{\\boldsymbol{\\theta }}(\\boldsymbol{w_{i}}, \\boldsymbol{c})} $$\n仔细看，其实重要性采样是利用蒙特卡洛采样将分母从一开始的$|\\mathcal{V}|$项降低到$k$项，另外还有InfoNCE\n","permalink":"http://yunpengtai.top/posts/nce-friends/","summary":"在Noise Contrastive Estimation中，我们详细介绍了 NCE 算法，其实还有很多跟它类似的算法，继续以文本生成为例，基于上下文$\\boldsymbo","title":"NCE 的朋友们"},{"content":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对较小影响不大；然而，如果数值大于某个特定值之后变成了$\\inf$（数值上溢 Overflow ），还有一种情况是当数值特别小时，会被近似为$0$（数值下溢 Underflow），这两种情形若继续计算误差将会进一步累积，那么就可能导致原本理论上成立的到实现时就不行了，此时就有必要对数值稳定性进行分析\n举个例子：\nimport numpy a = np.array([65599.], dtype=np.float16) print(a) b = np.array([1e-10], dtype=np.float16) print(b) [inf] # Overflow [0.] # Underflow 基础知识 再进一步解释解决方法时还是先铺垫基础知识：\n众所周知，各种数值在计算机底层是通过比特（bit）来进行表示的，有$0$和$1$，比如用$8$个比特就可以表示$[-2^{7}, 2^{7}-1]$（有一位是符号位）\n那么 float16 的意思是用$16$位比特来表示浮点数，同理 float32 的意思是用$32$位比特\n按照表示精度来看： $$ \\text{float64 \u0026gt; float32 \u0026gt; float16} $$ 按照占用空间来看：\n$$ \\text{float16 \u0026lt; float32 \u0026lt; float64 } $$\n$\\inf$的意思是超过了可以表示的范围，有$-\\inf$和$\\inf$两种，而NaN的产生大概可以分为几种情况：\n对负数开根号 对$\\inf$进行运算 除以$0$ 那么如何查看不同表示的范围呢？\nnp.finfo(np.float16) # finfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16) 当然，numpy 这里有个小坑，就是你输入的值较大或略微超过范围时，反而会用另一个数来表示，只有大到一定程度时，才会用$\\inf$，详见官网的release notes\nFloating-point arrays and scalars use a new algorithm for decimal representations, giving the shortest unique representation. This will usually shorten float16 fractional output, and sometimes float32 and float128 output. float64 should be unaffected.\na = np.array([65504.], dtype=np.float16) print(a) b = np.array([65388.], dtype=np.float16) print(b) c = np.array([65700.], dtype=np.float16) print(c) # [65500.] # [65380.] # [inf] 注意，这里的大是相对于计算精度来说的，而不是你感觉的，当你把精度调成float32，上面都会打印原来输入的结果\ne之殇 在上高中时，特别喜爱$e^{x}$，有很多好的性质，比如求导等于本身，然而在很多数值溢出的情形，总有它的参与，这是因为机器学习中很多东西都会和它挂钩，比如各种激活函数便有它的影子\n看个例子：\na = np.exp(np.array([654.], dtype=np.float16)) print(a) # [inf] 那么我如何知道输入大概多大会导致溢出呢？可以参见下述公式，当输入为$x$，计算$e^{x}$用十进制数表示大概有多少位\n$$ \\log_{10}(e^x) = x \\log_{10}(e) $$\n举个例子，上面我们看到float16当最大位数是$5$位（科学计数法后面得$+1$），那么根据上述公式你就可以算出最大可被接受的$x$，进而进行一些后处理：\ndef compute_max(n): return int(n * math.log(10)) print(compute_max(5)) # 11 我们来试试看：\na = np.exp(np.array([11.], dtype=np.float16)) print(a) b = np.exp(np.array([12.], dtype=np.float16)) print(b) # [59870.] # [inf] 接下来按照消除溢出的方法看看几大类常见例子：\n归一化指数 当$\\exp(x_{i})$形式出现，就会考虑通过代数恒等变换使得指数上多一些部分来进行归一化\nsoftmax 不妨假设$\\boldsymbol{x} \\in \\mathbb{R}^{n}$，当某个$x_{i}$特别大时，$\\exp(x_{i})$就会出现数值上溢，然后当分子分母都是$\\inf$的时候就会出现NaN，$0$是因为分母是$\\inf$导致的\nimport numpy as np def softmax(x): exp = np.exp(x) return exp / exp.sum(-1) x = np.array([-1., 20000, 0.1], dtype=np.float16) softmax(x) # array([ 0., nan, 0.]) 那么，有什么好的方法来防止这种数值溢出呢，我们通过一些不改变原式的代数运算可以做到：\n$$ \\begin{align} \\mathrm{softmax}(x_{j}) \u0026amp; = \\frac{e^{x_{j}}}{\\sum_{i} e^{x_{i}}} \\\\ \u0026amp;= {\\color{#337dff}\\frac{c}{c}} \\cdot \\frac{ e^{x_{j}}}{ \\sum_{i} e^{x_{i}}} \\\\ \u0026amp;= \\frac{e^{x_{j} + \\log c}}{ \\sum_{i} e^{x_{i} +\\log c}} \\end{align} $$ 观察上式，不难发现，我们可以控制常数$c$来对$x_{i}$进行规范化，相当于加上偏移量（offset）\n比较简单的做法即为设置$\\log c = -\\max(\\boldsymbol{x})$\n那么：\n$$ \\mathrm{softmax}(x_{j}) = \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i} - \\max(\\boldsymbol{x})}} $$\n代码实现即为：\ndef softmax(x): x -= max(x) exp = np.exp(x) return exp / exp.sum(-1) softmax(x) # array([0., 1., 0.]) 因为最大的数减去自身变为了$0$，$e^{0} = 1$，就不会有什么影响了\n这个与PyTorch官方实现也是一致的：\nimport torch import torch.nn.functional as F x = torch.tensor([-1., 20000, 0.1]) F.softmax(x) # tensor([0., 1., 0.]) Logsumexp 同理再看一个类似的：\n$$ \\begin{align} \\text{Logsumexp}(\\boldsymbol{x}) \u0026amp;= \\log \\sum_{i} e^{x_{i}} \\\\ \u0026amp;= \\log \\sum_{i} e^{x_{i}} \\frac{c}{c} \\\\ \u0026amp;= \\log \\left( \\frac{1}{c} \\sum_{i} e^{x_{i}} e^{\\log c}\\right) \\\\ \u0026amp;= -\\log c + \\log \\sum_{i} e^{x_{i} + \\log c} \\end{align} $$\n取$\\log c=-\\max(\\boldsymbol{x})$，那么：\n$$ \\text{logsumexp}(\\boldsymbol{x}) = \\max(\\boldsymbol{x}) + \\log \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})} $$\ndef logsumexp(x): maximum = max(x) x -= maximum exp = np.exp(x) return maximum + np.log(exp.sum(-1)) 跟 PyTorch 官方实现一致：\nprint(torch.logsumexp(x, dim=-1, keepdim=False)) print(logsumexp(x.numpy())) # tensor(200000.) # 200000.0 展开log内部 就是将log明显可能出现数值溢出的部分拆出来，刚刚logsumexp就是利用了这个道理\nlog-softmax 尽管softmax现在稳定了，然而log-softmax还是有风险溢出，比如上面的$\\log 0$，这也是数值上溢\n$$ \\mathrm{LogSoftmax}(x_{j}) = \\log\\left(\\frac{e^{x_{j}}}{\\sum_{i} e ^{x_{i}}}\\right) $$ 我们将其拆开： $$ \\begin{align} \\mathrm{LogSoftmax}(x_{j}) \u0026amp; = \\log \\left( \\frac{e^{x_{j} - \\max(\\boldsymbol{x})}}{\\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}} \\right) \\\\ \u0026amp;= \\log(e^{x_{j} - \\max(\\boldsymbol{x})}) - \\log \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) \\\\ \u0026amp;= x_{j} - \\max(\\boldsymbol{x}) - \\log \\underbrace{ \\left( \\sum_{i} e^{x_{i}-\\max(\\boldsymbol{x})}\\right) }_{ \\ge 1 } \\end{align} $$\n因为所有的$x_{i}$中肯定有最大的一个，那么$\\exp(x_{i} -\\max(\\boldsymbol{x}))=1$，剩下的肯定是正数\ndef log_softmax(x): x -= max(x) exp = np.exp(x) return x - np.log(exp.sum(-1)) 同样与PyTorch一致，后面是两个框架显示机制不同，从这个角度也可以看出，都是32位时，PyTorch会更精准\nx = torch.tensor([-1., 200000, 0.1]) print(F.log_softmax(x, dim=-1)) print(log_softmax(x.numpy())) # tensor([-200001.0000, 0.0000, -199999.9062]) # array([-200001. , 0. , -199999.9], dtype=float32) 截断 当输入大于某种阈值，直接输出原来的输入\nSoftplus 下面是PyTorch官方对于Softplus的实现\n$$ \\text{Softplus}({x}) = \\begin{cases} \\log (1 + e^{x}), \u0026amp; x \\leq \\text{threshold} \\\\ x, \u0026amp; \\text{otherwise} \\end{cases} $$ 官方的意思很简单，当$x$大于阈值（这里是置之为$20$），直接不变输出\neps 急救包 当分母可能出现为$0$时，给它加上一个较小的正数$\\varepsilon$\nLayer Normalization $$ \\text{LayerNorm}(\\boldsymbol{x}) = \\gamma \\left(\\frac{\\boldsymbol{x} - \\bar{\\boldsymbol{x}}}{\\sigma + {\\color{#337dff}\\varepsilon}} \\right) + \\beta $$ 分母加上一个$\\varepsilon$来防止变为$0$：\nclass LayerNorm(nn.Module): def init(self, features, eps=1e-6): super().__init__() self.gamma = nn.Parameter(torch.ones(features)) self.beta = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.gamma * (x - mean) / (std + self.eps) + self.beta ","permalink":"http://yunpengtai.top/posts/numerical-stability/","summary":"Why 当计算涉及到实数域时，比如圆周率的$\\pi$，因为小数部分是无穷的，计算机是无法准确表示，因而只会用近似的值进行替代，这种情况下，误差相对","title":"Numerical Stability"},{"content":"引言 我们规定，训练集记为$\\mathcal{D}$，我们从中取一个样本$\\boldsymbol{x}$，其训练集标签为$y_{\\mathcal{D}}$，一般假设训练集是从一个真实的分布中采样而来，而真实分布不可见，算法通过训练集来近似整个分布。而采样的过程中存在噪声$\\epsilon$，也就是说有可能因为噪声$y_{\\mathcal{D}}$和真实标签的期望$y$不一致，举个例子，手写数字体识别数据集中存在不少标注错误，此时它们真实的标签和训练集中的就不一致。\n$$ y = \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} $$ 这里为了讨论方便，不妨将噪声置为$0$，即：\n$$ \\varepsilon = \\mathbb{E}_{\\mathcal{D}} (y - y_{\\mathcal{D}}) = 0 $$\n我们还有个在$\\mathcal{D}$上训练好的模型，其对于$\\boldsymbol{x}$的预测记为$f(\\boldsymbol{x})$，在整个训练集上预测的期望为：\n$$ \\bar{f}(\\boldsymbol{x}) = \\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) $$\n偏差-方差分解 那么，我们将期望损失函数进行拆解：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left(f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}) - (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )\\right)^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left((f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2}-2(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x})(y_{\\mathcal{D}}-\\bar{f}(\\boldsymbol{x}))\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2} - 2\\mathbb{E}_{\\mathcal{D}}(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))(y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})) \\end{align} $$\n对于式中最后一项，展开看看：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))(y_{\\mathcal{D} }-\\bar{f}(\\boldsymbol{x})) \u0026amp;= \\mathbb{E}_{\\mathcal{D}} \\left((f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))y_{\\mathcal{D}} - (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))\\bar{f}(\\boldsymbol{x})\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))y_{\\mathcal{D}} - \\mathbb{E}_{\\mathcal{D}}(f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))\\bar{f}(\\boldsymbol{x}) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})\\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})\\mathbb{E}_{\\mathcal{D}} f(\\boldsymbol{x}) + \\bar{f}(\\boldsymbol{x})^{2} \\\\ \u0026amp;= {\\color{#337dff}\\mathbb{E}_{\\mathcal{D}}f(\\boldsymbol{x}) \\cdot\\mathbb{E}_{\\mathcal{D} } y_{\\mathcal{D}}} - \\bar{f}(\\boldsymbol{x}) \\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x})^{2} + \\bar{f}(\\boldsymbol{x}) ^{ 2} \\\\ \u0026amp;= (\\mathbb{E}_{\\mathcal{D}}f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x})) \\mathbb{E}_{\\mathcal{D}} y_{\\mathcal{D}} \\\\ \u0026amp;= 0 \\end{align} $$\n上述推导过程中，主要利用了两个信息：\n$\\bar{f}(\\boldsymbol{x})$是个常量，因而可以直接提出来 训练集本身的分布和模型预测的情况两者是相互独立的，因而就有了蓝色部分的分解 接下来我们还得继续把偏差凑出来，这里的$y$应该是指真实标签的期望：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - \\bar{f}(\\boldsymbol{x}) )^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x})-\\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}} (y_{\\mathcal{D}} - y + y - \\bar{f}(\\boldsymbol{x}))^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}\\left((y_{\\mathcal{D}}-y)^{2}+ (y-\\bar{f}(\\boldsymbol{x}))^{2} + 2(y_{\\mathcal{D}}-y)(y-\\bar{f}(\\boldsymbol{x}))\\right) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} + 2\\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)(y-\\bar{f}(\\boldsymbol{x})) \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} + 2({\\color{#337dff}y-\\bar{f}(\\boldsymbol{x})})\\underbrace{ \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y) }_{\\varepsilon=0 } \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{\\mathcal{D}}-y)^{2} + \\mathbb{E}_{\\mathcal{D}}(y-\\bar{f}(\\boldsymbol{x}))^{2} \\\\ \u0026amp;= \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{D} - y)^{2} + (y - \\bar{f}(\\boldsymbol{x}))^{2} \\end{align} $$\n那么：\n$$ \\begin{align} \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - y_{\\mathcal{D}})^{2} \u0026amp; = \\mathbb{E}_{\\mathcal{D}} (f(\\boldsymbol{x}) - \\bar{f}(\\boldsymbol{x}))^{2} + (y - \\bar{f}(\\boldsymbol{x}))^{2} + \\mathbb{E}_{\\mathcal{D}}(y_{D} - y)^{2} \\\\ \u0026amp;= \\mathbb{V}(\\boldsymbol{x}) + bias^{2}(\\boldsymbol{x}) + \\varepsilon^{2} \\end{align} $$\n泛化能力 偏差衡量了算法本身对于真实标签的拟合情况，the deviation of the expected estimator value from the true value 方差衡量了在不同训练集$\\mathcal{D}$情况下，即采样不同的训练集跟期望预测的偏差，也就是受扰动性影响，the deviation from the expected estimator value that any particular sampling of data is likely to cause（注意$f(\\boldsymbol{x})$是随着训练集而变化的，而$\\bar{f}(\\boldsymbol{x})$则是取一个任意训练集然后固定） 噪声则是衡量了期望误差的下界，也就是说，无论算法怎样，这个任务难度的初始值就是这样 ","permalink":"http://yunpengtai.top/posts/bias-variance-decomposition/","summary":"引言 我们规定，训练集记为$\\mathcal{D}$，我们从中取一个样本$\\boldsymbol{x}$，其训练集标签为$y_{\\mathca","title":"Bias Variance Decomposition"},{"content":"难以承受之重 文本生成是 NLP 任务中比较典型的一类，记参数为$\\boldsymbol{\\theta }$，给定的 context 为$\\boldsymbol{c}$，需要生成的文本记为$\\boldsymbol{w}$，我们通常通过最大似然法来使得模型预测的分布$p_{\\boldsymbol{\\theta}}$尽可能接近训练集分布$p_{d}(\\boldsymbol{w})$\n$$ \\boldsymbol{\\theta ^{\\ast}} = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta }) $$\n而在建模时，我们通常会在模型加入 Softmax 来将 score 转换为概率，使得对词表$\\mathcal{V}$所有词预测概率相加为 1：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{\\underbrace{ {\\color{#337dff}\\sum_{\\boldsymbol{w}' \\in \\mathcal{V}} u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}', \\boldsymbol{c})} }_{ \\text{Partition Function} }}, u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c}) = \\exp(s_{\\theta }(\\boldsymbol{w}, \\boldsymbol{c})) $$\n分母是用来归一化的，也被称作配分函数（Partition Function），为了使得表达更简便，我们将上述公式进一步压缩，将分母统称为$Z(\\boldsymbol{\\theta })$\n若是普通的多分类问题，参数量不大，求$Z(\\boldsymbol{\\theta })$感觉不到压力，可若是文本生成任务，例如，「文本____是自然语言处理的任务」，此时你得去整个词表$\\mathcal{V}$中来挑选词来填空，去计算$Z(\\boldsymbol{\\theta })$就是十分昂贵的事情了\n丢给参数 那么，有人就说了，不行那就直接交给参数处理吧，让模型自己去学，看看模型自己能不能学出归一化：\n$$ p(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta}) = \\frac{u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})}{Z(\\boldsymbol{\\theta })} = u_{\\boldsymbol{\\theta }}(\\boldsymbol{w}, \\boldsymbol{c})\\exp(z^{\\boldsymbol{c}}), \\, z^{\\boldsymbol{c}} = -\\log Z(\\boldsymbol{\\theta }) $$\n接着应用最大似然：\n$$ \\begin{align} \\boldsymbol{\\theta ^{\\ast}} \u0026amp; = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}|\\boldsymbol{c};\\boldsymbol{\\theta }) \\\\ \u0026amp;= \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }, \\boldsymbol{z}} \\ \\mathbb{E}_{\\boldsymbol{c}, \\boldsymbol{w} \\sim p_{d}} \\log u_{\\boldsymbol{\\theta }}(\\boldsymbol{w},\\boldsymbol{c})\\exp(z^{\\boldsymbol{c}}) \\end{align} $$\n这样的结果就是，为了最大化期望，会使得$Z(\\boldsymbol{\\theta }) \\to 0$，效果会很不好\n曲径通幽 那么 Noise Contrastive Estimation（NCE）说，既然这样，我们能不能引入参数的同时也可以出色地预估$Z(\\boldsymbol{\\theta })$呢？于是乎，它将问题从原本的多分类问题转换为二分类问题 Proxy Problem，指用新的任务或指标来完成对原本任务的建模 具体如下：\n首先，存在一个噪声分布$p_{n}$和经验概率分布$p_{d}$，这里$p_{d}$是从训练集提取的，就类似 word2vec 的训练，将句子切分成词 现代 NLP 基本都是 token，这里是为了表达简便 ，统计某几个词一起出现的概率，那么$p(\\boldsymbol{w}|\\boldsymbol{c})$就是对于$\\boldsymbol{c}$而言，下一个词是$\\boldsymbol{w}$的概率。举个例子，对于 love 而言：$p_{d}=\\{ \\text{games}: 0.9, \\text{study}: 0.1 \\}$\n每次从$p_{d}$中抽出一个候选词，从$p_{n}$中抽取$k$个候选词。模型的任务即为区分候选词是从训练集还是噪声中采样而来的，通过这个代理任务使得$p_{\\boldsymbol{\\theta}}(\\boldsymbol{c})$去逼近于$p_{d}(\\boldsymbol{c})$\n我们规定，当$\\mathcal{D}=1$时代表从训练集采样，而$\\mathcal{D}=0$则代表从噪声中采样，那么：\n$$ \\begin{align} p(\\boldsymbol{w}|\\mathcal{D} =1, \\boldsymbol{c}) \u0026amp; = p_{d}(\\boldsymbol{w})\\\\ p(\\boldsymbol{w}|\\mathcal{D} = 0, {\\boldsymbol{c}}) \u0026amp; = p_{n}({\\boldsymbol{w}}) \\end{align} $$\n那么总概率即为：\n$$ p_{joint}(\\boldsymbol{w}) =\\frac{1}{k+1}p_{d}(\\boldsymbol{w}) + \\frac{k}{k+1} p_{n}(\\boldsymbol{w}) $$\n接下来求一下来自哪个采样的条件概率，为了表达简便，对$\\boldsymbol{c}$进行省略：$p(\\mathcal{D}=1|\\boldsymbol{c}, \\boldsymbol{w}) = p(\\mathcal{D}=1|\\boldsymbol{w})$：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p(\\mathcal{D}=1,\\boldsymbol{w})}{p_{joint}(\\boldsymbol{w})} = \\frac{p(\\boldsymbol{w}|\\mathcal{D}=1)p(\\mathcal{D}=1)}{p_{joint}(\\boldsymbol{w})} \\\\ \u0026amp; = \\frac{p_{d}(\\boldsymbol{w})}{p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n同理：\n$$ p(\\mathcal{D}=0|\\boldsymbol{w}) = \\frac{kp_{n}(\\boldsymbol{w})}{p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} $$\n$\\mathcal{D}$代表训练集和噪声集的集合，那么 NCE 的目标即为最大化期望：\n$$ \\boldsymbol{\\theta }^{\\ast} = \\mathop{\\arg \\max}_{\\boldsymbol{\\theta }}\\ \\mathbb{E}_{\\boldsymbol{w} \\sim \\mathcal{D}} \\log p(\\mathcal{D}|\\boldsymbol{w}) $$\n又因为我们想要让模型分布$p_{\\boldsymbol{\\theta }}$尽可能接近训练集分布$p_{d}$，于是我们在求条件概率时，将$p_{d}$换成$p_{\\boldsymbol{\\theta }}$，即：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ p(\\mathcal{D}=0|\\boldsymbol{w}) \u0026amp; = \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n我们展开期望看看：\n$$ \\begin{align} \\mathbb{E}_{\\boldsymbol{w}\\sim \\mathcal{D}} \\log p(\\mathcal{D}|\\boldsymbol{w}) \u0026amp; = \\int_{\\boldsymbol{w}} p(\\boldsymbol{w})\\log p(\\mathcal{D}|\\boldsymbol{w}) \\, d\\boldsymbol{w} \\\\ \u0026amp;= \\int_{\\boldsymbol{w}} \\frac{1}{k+1}(p_{d}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))\\log p(\\mathcal{D}|\\boldsymbol{w})\\, d \\boldsymbol{w} \\\\ \u0026amp;= \\frac{1}{k+1} \\left( \\int_{\\boldsymbol{w}} p_{d}(\\boldsymbol{w}) \\log p(\\mathcal{D}=1|\\boldsymbol{w}) \\, d \\boldsymbol{w} + \\int _{\\boldsymbol{w}} k p_{n}(\\boldsymbol{w}) \\log p(\\mathcal{D}=0|\\boldsymbol{w}) \\, d\\boldsymbol{w} \\right) \\\\ \u0026amp;= \\frac{1}{k+1}\\bigg(\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w})\\bigg) \\end{align} $$\n上述期望计算初看肯定有两处疑问：\n第一、为啥要基于$\\boldsymbol{w}$而非$\\boldsymbol{c}$来展开概率计算呢？当然两者都可以，但是我们的目标是为了让模型分布去拟合训练集分布，若是按照$\\boldsymbol{c}$展开，也就是$p_{\\boldsymbol{\\theta }}(\\boldsymbol{c})\\approx p_{d}(\\boldsymbol{c})$，让模型预测输入的 feature 不合理\n第二、不是说好用模型分布代替数据分布吗？为什么$p(\\boldsymbol{w})$还是用的数据分布，我想可能是为了训练方便考虑，若两处都是模型分布，训练势必更难；同时，数据分布是一个既定事实，可以充当额外的信息量给模型，加快收敛\n因为$k$是常数，对优化目标函数无影响，下式省略之，那么，我们的目标函数即为：\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n极限的视角 你肯定好奇 Proxy Problem 是否可以近似原来的建模，目标函数相对于$\\boldsymbol{\\theta }$的微分告诉了我们答案\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n那么，我们求关于参数$\\boldsymbol{\\theta }$的微分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } J(\\boldsymbol{\\theta }) \u0026amp; = \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+\\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) \\\\ \u0026amp;= \\mathbb{E}_{\\boldsymbol{w}\\sim p_{d}} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} + k\\mathbb{E}_{\\boldsymbol{w}\\sim p_{n}} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\end{align} $$\n那么接下来我们拆开来求目标函数相对于参数的微分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \u0026amp; = \\frac{p_{\\theta }(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{p_{\\theta }(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})} \\frac{p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})(p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))-p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{(p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))^{2} } \\\\ \u0026amp;= \\frac{p_{\\boldsymbol{\\theta }}'(\\boldsymbol{w})kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})(p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))} \\\\ \u0026amp;= \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} {\\color{#337dff}\\frac{p_{\\boldsymbol{\\theta }}'(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}} \\\\ \u0026amp;=\\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\end{align} $$\n另一部分：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \u0026amp; =\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}{kp_{n}(\\boldsymbol{w})} \\frac{0-kp_{n}(\\boldsymbol{w})p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{(p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w}))^{2}} \\\\ \u0026amp;= -\\frac{p_{\\boldsymbol{\\theta}}'(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} {\\color{#337dff}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}}\\\\ \u0026amp;= - \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\end{align} $$\n合起来看看：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } J(\\boldsymbol{\\theta }) \u0026amp; = \\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) -k \\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}} p_{d}(\\boldsymbol{w}) \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) -k \\sum_{\\boldsymbol{w}}p_{n}(\\boldsymbol{w})\\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}) \\\\ \u0026amp;= \\sum_{\\boldsymbol{w}} \\underbrace{ {\\color{#337dff}\\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})}} }_{ k \\to \\infty, ratio \\to 1 }\\bigg(p_{d}(\\boldsymbol{w}) - p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})\\bigg) \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) \\\\ \u0026amp;\\approx \\sum_{\\boldsymbol{w}}\\bigg(p_{d}(\\boldsymbol{w}) - p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})\\bigg) \\frac{ \\partial }{ \\partial \\boldsymbol{\\theta } } \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}) \\end{align} $$\n对于第四步的近似，可以举个例子，比如$10000/(1+10000)$，其实因为$1$太小了，可以忽略不计\n这也是为什么这个 Proxy Problem 可以 work 的原因，当采样的噪声样本足够多时，NCE 的梯度就接近于一开始我们想要直接去做最大似然的梯度\n两次近似 尽管通过引入参数可以去估计$Z(\\boldsymbol{\\theta })$很巧妙，但有一个很大的问题，对于每一组词而言，尽管$\\mathcal{V}$是一样的，然而基于的$\\boldsymbol{c}$不一致，那么$p(\\boldsymbol{w}|\\boldsymbol{c})$也是不同的，即每组词你得去保存一个参数$z^{\\boldsymbol{c}}$\n这个时候作者发现了「神之一手」，直接令$Z(\\boldsymbol{\\theta })\\approx 1$，也就是俗称的 self-normalization，换句话说，压根没有转换为概率，你看到这肯定会露出不屑的表情，我也一样\n自归一化 work 的原因是什么呢？引用原著的说法：\nWe believe this is because the model has so many free parameters that meeting the approximate per-context normalization constraint encouraged by the objective function is easy.\n作者的意思就是参数很多，于是就有了 power，模型自己可以去学习归一化，当然，原著中做了对比，发现效果几乎没影响，才这么做的\n其实我看来还是目标函数选的好，因为当梯度近似为$0$时，$p_{d}$和$p_{\\boldsymbol{\\theta }}$很接近\n注意，当采用自归一化时：$p(\\boldsymbol{w}|\\boldsymbol{c}) = \\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c})) / 1$\n这里其实有个容易误解的点，其实这个目标函数只是为了拟合一对词$(\\boldsymbol{c}, \\boldsymbol{w})$：\n$$ J(\\boldsymbol{\\theta }) =\\mathbb{E}_{\\boldsymbol{w} \\sim p_{d}} \\log p(\\mathcal{D}=1|\\boldsymbol{w})+k\\mathbb{E}_{\\boldsymbol{w} \\sim p_{n}}\\log p(\\mathcal{D}=0|\\boldsymbol{w}) $$\n对于每组词都要计算期望，即考虑所有候选可能太过奢侈，所以原著进行了第二次近似，也有一些资料是说抽取$k$个是蒙特卡洛模拟的一种\n$$ J^{\\boldsymbol{c}}(\\boldsymbol{\\theta }) = \\log p(\\mathcal{D}=1|\\boldsymbol{w}_{0}) + \\sum_{i=1}^{k} \\log p(\\mathcal{D}=0|\\boldsymbol{w}_{i}) $$\n那么对于所有的词组该如何建模呢？我们定义一个全局 NCE 进行优化就行了：\n$$ J(\\boldsymbol{\\theta }) = \\sum_{\\boldsymbol{c}} p(\\boldsymbol{c)}J^{\\boldsymbol{c}}(\\boldsymbol{\\theta }) $$\nsigmoid 客串 当然，如果你看现在很多机器学习库的实现，你会发现跟上面的式子可能有点不一样？\n进行变形一下：\n$$ \\begin{align} p(\\mathcal{D}=1|\\boldsymbol{w}) \u0026amp; = \\frac{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})+kp_{n}(\\boldsymbol{w})} \\\\ \u0026amp;= \\frac{1}{1+ \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})}} \\\\ \u0026amp;= \\frac{1}{1+ \\exp\\left(\\log \\left( \\frac{kp_{n}(\\boldsymbol{w})}{p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})} \\right)\\right)} \\\\ \u0026amp;= \\frac{1}{1+\\exp(\\log kp_{n}(\\boldsymbol{w})-\\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w}))} \\\\ \u0026amp;= \\frac{1}{1+\\exp({\\color{red}-}(\\underbrace{ \\log p_{\\boldsymbol{\\theta }}(\\boldsymbol{w})-\\log kp_{n}(\\boldsymbol{w})) }_{ x })} \\end{align} $$\n将里面看成一个参数$x$，那么就看到了 sigmoid 函数：\n$$ p(\\mathcal{D}=1|\\boldsymbol{w}) = \\sigma(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})-\\log kp_{n}(\\boldsymbol{w})) $$\n同理：\n$$ p(\\mathcal{D}=0|\\boldsymbol{w}) = \\sigma(\\log kp_{n}(\\boldsymbol{w}) - \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w})) $$\n损失函数基本就呼之欲出了：\n$$ \\begin{align} L(\\boldsymbol{\\theta }) \u0026amp; = - \\sum_{\\boldsymbol{c}} p(\\boldsymbol{c})\\left( \\log p(\\mathcal{D}=1|\\boldsymbol{w}_{0})+\\sum_{i=1}^{k} \\log p(\\mathcal{D}=0|\\boldsymbol{w}_{i}) \\right) \\\\ \u0026amp;= -\\sum_{\\boldsymbol{c}} p(\\boldsymbol{c}) \\left( \\log \\sigma(\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{0})-\\log kp_{n}(\\boldsymbol{w}_{0})) + \\sum_{i=1}^{k} \\log \\sigma(\\log kp_{n}(\\boldsymbol{w}_{i})- \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{i})))\\right) \\\\ \u0026amp;= -\\sum_{\\boldsymbol{c}} p(\\boldsymbol{c})\\bigg(\\log\\sigma(s_{\\boldsymbol{\\theta }}( \\boldsymbol{w}_{0}, \\boldsymbol{c} )-\\log kp_{n}(\\boldsymbol{w}_{0}))+\\sum_{i=1}^{k} \\log \\sigma(\\log kp_{n}(\\boldsymbol{w}_{i})- s_{\\boldsymbol{\\theta }}(\\boldsymbol{w}_{i}, \\boldsymbol{c} ))\\bigg) \\end{align} $$ 因为$u_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}) = \\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w}, \\boldsymbol{c}))$，所以可以抵消掉$\\log$；同时因为采用自归一化，所以$p_{\\boldsymbol{\\theta}}(\\boldsymbol{w}_{0}) = \\exp(s_{\\boldsymbol{\\theta}}(\\boldsymbol{w_{0}}, \\boldsymbol{c}))/1$\n上代码 这里的$p_{n}$选的是 log-uniform，类别越往后出现的概率就越小，所以如果要用，可以将类别按照数目进行排序，将多的放在前面，举个例子，类别 A, B, C, D 分别出现的数目为 10, 20, 100, 15，那么类别排序就应该是 C B D A，类别 0 对应的就是 C。range_max 对应的就是类别总数，这里就是 4\n$$ \\log_{uniform}(class) = \\frac{\\log(class + 2) - \\log (class + 1)}{\\log(range_{max} + 1)} $$\n下面是训练的 loss，eval 的时候没有 noise，找出 labels 对应的 logits，然后算指标就行了，同时，这里考虑数值稳定性，用 pytorch 官方的 softplus 来取代 logsigmoid，详见[[Numerical Stability#Softplus]]\nimport math from einops import repeat import torch.nn.functional as F from torch import arange, randn, tensor, log, multinomial def nce_loss(logits_pos, logits_neg, log_pn_pos, log_pn_neg, k): \u0026#34;\u0026#34;\u0026#34;Compute the noise contrastive estimation loss in https://arxiv.org/abs/1806.03664. Params: - logits_pos: Tensor. Shape: (bs, 1). Logits corresponding to labels. - logits_neg: Tensor. Shape: (bs * k, 1). Logits corresponding to sampled classes. - log_pn_pos: Tensor. Shape: (bs, 1). Log-probability of labels sampled from noise distribution. - log_pn_neg: Tensor. Shape: (bs * k, 1). Log-probability of noise candidates sampled from noise distribution. - k: int. The number of noise candidates per training example. Note: This implementation assumes each context is equally shown which leads to final averge.\u0026#34;\u0026#34;\u0026#34; logk = math.log(k) # For numerical stability, replace logsigmoid by the torch softplus # for it considers the overflow situation. # log(sigmoid(x)) = -softplus(-x) # final return also contains minus(-), thus remove all the minus(-) pos = F.softplus((logk + log_pn_pos) - logits_pos).mean() neg = F.softplus(logits_neg - (logk + log_pn_neg)).mean() return pos + neg def log_uniform(num_sampled, range_max, replacement=True): \u0026#34;\u0026#34;\u0026#34;Sample classes from log-uniform distribution.: p(class) = (log(class + 2) - log(class + 1)) / (log(range_max + 1)). sampled_classes: [0, range_max). Also note that the data distribution should follow the log_uniform. e.g., the classes should be in decreasing order of frequencey when in text generation. Params: - num_sampled: int. The number to be sampled. - range_max: int. The number of total classes. - replacement: bool. If false, sampled candidates are unique. Examples: \u0026gt;\u0026gt;\u0026gt; log_uniform(2, 10) \u0026gt;\u0026gt;\u0026gt; # tensor([7, 2]) \u0026#34;\u0026#34;\u0026#34; classes = arange(0, range_max) probs = log((classes + 2) / (classes + 1)) / math.log(range_max + 1) return probs, multinomial(probs, num_sampled, replacement=replacement) def main(): bs, k = 2, 4 num_classes = 8 logits = randn(bs, num_classes) labels = tensor([2, 4]) probs, noise_classes = log_uniform(bs * k, num_classes) logits_pos = logits.take_along_dim(labels[:, None], dim=1) log_pn_pos = probs[labels] log_pn_neg = probs[noise_classes] logits_k = repeat(logits, \u0026#39;(b 1) h -\u0026gt; (b k) h\u0026#39;, k=k) logits_neg = logits_k.take_along_dim(noise_classes.reshape(bs * k, -1), dim=1) loss = nce_loss(logits_pos, logits_neg, log_pn_pos, log_pn_neg, k) print(\u0026#39;nce loss: %f\u0026#39; %loss) if __name__ == \u0026#39;__main__\u0026#39;: main() 至于实验，先鸽一下，留在后面与 info-nce，negative-sampling 等做对比\nReferences http://proceedings.mlr.press/v9/gutmann10a.html http://arxiv.org/abs/1206.6426 https://leimao.github.io/article/Noise-Contrastive-Estimation/ https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler ","permalink":"http://yunpengtai.top/posts/noise-contrastive-estimation/","summary":"难以承受之重 文本生成是 NLP 任务中比较典型的一类，记参数为$\\boldsymbol{\\theta }$，给定的 context 为$\\boldsymbol{c}$","title":"Noise Contrastive Estimation"},{"content":"问题 先规定一些术语：记选中元素构成的集合为$\\mathcal{S}$，未选中构成的元素记为$\\mathcal{R}$，$\\mathbf{L}$是核矩阵（核函数是内积），$\\mathbf{L_{V}}$是由集合$S$的元素构成的子矩阵\n在 Determinatal Point Process 中我们提到在大小为$n$的集合里去挑选$k$个物品构成集合$S$, 使得$\\det(\\mathbf{L}_{\\mathbf{V}})$最大便是我们的目标，然而，怎么去里面挑选$\\mathbf{V}$却是 NP-Hard 问题，为此，Chen et al., 2018 提出了一篇比较巧妙的贪婪算法作为近似解，并且整个算法的复杂仅有$\\mathcal{O}(nk^{2})$\n暴力求解 我们人为规定了要选择$k$个，这相当于是一种前验分布，那么 k-DPP 其实就是最大化后验概率（MAP）的一种，每一步的目标就是选择会让新矩阵的行列式变得最大的元素\n$$ j = \\mathop{ \\arg \\max}_{i \\in \\mathcal{R}} \\log \\det(\\mathbf{L}_{\\mathcal{S} \\cup \\{i\\}}) - \\log \\det(\\mathbf{L}_{\\mathcal{S}}) $$\n对于一个$n\\times n$的方阵而言，求它的行列式需要$\\mathcal{O}(n^{3})$（每一轮消元的复杂度是$\\mathcal{O}(n^{2})$，而要进行$n-1$轮消元）\n这里的话，每次要对$\\mathcal{R}$所有的元素求一次行列式，而行列式的为$\\mathcal{O}(|\\mathcal{S}|^{3})$，同时需要选$k$个，复杂度变为了$\\mathcal{O}(|\\mathcal{S}|^{3} \\cdot |\\mathcal{R}| \\cdot k)$，即为$\\mathcal{O}(nk^{4})$，暴力求解的话复杂度很大，此时原作者便提出了利用 Cholesky 分解的方式来进行求解，巧妙地将复杂度降到了$\\mathcal{O}(nk^{2})$\nCholesky 分解 $\\mathbf{L}_{\\mathcal{S}}$是对称半正定矩阵，证明如下：$\\forall \\boldsymbol{z} \\in \\mathbb{R}^{n}$\n$$ \\boldsymbol{z}^{\\top}\\mathbf{L}_{\\mathcal{S}}\\boldsymbol{z} = \\boldsymbol{z}^{\\top} \\mathbf{V}^{\\top}\\mathbf{V} \\boldsymbol{z} = \\|\\mathbf{V}\\boldsymbol{z}\\|_{2}^{2} \\geq 0 \\quad \\blacksquare $$\n那么$\\mathbf{L}_{\\mathcal{S}}$存在 Cholesky 分解，即$\\mathbf{L}_{\\mathcal{S}}=\\mathbf{U}\\mathbf{U}^{\\top}$，这里的$\\mathbf{U}$是大小为$|\\mathcal{S}|\\times|\\mathcal{S}|$的下三角矩阵，$\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}$比$\\mathbf{L}_{\\mathcal{S}}$多了一行和一列，即为：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} $$\n而这里默认每个向量是经过归一化的，即$\\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u}=1$，那么$\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}$的 Cholesky 分解即为下式，其中$\\boldsymbol{c}_{i} \\in \\mathbb{R}^{1 \\times |\\mathcal{S}| }, d_{i} \\geq 0$：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}^{\\top} $$\n结合上面两式：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{U}\\mathbf{U}^{\\top} \u0026amp; \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} \\\\ \\boldsymbol{c}_{i}\\mathbf{U}^{\\top} \u0026amp; \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} \\end{bmatrix} $$\n可得：\n$$ \\boldsymbol{u}_{i} = \\mathbf{U}\\boldsymbol{c}_{i}^{\\top}, 1 = \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} $$\n那么：\n$$ \\det(\\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}}) = \\det \\bigg(\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{i} \u0026amp; d_{i} \\end{bmatrix}^{\\top}\\bigg) = \\det(\\mathbf{U}\\mathbf{U}^{\\top})\\cdot d_{i}^{2}= \\det(\\mathbf{L}_{\\mathcal{S}}) \\cdot d_{i}^{2} $$\n这样我们一开始的优化目标就可以简化为：\n$$ j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log(d_{i}^{2}) $$\n接下来，当我们得到$d_j$时，便可以算出$c_{j}$，那么添加$j$之后的新集合$\\mathcal{S}'$的 Cholesky 分解便可以求得：\n$$ \\mathbf{L}_{\\mathcal{S}'} = \\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}^{\\top} $$\n增量更新 接下来便是重头戏，这一轮我们已经得到了最好的$d_{j}$了，下一轮我们怎么求出最大的$d_{i}$呢？\n可以利用之前求出的$c_{i}, d_{i}$来获取当前的$c_{i}', d_{i}'$，这便是论文的核心：增量更新\n在我们选择了$j$后，$c_{i}$多了一个元素，不妨记$\\boldsymbol{c}_{i}' = [\\boldsymbol{c}_{i}, {e}_{i}]$，回忆上面的式子：\n$$ \\mathbf{L}_{\\mathcal{S}\\cup \\{ i \\}} = \\begin{bmatrix} \\mathbf{L}_{\\mathcal{S} } \u0026amp; \\boldsymbol{u}_{i} \\\\ \\boldsymbol{u}_{i}^{\\top} \u0026amp; \\boldsymbol{u}_{i}^{\\top}\\boldsymbol{u} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{U}\\mathbf{U}^{\\top} \u0026amp; \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} \\\\ \\boldsymbol{c}_{i}\\mathbf{U}^{\\top} \u0026amp; \\boldsymbol{c}_{i}\\boldsymbol{c}_{i}^{\\top} + d_{i}^{2} \\end{bmatrix} $$\n也就是说，其中$\\boldsymbol{u}_{i}$就是第$i$个元素与集合$\\mathcal{S}$对应向量做内积的结果\n$$ \\boldsymbol{u}_{i} = \\mathbf{U}\\boldsymbol{c}_{i}^{\\top} $$\n那么，类比一下，$\\mathbf{U}_{j}$是$\\mathbf{L}_{\\mathcal{S}'}$的 Cholesky 分解\n$$ \\underbrace{\\begin{bmatrix} \\mathbf{U} \u0026amp; \\boldsymbol{0} \\\\ \\boldsymbol{c}_{j} \u0026amp; d_{j} \\end{bmatrix}}_{\\mathbf{U}_{j}} \\boldsymbol{c}_{i}'^{\\top} = \\boldsymbol{u}_{i}' = \\begin{bmatrix} \\boldsymbol{u}_{i} \\\\ \\mathbf{L}_{ji} \\end{bmatrix} $$\n继而：\n$$ \\langle \\boldsymbol{c}_{j},\\boldsymbol{c} _{i}\\rangle + d_{j}e_{i} = \\mathbf{L}_{ji} \\implies e_{i} = \\frac{\\mathbf{L}_{ji}-\\langle \\boldsymbol{c}_{j},\\boldsymbol{c}_{i} \\rangle }{d_{j}} $$\n求出$e_{i}$之后，我们便可以求出$d_{i}'$：\n$$ d_{i}'^{2} = 1 - \\|\\boldsymbol{c}_{i}'\\|_{2}^{2} =\\underbrace{ 1- \\|\\boldsymbol{c}_{i}\\|_{2}^{2} }_{ {\\color{#337dff}d_{i}^{2} }} - e_{i}^{2} = d_{i}^{2} - e_{i}^{2} $$\n流程 算法整体流程\n那我们来分析一下复杂度，每选一个$j$需要进行$|\\mathcal{R}| \\cdot |\\mathcal{S}|$次操作，而$|\\mathcal{R}|\\leq n, |\\mathcal{S}| \\leq k$，也就是$\\mathcal{O}(nk)$，得进行$k$次迭代，那么总的复杂度即为$\\mathcal{O}(nk^{2})$，由$\\mathcal{O}(nk^{4})$降到$\\mathcal{O}(nk^{2})$，是不错的进步\n代码 熟悉了整个流程之后，代码想必也是呼之欲出了\nimport math import numpy as np def fast_map_dpp(kernel_matrix, max_length): cis = np.zeros((max_length, kernel_matrix.shape[0])) di2s = np.copy(np.diag(kernel_matrix)) selected = np.argmax(di2s) selected_items = [selected] while len(selected_items) \u0026lt; max_length: idx = len(selected_items) - 1 ci_optimal = cis[:idx, selected] di_optimal = math.sqrt(di2s[selected]) elements = kernel_matrix[selected, :] eis = (elements - ci_optimal @ cis[:idx, :]) / di_optimal cis[idx, :] = eis di2s -= np.square(eis) di2s[selected] = -np.inf selected = np.argmax(di2s) selected_items.append(selected) return selected_items 这里实现比较有趣的点就是，尽管伪代码中是$i \\in \\mathcal{R}$，这里其实是全部算了，但他对已选的进行了后处理，置之为$-\\infty$\n接下来我们实操一下，从句子对匹配 BQ Corpus（Bank Question Corpus）拿出一条来看一下效果，首先是将其用预训练模型转换为表征向量，接着进行归一化操作，为了更好地看出 DPP 的效果，我们先用最大化内积来召回 50 个样本，再用 DPP 从这里召回 10 个具有多样性的样本：\n原句：我现在申请微粒货？ [\u0026#39;我现在申请微粒货？\u0026#39;, \u0026#39;申请微贷粒\u0026#39;, \u0026#39;申请微贷粒\u0026#39;, \u0026#39;我想申请微粒贷\u0026#39;, \u0026#39;可以么想申请微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我现在需要申请\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;, \u0026#39;我要求申请\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;开通微粒货\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;申请贷款\u0026#39;, \u0026#39;如何申请微粒贷\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;想办理微粒贷业务\u0026#39;, \u0026#39;申请贷款\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;可以么想申请微粒贷\u0026#39;, \u0026#39;开通微米粒\u0026#39;, \u0026#39;想开通\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;想开通\u0026#39;, \u0026#39;开通微粒貨\u0026#39;, \u0026#39;开通粒微贷\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;开通申请\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;开通微\u0026#39;] 可以看到有不少重复且意思一样的样本：\n接着看 DPP 的效果：\n[\u0026#39;我现在申请微粒货？\u0026#39;, \u0026#39;开通\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;] 可以发现里面没有重复的情况，而且语义具备多样性，而值得注意的是，此时就有一些和我们的原句意思不匹配的情况，在应用时可以自定义新的 kernel，让它同时注意相似性和多样性；或者可以对 DPP 的样本进行后处理等\nSliding Window 当$|\\mathcal{S}|$相当大的时候，就会有相似的样本开始出现，即超平行体会开始坍缩，不妨我们将$\\mathcal{S}$缩小成一个滑动窗口$\\mathcal{W}$，我们仅仅需要保证窗口内的样本具备多样性即可，即：\n$$ j = \\mathop{\\arg \\max}_{i \\in \\mathcal{R}} \\log \\det(\\mathbf{L}_{\\mathcal{W} \\cup \\{ i \\}}) - \\log \\det(\\mathbf{L}_{\\mathcal{W}}) $$\n推荐系统中有短序推荐（Short Sequence Recommendation）的说法，推荐的时候只考虑用户短期内的一些行为，而长序推荐会考虑一个较长时间跨度来进行推荐\nWindow size 的选择也是比较重要的，不妨看一些 demo：\n窗口 #不同样本 #重复样本 2 2 0 3 2 1 4 1 2 5 1 1 7 1 0 9 0 0 如果我们的目的是为了通过 Sliding Window 获取与直接 DPP 召回不一样的结果，窗口的大小要适当地小一些，然而小了导致看的范围小了，很有可能最后结果出现重复的情况，最好是将窗口设置到召回样本数目的$20\\% \\sim 30\\%$\n同时，为了防止样本重复，可以多召回一些，比较直觉的做法可以再加上一个 window 的大小，然后去重：\nw/o window [\u0026#39;开通\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;微粒貸申请\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;什么情况下才能申请微粒\u0026#39;, \u0026#39;你好我申请借款\u0026#39;, \u0026#39;我现在申请微粒货？\u0026#39;] w/ window [\u0026#39;开通\u0026#39;, \u0026#39;开通申请\u0026#39;, \u0026#39;怎么申请微粒货\u0026#39;, \u0026#39;何时才能申请啊\u0026#39;, \u0026#39;现在想获取资格\u0026#39;, \u0026#39;我要微粒贷\u0026#39;, \u0026#39;我可以申请微粒贷吗\u0026#39;, \u0026#39;可以申请开通吗\u0026#39;, \u0026#39;如何申请微粒\u0026#39;, \u0026#39;我现在申请微粒货？\u0026#39;] 可以看到会有 3 个不一样的样本，还是比较有效的\n","permalink":"http://yunpengtai.top/posts/fast-greedy-map-inference-for-dpp/","summary":"问题 先规定一些术语：记选中元素构成的集合为$\\mathcal{S}$，未选中构成的元素记为$\\mathcal{R}$，$\\mathbf{L}","title":"Fast Greedy MAP Inference for DPP"},{"content":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子集的质量高同时多样性好。比如在推荐系统中，我们就希望给用户推荐的东西尽可能的有质量，同时具有差异性。\n而使得采样的子集尽可能具备多样性便是行列式点过程（Determinantal Point Process）大展身手的地方，俗称 DPP\n边缘分布 首先引入 DPP 的边缘分布定义，当我们某次采样出子集$\\mathbf{A}$，「包括」$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$的概率：\n$$ P(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K_{V}}) $$\n$\\mathbf{K}$是核矩阵（Kernel Matrix），即：\n$$ \\mathbf{K}_{ij} = k(\\boldsymbol{v_{i}}, \\boldsymbol{v_{j}}) $$\n$\\mathbf{K_{V}}$是由$\\mathbf{V}$中元素构成的子矩阵，举个例子，假如$\\mathbf{V}=\\{ 1,2 \\}$，那么：\n$$ P(\\mathbf{V} \\subseteq \\mathbf{A}) = \\det(\\mathbf{K_{V}}) = \\left|\\begin{array}{cc} \\mathbf{K}_{11} \u0026amp; \\mathbf{K}_{12} \\\\ \\mathbf{K}_{21} \u0026amp; \\mathbf{K}_{22} \\end{array}\\right| = \\mathbf{K}_{11}\\mathbf{K}_{22} - \\mathbf{K}_{12}^{2} $$\n当$\\mathbf{K}_{12}$越大，则$\\{ 1,2 \\}$同时出现在$\\mathbf{V}$的概率就越小，从这个角度想，核函数应该是呈现出某种相似性\n从正定性出发，严格的定义如下是：$\\mathbf{0}\\preceq\\mathbf{K} \\preceq \\mathbf{I}$\n举个例子：\n$$ \\mathbf{K} = \\begin{bmatrix} 1 \u0026amp; -0.3 \\\\ -0.3 \u0026amp; 1 \\end{bmatrix} $$\n其特征值为$0.7, -1.3$，不满足$\\mathbf{K}-\\mathbf{0} \\succeq \\mathbf{0}$，即不是半正定矩阵\nL-Ensemble 然而，上面边缘定义只是告诉我们采样时，某个子集被「包括」的概率，并非就是这个子集，而这个问题可以通过 L-Ensemble 去解\n$$ P(\\mathbf{V}=\\mathbf{A}) \\propto \\det(\\mathbf{L}) $$\n这里的$\\mathbf{L}$省略了下标，跟上面的$\\mathbf{K}$一样，是跟$\\mathbf{V}$元素相关的子矩阵。$\\mathbf{L}$矩阵的核函数是内积是$\\boldsymbol{v_{i}^{\\top}\\boldsymbol{v_{j}}}$，$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$\n$$ \\mathbf{L} = \\mathbf{V}^{\\top}\\mathbf{V} = \\begin{bmatrix} \\langle \\boldsymbol{v_{1}}, \\boldsymbol{v_{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v_{1}},\\boldsymbol{v_{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v_{1}}, \\boldsymbol{v_{k}} \\rangle \\\\ \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v_{2}},\\boldsymbol{v_{k}} \\rangle \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{1}} \\rangle \u0026amp; \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{2}} \\rangle \u0026amp; \\dots \u0026amp; \\langle \\boldsymbol{v_{k}},\\boldsymbol{v_{k}} \\rangle \\end{bmatrix} $$\n注意，这里指的不是概率，而是说概率「正比于」$\\mathbf{L}$矩阵的行列式，那么如何计算概率呢？也就是说我们得计算一个归一化常数（normalization constant），可以类比抛硬币，我们得去求总的抛起次数，除以它才能得到概率\n引入下述定理：\n$$ \\sum_{\\mathbf{A}\\subseteq \\mathbf{V} \\subseteq \\mathbf{S}} \\det(\\mathbf{L}) = \\det(\\mathbf{L} + \\mathbf{I_{\\bar{A}}}) $$\n其中$\\mathbf{I_{\\bar{A}}}$是将单位矩阵中与$\\mathbf{A}$相关元素全部置零，举个例子，当$\\mathbf{S} = \\{ 1,2,3 \\}, \\mathbf{A}={1,2}$时：\n$$ \\mathbf{I_{\\bar{A}}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n那么如何求归一化常数呢，即将$\\mathbf{A}=\\emptyset$，当$\\mathbf{A}$为空集时，便包括了所有的情况，即：\n$$ P(\\mathbf{V}=\\mathbf{A}) = \\frac{\\det(\\mathbf{L})}{\\det(\\mathbf{L}+\\mathbf{I})} $$\n另外，L-Emsemble 的$\\mathbf{K}, \\mathbf{L}$对应关系如下：\n$$ \\begin{align} \\mathbf{K} \u0026amp; = \\mathbf{L}(\\mathbf{L} + \\mathbf{I})^{-1} \\\\ \\mathbf{L} \u0026amp; = \\mathbf{K}(\\mathbf{I}-\\mathbf{K})^{-1} \\end{align} $$\n直观解释 那么，行列式与多样性的直观解释是什么呢？\n多样性和相似性的意思正好相反，通常我们会定义相似性为两个向量之间做点积，即为$\\boldsymbol{v_{1}}^{\\top}\\boldsymbol{v_{2}}$，直观上看，两向量夹角的余弦值$\\cos \\theta$ 越大，相似性越高，反过来看，当$\\cos \\theta$最小即为两者相似性最差，多样性最好。显然，当两向量正交时多样性最好。\n那么，对于一个子集$\\mathbf{V} = [\\boldsymbol{v_{1}},\\boldsymbol{v_{2}},\\dots,\\boldsymbol{v_{k}}] \\in \\mathbb{R}^{{d \\times k}}$而言，该如何定义它的多样性呢？不难想出，可以通过线性无关向量的数量来定义，若两两都互不线性相关，此时的子集的多样性是最好的。直观上可以转换为构成的超平行体的体积，下方为$k=2,3$时的示意图\n图源自王树森老师的课程\n为什么呢？可以拿平行六面体为例，若其中一个向量与其他向量线性相关，那么则会坍缩成一个平面，构不成平行六面体\n图源自 3BlueBrown 对于行列式的介绍\n只有当所有向量两两都线性无关时，构成的超平行体体积最大，即多样性最好\n而行列式可以表示体积，下式中$\\text{vol}$代表体积（volume），此时$k=d$，$\\mathbf{V}$为方阵\n$$ \\det(\\mathbf{V})= \\text{vol}(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})) $$\n也就是说，我们可以通过行列式的大小来定义多样性\n那么，$\\mathbf{L}$的行列式是否也跟体积有关呢？答案是肯定的：\n$$ \\det(\\mathbf{L}) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots,\\boldsymbol{v_{k}})\\bigg)^{2} $$\n接下来证明这一结论：\n由于$k \\leq d$，因为$d$维空间至多存在$d$个两两线性无关的向量，那么肯定存在一个$k$维子空间$\\mathcal{H}$，存在正交矩阵$\\mathbf{R} \\in \\mathbb{R}^{d \\times d}$，对向量$\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}}$进行旋转，使得$\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}}$都落在子空间$\\mathcal{H}$上。不妨设$\\mathcal{H}$的基底是前$k$个标准正交基，那么：\n$$ \\mathbf{R}\\boldsymbol{v_{i}} = \\begin{bmatrix} \\boldsymbol{u_{i}} \\\\ \\mathbf{0} \\end{bmatrix} $$\n$\\boldsymbol{u_{i}} \\in \\mathbb{R}^{k}$，$\\mathbf{0}$一共有$d-k$个，因为用$\\mathcal{H}$的基底向量表示，后面只能为$0$，将$\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}},\\dots, \\boldsymbol{u_{k}}$当作$\\mathbf{U}$的列，就有：\n$$ \\mathbf{RV} = \\begin{bmatrix} \\mathbf{U} \\\\ \\mathbf{0} \\end{bmatrix}, \\mathbf{U} \\in \\mathbb{R}^{k \\times k} $$\n显然，$\\mathcal{P}(\\boldsymbol{u_{1}}, \\dots)$与$\\mathcal{P}([\\boldsymbol{u_{1}};\\boldsymbol{0}],\\dots )$两者体积相等\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}},\\dots, \\boldsymbol{u_{k}})) = \\text{vol}\\bigg(\\mathcal{P}\\bigg(\\begin{bmatrix} \\boldsymbol{u_{1}} \\\\ \\boldsymbol{0} \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{u_{2}} \\\\ \\boldsymbol{0} \\\\ \\end{bmatrix}, \\dots, \\begin{bmatrix} \\boldsymbol{u_{k}} \\\\ \\boldsymbol{0} \\end{bmatrix}\\bigg)\\bigg) $$\n那么：\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}}, \\dots, \\boldsymbol{u_{k}})) = \\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) $$\n由于对超平面体进行旋转不改变其体积（注意，这里是旋转而不是一般的线性变换，一般的线性变换不具备该性质）\n$$ \\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})\\bigg) $$\n那么：\n$$ \\text{vol}(\\mathcal{P}(\\boldsymbol{u_{1}}, \\boldsymbol{u_{2}}, \\dots, \\boldsymbol{u_{k}})) =\\text{vol}\\bigg(\\mathcal{P}(\\mathbf{R}\\boldsymbol{v_{1}}, \\mathbf{R}\\boldsymbol{v_{2}}, \\dots, \\mathbf{R}\\boldsymbol{v_{k}})\\bigg) $$\n又因为$\\mathbf{R}$正交矩阵，即$\\mathbf{R}^{\\top}\\mathbf{R} = \\mathbf{I}$，那么：\n$$ \\mathbf{U}^{\\top}\\mathbf{U} = (\\mathbf{RV})^{\\top}\\mathbf{RV} = \\mathbf{V}^{\\top}(\\mathbf{R}^{\\top}\\mathbf{R)}\\mathbf{V} = \\mathbf{V}^{\\top}\\mathbf{V} $$\n所以：\n$$ \\det(\\mathbf{V}^{\\top}\\mathbf{V}) = \\det(\\mathbf{U}^{\\top}\\mathbf{U}) = \\det(\\mathbf{U})^{2} = \\text{vol}\\bigg(\\mathcal{P}(\\boldsymbol{v_{1}}, \\boldsymbol{v_{2}}, \\dots, \\boldsymbol{v_{k}})\\bigg)^{2} $$\n从 L-Emsemble 角度看，被采样的概率正比于构成的超平面体的体积，即两两之间线性无关更容易被采样出\nDemo 接下来我们用例子来看一下是否 DPP 能够采样出更有多样性的子集\nfrom torch import det, eye from transformers import set_seed from transformers import BertModel, BertTokenizer set_seed(42) pretrain_path = \u0026#34;fabriceyhc/bert-base-uncased-imdb\u0026#34; model = BertModel.from_pretrained(pretrain_path).cuda() tk = BertTokenizer.from_pretrained(pretrain_path) input_text = [ \u0026#34;I am happy because the weather is extremely good!\u0026#34;, \u0026#34;I hate the bad weather\u0026#34;, \u0026#34;The weather is extremely good!\u0026#34;, ] inputs = tk(input_text, max_length=128, return_tensors=\u0026#34;pt\u0026#34;, truncation=True, padding=True) inputs = {k: v.cuda() for k, v in inputs.items()} outputs = model(**inputs).pooler_output.T vtv = outputs.T @ outputs group_12 = vtv[:2][:, [0, 1]] I = eye(2).cuda() p_12 = det(group_12) / det(group_12 + I) group_13 = vtv[[0, 2]][:, [0, 2]] p_13 = det(group_13) / det(group_13 + I) print(\u0026#39;采样到第一个和第二个的概率：%f\u0026#39;%p_12) print(\u0026#39;采样到第一个和第三个的概率：%f\u0026#39;%p_13) # 采样到第一个和第二个的概率：0.983567 # 采样到第一个和第三个的概率：0.923823 然而，对于一个大小为$n$的集合，一共有$2^{n}$种组合，如何快速地进行 DPP 的计算以及如何最快找到大小为$k$的多样性最大的子集是比较困难的，留给下一篇 post\n","permalink":"http://yunpengtai.top/posts/determinantal-point-process/","summary":"在机器学习中，我们通常会面临一个问题：给定一个集合$\\mathbf{S}$，从中寻找$k$个样本构成子集$\\mathbf{V}$，尽量使得子","title":"Determinantal Point Process"},{"content":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员\n$$ \\begin{equation} p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) \\end{equation} $$ 其中$\\eta$被称为分布的自然参数（natural parameter）或标准参数（canonical parameter）；而$T(y)$被称为统计充分量（sufficient statistic），通常而言：$T(y) = y$；$a(\\eta)$是对数配分函数（log partition）\n例子 接下来展示下伯努利分布和高斯分布都是指数族的一员\n期望为$\\phi$的伯努利分布且$y \\in \\{1, 0\\}$，那么：\n$$ p(y=1) = \\phi; p(y=0) = 1-\\phi $$\n接着进行转换：\n$$ \\begin{align} p(y;\\phi ) \u0026amp; = \\phi^{y}(1-\\phi )^{1-y} \\\\ \u0026amp;= \\exp \\bigg(\\log(\\phi^{y}(1-\\phi )^{1-y})\\bigg) \\\\ \u0026amp;= \\exp(y\\log \\phi + (1-y)\\log(1-\\phi )) \\\\ \u0026amp;= \\exp\\left( \\left( \\log \\frac{\\phi }{1-\\phi } \\right)y + \\log(1-\\phi) \\right) \\end{align} $$\n那么可以对比着指数族的定义，易得$b(y)=1$以及：\n$$ \\begin{align} \\eta \u0026amp; = \\log\\left( \\frac{\\phi }{1-\\phi } \\right) \\\\ e^{\\eta } \u0026amp; = \\frac{\\phi}{1-\\phi } \\\\ {\\color{red}e^{-\\eta }} \u0026amp; = \\frac{1-\\phi}{\\phi } = \\frac{1}{\\phi } - 1 \\\\ \\phi \u0026amp; = \\frac{1}{1+e^{-\\eta }} \\end{align} $$\n发现很有趣的一点，$\\phi(\\eta)$不就是逻辑回归中的sigmoid函数嘛，继续比对将其他的参数写完整：\n$$ \\begin{align} a(\\eta ) \u0026amp; = -\\log(1-\\phi ) = -\\log\\left( 1-\\frac{1}{1+e^{-\\eta }} \\right) \\\\ \u0026amp;= \\log (1+e^{\\eta }) \\end{align} $$\n接下来讨论高斯分布，$\\sigma^{2}$对$\\theta, h_{\\theta }(x)$是不影响的（相当于常数），为了简化表示，约定$\\sigma^{2}=1$\n$$ \\begin{align} p(y;\\mu ) \u0026amp; = \\frac{1}{\\sqrt{ 2\\pi }}\\exp\\left( -\\frac{(y-\\mu )^{2}}{2} \\right) \\\\ \u0026amp;= \\underbrace{ \\frac{1}{\\sqrt{ 2\\pi }} \\exp\\left( -\\frac{y^{2}}{2} \\right) }_{ b(y) }\\exp\\left( \\mu y - \\frac{\\mu^{2}}{2} \\right) \\end{align} $$\n比对定义，可以发现：\n$$ \\eta = \\mu; a(\\eta ) = -\\frac{\\eta^{2}}{2} $$\n性质 $$ p(y;\\eta ) = b(y)\\exp (\\eta^{\\mathbf{T}}T(y) - a(\\eta)) $$\n指数族分布的期望是$a(\\eta)$对$\\eta$的一阶微分 指数族分布的方差是$a(\\eta)$对$\\eta$的二阶微分 指数族分布的NLL loss是concave的 下面来证明上述观点：\n$$ \\begin{align} \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) \u0026amp; = b(y) \\exp(\\eta^{\\mathbf{T}}y - a(\\eta )) \\left( y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\right) \\\\ \u0026amp;= yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\end{align} $$\n那么：\n$$ y p(y;\\eta ) = \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) $$\n又因为：\n$$ \\begin{align} \\mathbb{E}[Y;\\eta ] \u0026amp; = \\mathbb{E}[Y|X;\\eta] \\\\ \u0026amp;= \\int yp(y;\\eta ) \\,dy \\\\ \u0026amp;= \\int \\frac{ \\partial }{ \\partial \\eta } p(y;\\eta ) + p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026amp;= \\int \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\, dy + \\int p(y;\\eta )\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\int p(y;\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } \\cdot 1 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\cdot 1 \\\\ \u0026amp;= 0 + \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta } a(\\eta) \\quad \\blacksquare \\end{align} $$\n下面来证明方差：\n$$ \\begin{align} \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) \u0026amp; = \\frac{ \\partial }{ \\partial \\eta } \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg) \\\\ \u0026amp;= y\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\\\ \u0026amp;= \\frac{ \\partial }{ \\partial \\eta }p(y;\\eta ) \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta )\\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\bigg(yp(y;\\eta ) - p(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta } a(\\eta )\\bigg)\\bigg(y- \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) \\bigg) - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= y^{2}p(y;\\eta ) - 2yp(y;\\eta ) \\frac{ \\partial }{ \\partial \\eta }a(\\eta ) + p(y;\\eta ) (\\frac{ \\partial }{ \\partial \\eta }a(\\eta ) )^{2} - p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) -p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) \\end{align} $$\n那么：\n$$ \\bigg(y - \\frac{ \\partial }{ \\partial \\eta }a(\\eta) \\bigg)^{2} p(y;\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } p(y;\\eta ) +p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta) $$\n又因为：\n$$ \\begin{align} \\mathbb{V}[Y;\\eta ] \u0026amp; = \\mathbb{V}[Y|X;\\eta] \\\\ \u0026amp;= \\int \\left( y - \\frac{ \\partial }{ \\partial \\eta } a(\\eta ) \\right)^{2} p(y;\\eta)\\, dy \\\\ \u0026amp;= \\int \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }p(y;\\eta) + p(y;\\eta ) \\frac{ \\partial^{2} }{ \\partial \\eta^{2} }a(\\eta ) \\, dy \\\\ \u0026amp;= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } \\int p(y;\\eta ) \\, dy + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta )\\int p(y;\\eta ) \\, dy \\\\ \u0026amp;= 0 + \\frac{ \\partial ^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\\\ \u0026amp;= \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) \\quad \\blacksquare \\end{align} $$\n接下来证明NLL Loss是凸函数，其中$a(\\eta) \\in \\mathbb{R}^{m}, \\mathbf{y} \\in \\mathbb{R}^{m}$：\n$$ \\begin{align} J(\\eta) \u0026amp; = -\\log \\sum_{i}^{m} p(y^{(i)};\\eta_{i} ) \\\\ \u0026amp;= -\\log \\sum_{i}^{m} b(y^{(i)})\\exp(\\eta_{i} T(y^{(i)}) - a(\\eta_{i})) \\\\ \u0026amp;= -\\log \\sum_{i}^{m} b(y^{(i)}) \\exp( \\eta_{i} y^{(i)}- a(\\eta_{i})) \\\\ \u0026amp;= a(\\eta) -\\sum_{i}^{m} \\log b(y^{(i)}) + \\eta_{i}y^{(i)} \\end{align} $$ 同时因为自身的协方差矩阵是「半正定」的：\n$$ \\begin{align} \\nabla_{\\eta }^{2} J(\\eta ) = \\frac{ \\partial^{2} }{ \\partial \\eta^{2} } a(\\eta ) = \\mathbb{V}[\\mathbf{y};\\eta ] \\implies PSD\\quad \\blacksquare \\end{align} $$\n由[[Critical Points#多变量]]可知，当Hessian矩阵是半正定时，NLL Loss是凸函数，有局部最小点\n构建GLM 在现实生活中，根据我们需要预测的变量来选取合适的分布，那么，如何构建模型去预测它呢？这里模型又被称为广义线性模型（Generalized Linear Model），要构建GLM，需要先进行一些假设：\n$y|x; \\theta \\sim \\mathrm{ExponentialFamily}(\\eta )$，也就是说，给定$x, \\theta$ 我们可以得到$y$的分布就是带有参数$\\eta$的指数族分布 给定$x$，我们需要去预测$y$，在指数族中，$y=T(y)$，也就是说我们得去预测期望，即学得的假设$h$需要去预测期望，即：$h(x) = \\mathbb{E}[y|x]$，这个在线性回归和逻辑回归都是满足的，举个逻辑回归的例子： $$ h_{\\theta }(x) = p(y=1|x;\\theta ) = 0 \\cdot p(y=0|x;\\theta) + 1 \\cdot p(y=1|x;\\theta) = \\mathbb{E}[y|x;\\theta ] $$ 自然参数$\\eta$与$x$的联系是线性的，即$\\eta = \\theta^{\\mathbf{T}}x$，若$\\eta \\in \\mathbb{R}^{n}$，则$\\eta_{i} = \\theta_{i}^{\\mathbf{T}}{x}$ 根据变量的特点来选择合适的分布：\nVariable Distribution Real Numbers $\\mathbb{R}$ Gaussian Binary Classification Bernoulli Count Poisson $\\mathbb{R}^{+}$ Gamma, Exponential Distribution Beta, Dirichlet GLM例子 接下来举一些GLM的例子\nOLS Ordinary Least Squares（线性回归）中的$y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，即服从高斯分布，其中$\\eta = \\mu$，那么： $$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[y|x;\\theta] \\\\ \u0026amp; = \\mu \\\\ \u0026amp;= \\eta \\\\ \u0026amp;= \\theta^{\\mathbf{T}}{x} \\end{align} $$ 第一个等号是因为第二个假设，第三个等号是根据指数族的定义来的，而第四个等号则是第三个假设\n逻辑回归 逻辑回归中$y \\in \\{0, 1\\}$，自然就想到了伯努利分布，即$y|x;\\theta \\sim \\text{Bernoulli}(\\phi )$，其中$\\phi=1/1+ e^{-\\eta }$，那么：\n$$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[y|x;\\theta ] \\\\ \u0026amp; = \\phi \\\\ \u0026amp;= \\frac{1}{1 + e^{-\\eta }} \\\\ \u0026amp;= \\frac{1}{1 + e^{-\\theta^{\\mathbf{T}}x}} \\end{align} $$ 是不是很神奇，那么关于为何逻辑回归中的假设函数取上述形式，又多了一种解释，即根据指数族分布和GLM的定义而来\n$g(\\eta) = \\mathbb{E}[T(y);\\eta ]$被称为响应函数（canonical response function），在深度学习中，常被称作为激活函数，而$g^{-1}$被称作为链接函数（canonical link function）。那么，对于高斯分布而言，响应函数就是单位函数；而对于伯努利分布而言，响应函数即为sigmoid函数（对于两个名词的定义，不同的文献可能相反）\nsoftmax回归 构建GLM 之前逻辑回归中是只有两类，当$y \\in \\{1, 2, \\dots, k\\}$，即现在是$k$分类，分布是multinomial distribution，接下来让我们构建GLM：\n规定$\\phi_{i}$规定了输出$y_{i}$的概率，那么$\\phi_{i}, \\dots, \\phi_{k-1}$即是我们的参数，那你肯定好奇为什么$\\phi_{k}$不是，因为输出所有类的概率之和为$1$，即$\\phi_{k}$可被其他的概率表示：\n$$ \\phi_{k} = 1-\\sum_{i}^{k-1} \\phi_{i} $$ 以往的$T(y)=y$，对于多分类而言，我们采用独热编码（one-hot），即：\n$$ T(1) = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, T(2) = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\dots ,T(k-1) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, T(k) = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} $$ 注意$T(y) \\in \\mathbb{R}^{k-1}$，因为$T(k)$定义为全零向量，那么如何表示$T(y)$的第$i$个元素呢？\n$$ (T(y))_{i} = \\mathbb{1}\\{y=i\\} $$\n接下来来构建GLM，写出其概率密度表示，注意：这里容易误以为是MLE中的所有概率相乘，然而当$y$取一个具体值时，只有一个指示函数为$1$，其他为$0$，即$\\phi_{i}^{0} = 1$\n$$ \\begin{align} p(y;\\phi ) \u0026amp; = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{\\mathbb{1}\\{y=k\\}}_{k} \\\\ \u0026amp; = \\phi^{\\mathbb{1}\\{y=1\\}}_{1} \\phi_{2}^{\\mathbb{1}\\{y=2\\}} \\dots \\phi^{1-\\sum_{i}^{k-1}\\mathbb{1}\\{y=i\\}}_{k} \\\\ \u0026amp;= \\phi_{1}^{(T(y))_{1}} \\phi_{2}^{(T(y)_{2})} \\dots \\phi_{k}^{1-\\sum_{i}^{k-1}(T(y))_{i}} \\\\ \\end{align} $$ 继续变形来跟定义做比较：\n$$ \\begin{align} p(y; \\phi)\u0026amp;= \\exp \\bigg((T(y))_{1}\\log \\phi_{1} + (T(y))_{2} \\log \\phi_{2} + \\dots +(1-\\sum_{i}^{k-1}(T(y))_{i})\\log \\phi_{k} \\bigg) \\\\ \u0026amp;= \\exp \\bigg((T(y))_{1}\\log \\frac{\\phi_{1}}{\\phi_{k}} + (T(y))_{2}\\log \\frac{\\phi_{2}}{\\phi_{k}} + \\dots+ (T(y))_{k-1}\\log \\frac{\\phi_{k-1}}{\\phi_{k}} + \\log \\phi_{k}\\bigg) \\end{align} $$\n那么：\n$$ \\eta = \\begin{bmatrix} \\log (\\phi_{1} / \\phi_{k}) \\\\ \\log (\\phi_{2} / \\phi_{k}) \\\\ \\vdots \\\\ \\log(\\phi_{k-1} / \\phi_{k}) \\end{bmatrix}, a(\\eta ) = -\\log(\\phi_{k}), b(y) =1 $$ 链接函数容易发现是：\n$$ \\eta_{i} = \\log \\frac{\\phi_{i}}{\\phi_{k}} $$ 接下来求响应函数：\n$$ \\begin{align} e^{\\eta_{i}} \u0026amp; = \\frac{\\phi_{i}}{\\phi_{k}} \\\\ \\phi_{k}e^{\\eta_{i}} \u0026amp; = \\phi_{i} \\\\ \\phi_{k}\\sum_{i}^{k} e^{\\eta_{i}} \u0026amp; = \\sum_{i}^{k} \\phi_{i} = 1 \\end{align} $$ 那么：\n$$ \\phi_{k} = \\frac{1}{\\sum_{i}^{k} e^{\\eta_{i}}} $$ 将$\\phi_{k}$代入上式：\n$$ \\phi_{i} = \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} $$ 这就是我们的激活函数，在深度学习中常被称为「softmax」函数，接下来便可构建GLM：\n$$ \\begin{align} p(y=i|x;\\theta ) \u0026amp; = \\phi_{i} \\\\ \u0026amp;= \\frac{e^{\\eta_{i}}}{\\sum_{j=1}^{k} e^{\\eta_{j}}} \\\\ \u0026amp;= \\frac{e^{\\theta_{i}^{\\mathbf{T}}x}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x}} \\end{align} $$ 多分类问题被看作是逻辑回归的推广版，又被称为「softmax regression」，我们的假设函数如下：\n$$ \\begin{align} h_{\\theta }(x) \u0026amp; = \\mathbb{E}[T(y)|x;\\theta ] \\\\ \u0026amp;= \\mathbb{E}\\left[\\begin{array}{c|} \\mathbb{1}\\{y=1\\} \\\\ \\mathbb{1}\\{y=2\\} \\\\ \\vdots \\\\ \\mathbb{1}\\{y=k-1\\} \\end{array} x;\\theta \\right] \\\\ \\end{align} $$\n又因为：\n$$ \\mathbb{E}[(T(y))_{i}] = \\phi_{i} $$ 为啥会这样呢？因为对于$(T(y))_{i}$只有两个可能，$1$或$0$，那么它的期望是不是：\n$$ \\mathbb{E}[(T(y))_{i}] = 1 \\cdot \\phi_{i} + 0 \\cdot (1-\\phi ) = \\phi_{i} $$\n$$ h_{\\theta }(x)= \\begin{bmatrix} \\phi_{1} \\\\ \\phi_{2} \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix}= \\begin{bmatrix} \\frac{\\exp ({\\theta_{1}^{\\mathbf{T}}x)}}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\frac{\\exp ({\\theta_{2}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\\\ \\vdots \\\\ \\frac{\\exp ({\\theta_{k-1}^{\\mathbf{T}}x})}{\\sum_{j=1}^{k} \\exp(\\theta_{j}^{\\mathbf{T}}x)} \\end{bmatrix} $$ 也就是说我们的假设函数需要输出每个类的概率，尽管只有$k-1$类，$\\phi_{k} = 1- \\sum_{i}^{k-1} \\phi_{i}$得到\n接下来进行最大似然估计并对$\\ell(\\theta)$进行化简：\n$$ \\begin{align} \\ell(\\theta ) \u0026amp; = \\sum_{i} \\log p(y^{(i)}|x^{(i)};\\theta ) \\\\ \u0026amp;= \\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026amp;= \\sum_{i}^{m} \\sum_{l=1}^{k} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} \\\\ \u0026amp;= \\sum_{i}^{m} \\sum_{l=1}^{k} {\\color{red}\\mathbb{1}\\{y^{(i)}=l\\}} \\log \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left( \\log e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}} \\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\left(\\theta_{l}^{\\mathbf{T}}x^{(i)}- \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\right) \\\\ \u0026amp;= \\sum_{i}^{m}\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)}-\\left( \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\underbrace{ \\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} }_{ 1 }\\right) \\\\ \u0026amp;= \\sum_{i}^{m} \\bigg(\\sum_{l=1}^{k} \\mathbb{1}\\{y^{(i)} = l\\} \\theta_{l}^{\\mathbf{T}}x^{(i)} - \\log \\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}\\bigg) \\end{align} $$\n上述化简主要利用了指示函数的性质以及$\\log$的运算法则，同时$\\theta \\in \\mathbb{R}^{k \\times n}$，我们利用布局法来求：\n$$ \\begin{align} \\frac{ \\partial \\ell(\\theta ) }{ \\partial \\theta_{pq} } \u0026amp; = \\sum_{i}^{m} \\mathbb{1}\\{y^{(i)} = p\\} x^{(i)}_{q} - \\frac{1}{\\sum_{j=1}^{k}e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}x^{(i)}_{q} \\\\ \u0026amp;= \\sum_{i}^{m} \\left( \\mathbb{1}\\{y^{(i)} = p\\} - \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)x^{(i)}_{q} \\end{align} $$ 因为这里是最大化$\\ell(\\theta)$，作为损失函数还应加个负号，这样才是最小，即\n$$ J(\\theta ) = -\\sum_{i} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} $$\n对应的微分如下： $$ \\frac{ \\partial J(\\theta ) }{ \\partial \\theta_{pq} } = \\sum_{i}^{m} \\left( \\frac{e^{\\theta_{p}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} - \\mathbb{1}\\{y^{(i)} = p\\} \\right)x^{(i)}_{q} $$\n交叉熵 我们常常称多分类的损失叫「交叉熵损失」（cross entropy loss），那么根据GLM推导的式子和交叉熵的联系是什么呢？\n联想交叉熵的定义：\n$$ H(P, Q) = - \\mathbb{E}_{x \\sim P}[\\log Q(x)] $$ 即使得模型输出的分布尽可能靠近训练集原来的分布：\n$$ \\theta^{\\ast} = \\mathop{\\arg \\min}_{\\theta} -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] $$ 我们接下来展开期望的计算：\n$$ -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log p_{model}(x)] = \\frac{1}{m}\\underbrace{ -\\sum_{i}^{m} \\log \\prod_{l=1}^{k} \\left( \\frac{e^{\\theta_{l}^{\\mathbf{T}}x^{(i)}}}{\\sum_{j=1}^{k} e^{\\theta_{j}^{\\mathbf{T}}x^{(i)}}} \\right)^{\\mathbb{1}\\{y^{(i)}=l\\}} }_{ J(\\theta) } $$ 两者其实就差一个常数，本质是一样的\n代码实现也比较轻松：\ndef CrossEntropy(y_pred, y_true): batch_size = y_pred.shape[0] y_pred = np.exp(y_pred) y_pred /= np.sum(y_pred, axis=1)[:, None] y_pred = np.take_along_axis(y_pred, y_true[:, None], axis=1) y_pred = np.log(y_pred) return -np.sum(y_pred) / batch_size ","permalink":"http://yunpengtai.top/posts/generalized-linear-models/","summary":"定义 若一个分布能够以下述方式进行表示，则称之为指数族（ Exponential Family）的一员 $$ \\begin{equation} p(y; \\eta ) = b(y)\\exp(\\eta^{\\mathbf{T}}T(y) - a(\\eta )) \\end{equation} $$ 其中$\\eta$被称为分布的自然参数（n","title":"Generalized Linear Models"},{"content":"鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！代码开源在此：\nDL-Tools Cache effective tools for deep learning. 用以存放深度学习有用的代码片段 Python 在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的服务器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份恩情值得一辈子铭记！\nWhy Parallel 我们在两种情况下进行并行化训练1：\n模型一张卡放不下：我们需要将模型不同的结构放置到不同的 GPU 上运行，这种情况叫ModelParallel(MP) 一张卡的 batch size(bs)过小：有些时候数据的最大长度调的比较高（e.g., 512），可用的 bs 就很小，较小的 bs 会导致收敛不稳定，因而将数据分发到多个 GPU 上进行并行训练，这种情况叫DataParallel(DP)。当然，DP 肯定还可以加速训练，常见于大模型的训练中 这里只讲一下 DP 在 pytorch 中的原理和相关实现，即 DataParallel 和 DistributedParallel\nData Parallel 实现原理 实现就是循环往复一个过程：数据分发，模型复制，各自前向传播，汇聚输出，计算损失，梯度回传，梯度汇聚更新，可以参见下图2：\nData Parallel 过程\npytorch 中部分关键源码3截取如下：\nData Parallel 源码 def data_parallel( module, input, device_ids, output_device=None ): if not device_ids: return module(input) if output_device is None: output_device = device_ids[0] # 复制模型 replicas = nn.parallel.replicate(module, device_ids) # 拆分数据 inputs = nn.parallel.scatter(input, device_ids) replicas = replicas[:len(inputs)] # 各自前向传播 outputs = nn.parallel.parallel_apply(replicas, inputs) # 汇聚输出 return nn.parallel.gather(outputs, output_device) 代码使用 因为运行时会将数据平均拆分到 GPU 上，所以我们准备数据的时候， batch size = per_gpu_batch_size * n_gpus\n同时，需要注意主 GPU 需要进行汇聚等操作，因而需要比单卡运行时多留出一些空间\nimport torch.nn as nn # device_ids 默认所有可使用的设备 # output_device 默认cuda:0 net = nn.DataParallel(model, device_ids=[0, 1, 2], output_device=None, dim=0) # input_var can be on any device, including CPU output = net(input_var) 接下来看个更详细的例子4，需要注意的是被 DP 包裹之后涉及到模型相关的，需要调用 DP.module，比如加载模型\nDP 例子 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() # for convenience self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\u0026#34;\\tIn Model: input size\u0026#34;, input.size(), \u0026#34;output size\u0026#34;, output.size()) return output bs, input_size, output_size = 6, 8, 10 # define inputs inputs = torch.randn((bs, input_size)).cuda() model = Model(input_size, output_size) if torch.cuda.device_count() \u0026gt; 1: print(\u0026#34;Let\u0026#39;s use\u0026#34;, torch.cuda.device_count(), \u0026#34;GPUs!\u0026#34;) # dim = 0 [6, xxx] -\u0026gt; [2, ...], [2, ...], [2, ...] on 3 GPUs model = nn.DataParallel(model) # 先 DataParallel，再 cuda model = model.cuda() outputs = model(inputs) print(\u0026#34;Outside: input size\u0026#34;, inputs.size(), \u0026#34;output_size\u0026#34;, outputs.size()) # assume 2 GPUS are available # Let\u0026#39;s use 2 GPUs! # In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10]) # In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10]) # Outside: input size torch.Size([6, 8]) output_size torch.Size([6, 10]) # save the model torch.save(model.module.state_dict(), PATH) # load again model.module.load_state_dict(torch.load(PATH)) # do anything you want 如果经常使用 huggingface，这里有两个误区需要小心：\n# data parallel object has no save_pretrained model = xxx.from_pretrained(PATH) model = nn.DataParallel(model).cuda() model.save_pretrained(NEW_PATH) # error # 因为 model 被 DP wrap 了，得先取出模型 # model.module.save_pretrained(NEW_PATH) # HF实现貌似是返回 N 个 loss （N 为 GPU 数量） # 然后对 N 个 loss 取 mean outputs = model(**inputs) loss, logits = outputs.loss, outputs.logits loss = loss.mean() loss.backward() # 返回的 logits 是汇聚后的 # HF 实现和我们手动算 loss 有细微差异 # 手动算略好于 HF loss2 = loss_fct(logits, labels) assert loss != loss2 # True 显存不均匀 了解前面的原理后，就会明白为什么会显存不均匀。因为 GPU0 比其他 GPU 多了汇聚的工作，得留一些显存，而其他 GPU 显然是不需要的。那么，解决方案就是让其他 GPU 的 batch size 开大点，GPU0 维持原状，即不按照默认实现的平分数据\n首先我们继承原来的 DataParallel（此处参考5），这里我们给定第一个 GPU 的 bs 就可以，这个是实际的 bs 而不是乘上梯度后的。假如你想要总的 bs 为 64，梯度累积为 2，一共 2 张 GPU，而一张最多只能 18，那么保险一点 GPU0 设置为 14，GPU1 是 18，也就是说你 DataLoader 每个 batch 大小是 32，gpu0_bsz=14\nclass BalancedDataParallel(DataParallel): def __init__(self, gpu0_bsz, *args, **kwargs): self.gpu0_bsz = gpu0_bsz super().__init__(*args, **kwargs) 核心代码就在于我们重新分配 chunk_sizes，实现思路就是将总的减去第一个 GPU 的再除以剩下的设备，源码的话有些死板，用的时候不妨参考我的6\n修改后的 scatter 代码 def scatter(self, inputs, kwargs, device_ids): # 不同于源码，获取 batch size 更加灵活 # 支持只有 kwargs 的情况，如 model(**inputs) if len(inputs) \u0026gt; 0: bsz = inputs[0].size(self.dim) elif kwargs: bsz = list(kwargs.values())[0].size(self.dim) else: raise ValueError(\u0026#34;You must pass inputs to the model!\u0026#34;) num_dev = len(self.device_ids) gpu0_bsz = self.gpu0_bsz # 除第一块之外每块GPU的bsz bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1) if gpu0_bsz \u0026lt; bsz_unit: # adapt the chunk sizes chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - 1) delta = bsz - sum(chunk_sizes) # 补足偏移量 # 会有显存溢出的风险，因而最好给定的 bsz 是可以整除的 # e.g., 总的 =52 =\u0026gt; bsz_0=16, bsz_1=bsz_2=18 # 总的 =53 =\u0026gt; bsz_0=16, bsz_1=19, bsz_2=18 for i in range(delta): chunk_sizes[i + 1] += 1 if gpu0_bsz == 0: chunk_sizes = chunk_sizes[1:] else: return super().scatter(inputs, kwargs, device_ids) return scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim) 优缺点 优点：便于操作，理解简单 缺点：GPU 分配不均匀；每次更新完都得销毁线程（运行程序后会有一个进程，一个进程可以有很多个线程）重新复制模型，因而速度慢 Distributed Data Parallel 实现原理 与 DataParallel 不同的是，Distributed Data Parallel 会开设多个进程而非线程，进程数 = GPU 数，每个进程都可以独立进行训练，也就是说代码的所有部分都会被每个进程同步调用，如果你某个地方 print 张量，你会发现 device 的差异 sampler 会将数据按照进程数切分，确保不同进程的数据不同 每个进程独立进行前向训练 每个进程利用 Ring All-Reduce 进行通信，将梯度信息进行聚合 每个进程同步更新模型参数，进行新一轮训练 按进程切分 如何确保数据不同呢？不妨看看 DistributedSampler 的源码\nDistributedSampler 的源码 # 判断数据集长度是否可以整除 GPU 数 # 如果不能，选择舍弃还是补全，进而决定总数 # If the dataset length is evenly divisible by # of replicas # then there is no need to drop any data, since the dataset # will be split equally. if (self.drop_last and len(self.dataset) % self.num_replicas != 0): # num_replicas = num_gpus self.num_samples = math.ceil((len(self.dataset) - self.num_replicas) /self.num_replicas) else: self.num_samples = math.ceil(len(self.dataset) / self.num_replicas) self.total_size = self.num_samples * self.num_replicas # 根据是否 shuffle 来创建 indices if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) indices = torch.randperm(len(self.dataset), generator=g).tolist() else: indices = list(range(len(self.dataset))) if not self.drop_last: # add extra samples to make it evenly divisible padding_size = self.total_size - len(indices) if padding_size \u0026lt;= len(indices): # 不够就按 indices 顺序加 # e.g., indices 为 [0, 1, 2, 3 ...]，而 padding_size 为4 # 加好之后的 indices[..., 0, 1, 2, 3] indices += indices[:padding_size] else: indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size] else: # remove tail of data to make it evenly divisible. indices = indices[:self.total_size] assert len(indices) == self.total_size # subsample # rank 代表进程 id indices = indices[self.rank:self.total_size:self.num_replicas] return iter(indices) Ring All-Reduce 那么什么是Ring All-Reduce呢？又为啥可以降低通信成本呢？\n首先将每块 GPU 上的梯度拆分成四个部分，比如$g_0 = [a_0; b_0; c_0; d_0]$，如下图（此部分原理致谢下王老师，讲的很清晰7）：\nRing All-Reduce 1\n所有 GPU 的传播都是同步进行的，传播的规律有两条：\n只与自己下一个位置的 GPU 进行通信，比如 0 \u0026gt; 1，3 \u0026gt; 0 四个部分，哪块 GPU 上占的多，就由该块 GPU 往它下一个传，初始从主节点传播，即 GPU0，你可以想象跟接力一样，a 传 b，b 负责传给 c 第一次传播如下：\nRing All-Reduce 2\n那么结果就是：\nRing All-Reduce 3\n那么，按照谁多谁往下传的原则，此时应该是 GPU1 往 GPU2 传 a0 和 a1，GPU2 往 GPU3 传 b1 和 b2，以此类推\nRing All-Reduce 4\n接下来再传播就会有 GPU3 a 的部分全有，GPU0 上 b 的部分全有等，就再往下传\nRing All-Reduce 5\n再来几遍便可以使得每块 GPU 上都获得了来自其他 GPU 的梯度啦\nRing All-Reduce 6\n代码使用 基础概念 第一个是后端的选择，即数据传输协议，从下表可以看出8，当使用 CPU 时可以选择gloo而 GPU 则可以是nccl\nBackend gloo mpi nccl Device CPU GPU CPU GPU CPU GPU send ✓ ✘ ✓ ? ✘ ✓ recv ✓ ✘ ✓ ? ✘ ✓ broadcast ✓ ✓ ✓ ? ✘ ✓ all_reduce ✓ ✓ ✓ ? ✘ ✓ reduce ✓ ✘ ✓ ? ✘ ✓ all_gather ✓ ✘ ✓ ? ✘ ✓ gather ✓ ✘ ✓ ? ✘ ✓ scatter ✓ ✘ ✓ ? ✘ ✘ reduce_scatter ✘ ✘ ✘ ✘ ✘ ✓ all_to_all ✘ ✘ ✓ ? ✘ ✓ barrier ✓ ✘ ✓ ? ✘ ✓ 接下来是一些参数的解释9：\nArg Meaning group 一次发起的所有进程构成一个 group，除非想更精细通信，创建 new_group world_size 一个 group 中进程数目，即为 GPU 的数量 rank 进程 id，主节点 rank=0，其他的在 0 和 world_size-1 之间 local_rank 进程在本地节点/机器的 id 举个例子，假如你有两台服务器（又被称为 node），每台服务器有 4 张 GPU，那么，world_size 即为 8，rank=[0, 1, 2, 3, 4, 5, 6, 7], 每个服务器上的进程的 local_rank 为[0, 1, 2, 3]\n然后是初始化方法的选择，有TCP和共享文件两种，一般指定 rank=0 为 master 节点\nTCP 显而易见是通过网络进行传输，需要指定主节点的 ip（可以为主节点实际 IP，或者是 localhost）和空闲的端口\nimport torch.distributed as dist dist.init_process_group(backend, init_method=\u0026#39;tcp://ip:port\u0026#39;, rank=rank, world_size=world_size) 共享文件的话需要手动删除上次启动时残留的文件，加上官方有一堆警告，还是建议使用 TCP\ndist.init_process_group(backend, init_method=\u0026#39;file://Path\u0026#39;, rank=rank, world_size=world_size) launch 方法 初始化 这里先讲用 launch 的方法，关于 torch.multiprocessing 留到后面讲\n在启动后，rank 和 world_size 都会自动被 DDP 写入环境中，可以提前准备好参数类，如argparse这种\nargs.rank = int(os.environ[\u0026#39;RANK\u0026#39;]) args.world_size = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) args.local_rank = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) 首先，在使用distributed包的任何其他函数之前，按照 tcp 方法进行初始化，需要注意的是需要手动指定一共可用的设备CUDA_VISIBLE_DEVICES\nDDP launch 源码 def dist_setup_launch(args): # tell DDP available devices [NECESSARY] os.environ[\u0026#39;CUDA_VISIBLE_DEVICES\u0026#39;] = args.devices args.rank = int(os.environ[\u0026#39;RANK\u0026#39;]) args.world_size = int(os.environ[\u0026#39;WORLD_SIZE\u0026#39;]) args.local_rank = int(os.environ[\u0026#39;LOCAL_RANK\u0026#39;]) dist.init_process_group(args.backend, args.init_method, rank=args.rank, world_size=args.world_size) # this is optional, otherwise you may need to specify the # device when you move something e.g., model.cuda(1) # or model.to(args.rank) # Setting device makes things easy: model.cuda() torch.cuda.set_device(args.rank) print(\u0026#39;The Current Rank is %d | The Total Ranks are %d\u0026#39; %(args.rank, args.world_size)) DistributedSampler 接下来创建 DistributedSampler，是否 pin_memory，根据你本机的内存决定。pin_memory 的意思是提前在内存中申请一部分专门存放 Tensor。假如说你内存比较小，就会跟虚拟内存，即硬盘进行交换，这样转义到 GPU 上会比内存直接到 GPU 耗时。\n因而，如果你的内存比较大，可以设置为 True；然而，如果开了导致卡顿的情况，建议关闭\n加载 DataLoader from torch.utils.data import DataLoader, DistributedSampler train_sampler = DistributedSampler(train_dataset, seed=args.seed) train_dataloader = DataLoader(train_dataset, pin_memory=True, shuffle=(train_sampler is None), batch_size=args.per_gpu_train_bs, num_workers=args.num_workers, sampler=train_sampler) eval_sampler = DistributedSampler(eval_dataset, seed=args.seed) eval_dataloader = DataLoader(eval_dataset, pin_memory=True, batch_size=args.per_gpu_eval_bs, num_workers=args.num_workers, sampler=eval_sampler) 加载模型 然后加载模型，跟 DataParallel 不同的是需要提前放置到 cuda 上，还记得上面关于设置 cuda_device 的语句嘛，因为设置好之后每个进程只能看见一个 GPU，所以直接model.cuda()，不需要指定 device\n同时，我们必须给 DDP 提示目前是哪个 rank\nfrom torch.nn.parallel import DistributedDataParallel as DDP model = model.cuda() # tell DDP which rank model = DDP(model, find_unused_parameters=True, device_ids=[rank]) 注意，当模型带有 Batch Norm 时：\nif args.syncBN: nn.SyncBatchNorm.convert_sync_batchnorm(model).cuda() 训练相关 每个 epoch 开始训练的时候，记得用 sampler 的 set_epoch，使得每个 epoch 打乱顺序是不一致的\n关于梯度回传和参数更新，跟正常情况无异\nfor epoch in range(epochs): # record epochs train_dataloader.sampler.set_epoch(epoch) outputs = model(inputs) loss = loss_fct(outputs, labels) loss.backward() optimizer.step() optimizer.zero_grad() 这里有一点需要小心，这个 loss 是各个进程的 loss 之和，如果想要存储每个 step 平均损失，可以进行 all_reduce 操作，进行平均，不妨看官方的小例子来理解下：\n\u0026gt;\u0026gt;\u0026gt; # All tensors below are of torch.int64 type. \u0026gt;\u0026gt;\u0026gt; # We have 2 process groups, 2 ranks. \u0026gt;\u0026gt;\u0026gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank \u0026gt;\u0026gt;\u0026gt; tensor tensor([1, 2]) # Rank 0 tensor([3, 4]) # Rank 1 \u0026gt;\u0026gt;\u0026gt; dist.all_reduce(tensor, op=ReduceOp.SUM) \u0026gt;\u0026gt;\u0026gt; tensor tensor([4, 6]) # Rank 0 tensor([4, 6]) # Rank 1 @torch.no_grad() def reduce_value(value, average=True): world_size = get_world_size() if world_size \u0026lt; 2: # 单 GPU 的情况 return value dist.all_reduce(value) if average: value /= world_size return value 看到这，肯定有小伙伴要问，那这样我们是不是得先求平均损失再回传梯度啊，不用，因为，当我们回传 loss 后，DDP 会自动对所有梯度进行平均10，也就是说回传后我们更新的梯度和 DP 或者单卡同样 batch 训练都是一致的\nloss = loss_fct(...) loss.backward() # 注意在 backward 后面 loss = reduce_value(loss, world_size) mean_loss = (step * mean_loss + loss.item()) / (step + 1) 还有个注意点就是学习率的变化，这个是和 batch size 息息相关的，如果 batch 扩充了几倍，也就是说 step 比之前少了很多，还采用同一个学习率，肯定会出问题的，这里，我们进行线性增大11\nN = world_size lr = args.lr * N 肯定有人说，诶，你线性增大肯定不能保证梯度的 variance 一致了，正确的应该是正比于$\\sqrt{N}$，关于这个的讨论不妨参考12\nevaluate 相关 接下来，细心的同学肯定好奇了，如果验证集也切分了，metric 怎么计算呢？此时就需要咱们把每个进程得到的预测情况集合起来，t 就是一个我们需要 gather 的张量，最后将每个进程中的 t 按照第一维度拼接，先看官方小例子来理解 all_gather\n\u0026gt;\u0026gt;\u0026gt; # All tensors below are of torch.int64 dtype. \u0026gt;\u0026gt;\u0026gt; # We have 2 process groups, 2 ranks. \u0026gt;\u0026gt;\u0026gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)] \u0026gt;\u0026gt;\u0026gt; tensor_list [tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1 \u0026gt;\u0026gt;\u0026gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank \u0026gt;\u0026gt;\u0026gt; tensor tensor([1, 2]) # Rank 0 tensor([3, 4]) # Rank 1 \u0026gt;\u0026gt;\u0026gt; dist.all_gather(tensor_list, tensor) \u0026gt;\u0026gt;\u0026gt; tensor_list [tensor([1, 2]), tensor([3, 4])] # Rank 0 [tensor([1, 2]), tensor([3, 4])] # Rank 1 def sync_across_gpus(t, world_size): gather_t_tensor = [torch.zeros_like(t) for _ in range(world_size)] dist.all_gather(gather_t_tensor, t) return torch.cat(gather_t_tensor, dim=0) 可以简单参考我前面提供的源码的 evaluate 部分，我们首先将预测和标签比对，把结果为 bool 的张量存储下来，最终 gather 求和取平均。\n这里还有个有趣的地方，tensor 默认的类型可能是 int，bool 型的 res 拼接后自动转为 0 和 1 了，另外 bool 型的张量是不支持 gather 的\ndef eval(...): results = torch.tensor([]).cuda() for step, (inputs, labels) in enumerate(dataloader): outputs = model(inputs) res = (outputs.argmax(-1) == labels) results = torch.cat([results, res], dim=0) results = sync_across_gpus(results, world_size) mean_acc = (results.sum() / len(results)).item() return mean_acc 模型保存与加载 模型保存，参考部分官方教程13，我们只需要在主进程保存模型即可，注意，这里是被 DDP 包裹后的，DDP 并没有 state_dict，这里 barrier 的目的是为了让其他进程等待主进程保存模型，以防不同步\ndef save_checkpoint(rank, model, path): if is_main_process(rank): # All processes should see same parameters as they all # start from same random parameters and gradients are # synchronized in backward passes. # Therefore, saving it in one process is sufficient. torch.save(model.module.state_dict(), path) # Use a barrier() to keep process 1 waiting for process 0 dist.barrier() 加载的时候别忘了 map_location，我们一开始会保存模型至主进程，这样就会导致 cuda:0 显存被占据，我们需要将模型 remap 到其他设备\ndef load_checkpoint(rank, model, path): # remap the model from cuda:0 to other devices map_location = {\u0026#39;cuda:%d\u0026#39; % 0: \u0026#39;cuda:%d\u0026#39; % rank} model.module.load_state_dict( torch.load(path, map_location=map_location) ) 进程销毁 运行结束后记得销毁进程：\ndef cleanup(): dist.destroy_process_group() cleanup() 如何启动 在终端输入下列命令【单机多卡】\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS main.py (--arg1 --arg2 --arg3 and all other arguments of your training script) 目前 torch 1.10 以后更推荐用 run\ntorch.distributed.launch -\u0026gt; torch.distributed.run / torchrun 多机多卡是这样的：\n# 第一个节点启动 python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=0 \\ --master_addr=\u0026#34;192.168.1.1\u0026#34; \\ --master_port=1234 main.py # 第二个节点启动 python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=1 \\ --master_addr=\u0026#34;192.168.1.1\u0026#34; \\ --master_port=1234 main.py mp 方法 第二个方法就是利用 torch 的多线程包\nimport torch.multiprocessing as mp # rank mp 会自动填入 def main(rank, arg1, ...): pass if __name__ == \u0026#39;__main__\u0026#39;: mp.spawn(main, nprocs=TOTAL_GPUS, args=(arg1, ...)) 这里输入参数时务必注意要给定 iterable 形式，比如你的 main 方法除了 rank 还接受一个 int 类型参数，你 spawn 方法时需要以元组形式给定：\nif __name__ == \u0026#39;__main__\u0026#39;: # Note that is (1, ) not (1) mp.spawn(main, nprocs=TOTAL_GPUS, args=(1, )) 同时因为此种方法是自动给定 rank，我们不需要在初始化时告诉可用的设备有哪些，否则多个进程无法同时启动\ndef dist_setup_mp(args): # there is no need to set CUDA_VISIBLE_DEVICES # os.environ[\u0026#39;CUDA_VISIBLE_DEVICES\u0026#39;] = args.devices 这种运行的时候就跟正常的 python 文件一致：python main.py\n优缺点 优点： 相比于 DP 而言，不需要反复创建和销毁线程；Ring All-Reduce 算法提高通信效率；模型同步方便 缺点：操作起来可能有些复杂，一般可满足需求的可先试试看 DataParallel Possible Bugs Producer process has been terminated before all shared CUDA tensors released. See Note Sharing CUDA tensors\n这个应该是用 mp 方法时在 DataLoader 里设置 num_workers 大于导致，应该是 torch 的 multiprocessing 会每一个 num_worker 复制一个模型14\nRuntimeError: cannot pin ‘torch.cuda.ByteTensor’ only dense CPU tensors can be pinned\n还记得我们前面对于pin_memory的解释，是在内存中专门开辟一块空间，专门用于与 GPU 的通信，那么没到 GPU 之前就是 CPU，也就是 CPU tensors 才能被 pin\n出现这个 bug 的原因可能是你在 DataLoader 内部处理时已经将张量放到了 GPU 上15，比如自定义collate_func\nhttps://blog.csdn.net/qq_37541097/article/details/109736159\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.cnblogs.com/ljwgis/p/15471530.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html?highlight=dataparallel\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/kimiyoung/transformer-xl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/sherlcok314159/dl-tools/blob/main/balanced_data_parallel/README.md\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.youtube.com/watch?v=rj-hjS5L8Bw\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pytorch.org/docs/stable/distributed.html#backends\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/58271635/in-distributed-computing-what-are-world-size-and-rank\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discuss.pytorch.org/t/average-loss-in-dp-and-ddp/93306/4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://arxiv.org/abs/1706.02677\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/Lightning-AI/lightning/discussions/3706\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pytorch.org/tutorials/intermediate/ddp_tutorial.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discuss.pytorch.org/t/w-cudaipctypes-cpp-22-producer-process-has-been-terminated-before-all-shared-cuda-tensors-released-see-note-sharing-cuda-tensors/124445/1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://discuss.pytorch.org/t/what-are-dense-cpu-tensor/55703\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://yunpengtai.top/posts/dive-in-distributed-training/","summary":"鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！代码开源在此： DL-Tools Cache effective tools for deep","title":"Diving in distributed training in PyTorch"},{"content":"1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.\n$$ \\boldsymbol{w}^{\\tau + 1} = \\boldsymbol{w}^{\\tau} - \\eta \\nabla_{\\boldsymbol{w}^{\\tau}} E \\tag{1.1} $$\nwhere $\\eta, \\tau, E$ label learning rate ($\\eta \u0026gt; 0$), the iteration step and the loss function. Wait! But why is the negative gradient?\n2. Why negative gradient The function increases the most sharply by following the direction of the gradient.\nThe below is an example. The three-dimensional plane is $z = F(x, y)$. The black point is on the plane. You can try to move the point to see how the arrow changes. Interestingly, the arrow always points to the direction which leads to the biggest increase of the function value. Note that when you move one step, the gradient just changes. Thus if you still want to increase the function value in the most sharp way, another computation is needed.\nThe starting point of the arrow is the mapping of the black point to the $xoy$ plane. The arrow is parallel to the gradient.\nLet us use another graph to better understand what the mapping means. The left graph is contour plot while the right is the plane. The red point is just the mapping of the black point to $xoy$ plane. The blue arrow is just the direction of the gradient. And you can move the point to feel about it.\nThat is the intuitive way to feel about the gradient. Furthermore, we can just try to prove it. from the question in stackexchange\nConsider a Taylor Expansion:\n$$ \\begin{aligned} F(\\boldsymbol{r_0} + \\boldsymbol{r}) \u0026amp;= F(\\boldsymbol{r_0}) + \\nabla_{r_0}F \\cdot \\boldsymbol{r} \\\\ \u0026amp;= F(\\boldsymbol{r_0}) + \\|\\nabla_{r_0}F\\|\\cdot \\|\\boldsymbol{r}\\| \\cdot \\cos \\theta \\end{aligned}\\tag{2.1} $$\nWhen you decide to move a small step, the two magnitudes are certain. If $\\theta=0$, you can maximize the function value (i.e. in the direction of gradient).\nThus if we want to minimize our loss function, we need to go in the opposite direction of the gradient. That is why we need a negative gradient. Also, note that Taylor Expansion only applies to small $\\Delta x$ which requires $\\eta$ to be small (e.g. $2 \\times 10^{-5}, 5 \\times 10^{-5}$).\nBut how to compute the gradients needs a powerful technique: back-propagation.\n3. Definition of back-propagation Back-propagation allows information from the cost to then flow backwards through the network, in order to compute the gradients used to adjust the parameters.\nBack-propagation can be new to the novices, but it does exist in the life widely. For instance, the loss can be your teacher\u0026rsquo;s attitude towards you. If you fail in one examination, your teacher can be disappointed with you. Then, he can tell your parents about your failure. Your parents then ask you to work harder to win the examination.\nYour parents can be seen as hidden units in the neural network, and you are the parameter of the network. Your teacher\u0026rsquo;s bad attitude towards your failure can ask you to make adjustments: working harder. Similarly, the loss can require the parameters to make adjustments via gradients.\n4. Chain Rule Suppose $z = f(y), y = g(x) \\implies z = (f \\circ g)(x)$, how to calculate the derivative of $z$ with respect to $x$? The chain rule of calculus is used to compute the derivatives of functions formed by composing other functions whose derivatives are known.\n$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} \\tag{4.1} $$\n5. Case Study Fully-connected layer\nLet\u0026rsquo;s first see an important example. In fully connected layers, one input neuron sends information (i.e., multiplied by weights) to every output neuron. Denote $w_{ji}$ as the weight from $x_i$ to $y_j$. Then for every output neuron (e.g., $y_j$), it accepts the information sent by every input neuron:\n$$ y_{j}= \\sum\\limits_{i} w_{ji} x_{i} \\tag{5.1} $$\nThen the partial derivative of $y_j$ with respect to $x_i$:\n$$ \\frac{\\partial y_j}{\\partial x_{i}}= w_{ji} \\tag{5.2} $$\nLet\u0026rsquo;s see another example which is represented by the computational graph below. Bishop-Pattern-Recognition-and-Machine-Learning-2006\nAnother example\nAnd we can perform a forward propagation according to the computational graph.\n$$ \\begin{align} h_{j} \u0026amp;= \\sum\\limits_{i} w_{ji}^{(1)} x_{i} \\tag{5.3} \\\\ z_{j} \u0026amp;= f(h_{j}) \\tag{5.4} \\\\ y_{k} \u0026amp;= \\sum\\limits_{j}w_{kj}^{(2)} z_{j} \\tag{5.5} \\end{align} $$\nwhere\n$$ f(h) = \\tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} \\tag{5.6} $$\nA useful feature of this activation is that its derivative can be expressed in a particularly simple form:\n$$ f'(h) = 1 - f(h)^2 \\tag{5.7} $$\nThe error function can be mean squared errors:\n$$ E(\\boldsymbol{w}) = \\frac{1}{2} \\sum\\limits_{k}(y_{k}- \\hat{y}_k)^2 \\tag{5.8} $$\nIf we want to update the parameters, we need first to compute the partial derivative of $E(\\boldsymbol{w})$ with respect to them.\n$$ \\frac{\\partial E(\\mathbf{w})}{\\partial w_{kj}^{(2)}} = \\frac{\\partial E(\\mathbf{w})}{\\partial y_{k}} \\frac{\\partial y_k}{\\partial w_{kj}^{(2)}} = (y_{k}- \\hat{y}_k)z_j \\tag{5.9} $$\n$$ \\begin{align} \\frac{\\partial E(\\boldsymbol{w})}{\\partial w_{ji}^{(1)}} \u0026amp;= \\frac{\\partial E(\\boldsymbol{w})}{\\partial h_{j}}\\frac{\\partial h_j}{\\partial w_{ji}^{(1)}} = \\left(\\frac{\\partial E(\\boldsymbol{w})}{\\partial z_{j}} \\frac{\\partial z_j}{\\partial h_j}\\right)x_{i} \\tag{5.10}\\\\ \\end{align} $$\n$$ \\frac{\\partial E(\\boldsymbol{w})}{\\partial z_j} = \\sum\\limits_k\\frac{\\partial E(\\boldsymbol{w})}{\\partial y_k}\\frac{\\partial y_k}{\\partial z_j}= \\sum\\limits_k (y_k- \\hat{y}_k) w_{kj}^{(2)} \\tag{5.11} $$\n$\\text{Remark.}$ $z_j$ can send information to all the output neurons (e.g., $y_k$), thus we need to sum over all the derivatives with respect to $z_j$.\nSubstituting $\\text{(4.11)}$ into $\\text{(4.10)}$ we obtain\n$$ \\frac{\\partial E(\\boldsymbol{w})}{\\partial w_{ji}^{(1)}} = (1 - z_j^2)x_{i} \\sum\\limits_{k} (y_{k} - \\hat{y}_{k}) w_{kj}^{(2)} \\tag{5.12} $$\n6. Interpretation Recall the Taylor approximation of the two variables function:\n$$ f(x, y) = f(x_0, y_0) + f_x (x- x_0) + f_y(y-y_0) \\tag{6.1} $$\n$\\text{Remark.}$ $(x, y)$ needs to be close to $(x_0, y_0)$, otherwise the approximation can fail.\nWe can transform $\\text{(5.1)}$ into $\\text{(5.3)}$:\n$$ \\begin{align} f(x,y) - f(x_{0},y_0) \u0026amp;= f_x (x- x_0) + f_y(y-y_0) \\tag{6.2}\\\\\\ \\implies \\Delta f \u0026amp;= f_x \\Delta x + f_y \\Delta y\\tag{6.3} \\end{align} $$ If we apply $\\text{(5.3)}$ in the example above, we can obtain\n$$ \\Delta E(\\boldsymbol{w}) = \\nabla_{\\boldsymbol{w}}E(\\boldsymbol{w}) \\Delta \\boldsymbol{w} \\tag{6.4} $$\nFrom another perspective, a small change in the parameters will propagate into a small change in object function by getting multiplied by the gradient.\nTo summarize, back-propagation allows information to flow backwards through the network. This information can tell the model a small change in one particular parameter can result in what change in the object function. And gradient descent can use this information to adjust the parameters for optimizing the object function.\n","permalink":"http://yunpengtai.top/posts/deep-back-propagation/","summary":"1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.\n$$ \\boldsymbol{w}^{\\tau + 1} = \\boldsymbol{w}^{\\tau} - \\eta \\nabla_{\\boldsymbol{w}^{\\tau}} E \\tag{1.1} $$\nwhere $\\eta, \\tau, E$ label learning rate ($\\eta \u0026gt; 0$), the iteration step and the loss function. Wait!","title":"Going Deeper into Back-Propagation"},{"content":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog\u0026rsquo;s interesting part.\nNowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others\u0026rsquo; work (e.g. reproducing a BERT) because everything is there for you. However, when designing a NN or facing a new task, you are most probably trapped somewhere.\nAnd this blog is meant to guide you to handle new problems.\nLet\u0026rsquo;s first begin with some basic rules. Hope you guys enjoy it!\nRush into training neural networks leads to suffering. Training NN is not like writing the common code. For instance, if you plug in a int while it needs a string, errors just come out. However, writing the code about NN can not be so easy for it won\u0026rsquo;t show you bugs automatically (only if you make big mistakes). Sweating the details always pays off. Someone may say the details are infinite and can stop you from marching. Note that the details mentioned here are all necessary instead of some trivialities. And sweating the details can reduce your pain. Observation leads to intuition. Sadly, if you just keep thinking about something, inspiration will never come to you. For instance, if you want to upgrade the algorithm, you had better observe the data where the algorithm fails instead of just thinking about the algorithm. Trusting your intuition instead of your implementation. Sometimes when you come up with a new idea, the implementation of it may go wrong to some degree. When the result is opposite to your assumption, always check your code first before doubting your idea. Quicker, then better. When you are trying to test your hypothesis, use the most efficient way to verify it as fast as possible. Then let us go through concrete parts.\nFamiliar with Data At this stage, you need to do some basic analysis and data mining. Assume we have a classification dataset in NLP. There are several aspects to think about.\nThe Distribution. To begin with, you need to know the label distribution especially and visualize it. For instance, if you observe long-tailed distribution (e.g. the number of the instances for good emotion is 900 while for the bad emotion is 10), then some methods such as data augmentation or cost-sensitive loss functions can play their part. For your interest, you can refer to this up-to-date survey. Similarly, you may also need to know the distribution of the length of sequence. Thus you can set the appropriate maximum sequence length for your task. Moreover, you can also pass the original data through the feature extractor (such as BERT) to gain their representation. Then you can cluster them. Greed is good. I strongly suggest that you look through the whole dataset ambitiously just like the greedy algorithm. And I promise you are bound to find surprise. You can have a whole understanding of the domain of this dataset. And you can choose appropriate pre-trained models according to the domain. Also, remember to understand the labels. Once you understand the labels, you can see if the annotation is contradictory. And you can select certain samples to see the annotation and estimate how noisy the whole dataset is. You may also need to think for the machine. Are the sequences easy to understand? If they are easy to understand, then we do not need to apply very complicated models to tackle this problem. Is the local information more important than the global? Your understanding about the dataset can help you figure out some basic modeling problems and offer you intuition about rule-based methods. Simple quantification. You may need to know the size of the dataset. If the size is small, we can use the simple models such as textCNN or FastText instead of BERT-based models for the complicated models need more data to model the inductive bias. Also, you can write simple code for detecting the duplicate instances and instances which are corrupted (e.g. lack of the label). Stand on the shoulder of the model. When your model is trained on the dataset, you can see how it performs on the dev/eval set. You need to pay attention to those mis-predicted instances (i.e. bad cases) and think about why the prediction is wrong. Is the label wrong or the way of modeling weak to capture these information. Filter / Sort / Clean. You can decide whether to filter or clean the dataset based on your thorough observation. End-to-End Pipeline When you finish observing the dataset, you need to build the simple pipeline to ensure everything goes well.\nFix the random seed. When carrying out the experiments, you had better fix the seed to reduce the influences of randomness on the experiments. As simple as possible. When building the pipeline, you do not have to use very complicated modeling methods, etc. We are just testing. Thus make everything as simple as possible. For instance, you can use the simple classifier such as SVM and MLP (Multi-Layer Perceptron). Record the accuracy and loss. Training accuracy and loss are very useful for you to figure out which difference is beneficial. Also, we do not need complicated tools (e.g. Tensorboard and Wandb) to do so. You can use a list to store things you want and visualize it by matplotlib or write it down in a txt (Sometimes, the data on the terminal can disappear for certain reasons). Track the progress. In python, you can simply use the tqdm to track the progress. And you can also add the immediate accuracy and loss on the progress bar. Believe me, this can reduce your anxiety. Verify the init loss. For the multi-label classification problem, its loss should equal $-\\log (1/ \\text{num classes})$ (with a softmax). For instance, if you need to make the true prediction among 4 labels, the init loss should equal $1.386$. Good Initialization. For regression problems, if the average of your data is 6, then you can initialize the bias as 6 which can lead to fast and stable convergence. One more example, if you want to initialize the weights and you do not want the weights to be influenced by the output shape, then you may prefer Lecun Normal to Glorot Normal (all initialize with $\\text{N(0, scale)}$). Also, normal initialization is better than uniform initialization by experience. Last but not least, when facing an imblanced dataset with ratio 1:10 (positive cases V.S. negative cases), set the bias on the logits so that the model can learn the bias with the first few iterations. # fan_in, fan_out represent the input and output shape scale = 1. # lecun normal scale_lecun /= max(1., fan_in) scale_lecun = sqrt(scale_lecun) # glorot normal scale_glorot /= max(1., fan_in + fan_out) scale_glorot = sqrt(scale_glorot) Human Baseline. If the dataset is very particular and there are few related evaluation methods, you had better set the human baseline in sampled instances. Compared to the human baseline, you can have an idea that where your model has gone. Input-Independent Baseline. You can set the input all zeros and see the performance. And it should be worse than the performance of plugging in your data. Overfit a small batch. The model should overfit a batch of few instances (e.g. 10 or 20). Theoretically speaking, you should achieve the least loss. If the model fails to do so, then you can go and find the foxy bug. Visualize the input before going into the NN. Take Google\u0026rsquo;s code as example, it shows the input tensors when performing classification problems by BERT. This habit has saved me many times when coming up with a brand-new task because the preparation of data can be hard to some degree. Visualize the predictions dynamically over the course of training. By doing so, we can have a direct picture about where the model has gone and how it performs. Try Back-Propogation yourself. Gradients can give you information about what the model depends on to make such predictions. Generalize a special case. You should not write the general functions at the beginning because your thoughts can be easy to change, thus these general functions are fragile. You can generalize a special case when you are sure that it won\u0026rsquo;t change a lot. Overfitting Since we have built a pipeline and tested it, it\u0026rsquo;s time for us to make the model overfit the whole dataset.\nPicking the right model. The selection of models is related to the size of the dataset and complexity of the task. If your dataset is small, you can choose relatively big models to overfit. Borrow experience from the giants. Sometimes you are unfamiliar with the task, you may have no idea about which hyper-parameter to choose (e.g. learning rate). Then you can search some related papers and see the appendix for training details. Carry out many controlled experiments. Deep Learning is a experimental subject. Sometimes observation fails to give you idea about what exactly it will perform. For instance, if you want to know which learning rate is most suitable for this task, try more options to select the best. Remember change a variable once a time to reduce the influence of mixture. Turn to tricks. Tricks are infinite. For instance, you can apply adversarial training like FGM or PGD to improve the model\u0026rsquo;s performance. Also, if permitted, you can use random searching for the best parameters. Regularize More data is better. The most effective way to regularize the model is collecting more real-world data for training. After all, we are using the small set of data to approximate the distribution of the real-world. Data Augmentation. If you lack data, you can apply data augmentation to increase your data. Although this method seems very easy, it does demand your thorough understanding of your data and task. And creative methods can always pay off. For instance, in NLP, you can use back-translation to augment. Cross Validation. You can split the data several times. And use separate data to train some models. Then we can ensemble them to gain the final prediction. Others Always remember to record your results in a good order. For instance, you must record all the parameters and the model\u0026rsquo;s performances at the dev/eval set. You had better record the motivation for you trying out this experiment. Always back up your code and data. When you are trying some new methods, do not just try it on the original code. The same for the data. You need to back up the original pipeline and data for bad things happening. ","permalink":"http://yunpengtai.top/posts/tips-for-training-nn/","summary":"Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog\u0026rsquo;s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well,","title":"Tips for Training Neural Networks"},{"content":" Life is complex, and it has both real and imaginary parts. — Someone\nBasically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell\nTo not know maths is a severe limitation to understanding the world. — Richard Feynman\nProblems worthy of attack prove their worth by fighting back. — Piet Hein\nAn expert is a person who has made all the mistakes that can be made in a very narrow field. — Niels Bohr\nThe beauty of mathematics is only shown to its patient followers. — Maryam Mirzakhani\nThe essence of Mathematics lies precisely in its freedom. — Georg Cantor\nWe must know, we will know. — David Hilbert\nIf your tendency is to make sense out of chaos, start chaos. — Carlos Castaneda\nPerhaps I could best describe my experience of doing mathematics in terms of entering a dark mansion. You go into the first room and it’s dark, completely dark. You stumble around, bumping into the furniture. Gradually, you learn where each piece of furniture. And finally, after six months or so, you find the light switch and turn it on. Suddenly, it’s all illuminated and you can see exactly where you were. Then you enter the next dark room… — Andrew Wiles\nIf we knew what it is we were doing, it would not be called research. Would it? — Albert Einstein\nIf you can’t solve a problem, then there is an easier problem you can solve: find it. — George Pólya\nYoung man, in mathematics you don’t understand things. You just get used to them. — John Von Neumann\nMathematics is the art of giving the same name to different things. — Henri Poincare\nMathematics is the subject in which we never know what we’re talking about, nor whether what we are saying is true. — Bertrand Russell\nAll mathematics models are wrong. Some of them are useful. — George Box\nWhenever something goes wrong, remember 1+1 is still 2. — Someone\nThe study of mathematics is apt to commence in disappointment… We are told that by its aid the stars are weighed and the billions of molecules in a drop of water are counted. Yet, like the ghost of Hamlet’s father, this great science eludes the efforts of our mental weapons to grasp it. — A. N. Whitehead\nWhen one does a theoretical calculation, there are two ways of doing it. Either one should have a clear physical model in mind or a rigorous mathematical basis. — Enrico Fermi\nLet the formula guide your thinking. — Someone\nA mathematician is a device for turning coffee into theorems. — Alfréd Rényi\nAs long as there’s a life, there’s hope. — Stephen Hawking\nEvery problem was once “unsolvable” until it was solved.\nLet us assume we know nothing, which is a reasonable approximation. — D. Kazhdan\nFailure is not an option. Failure is mandatory. The option is whether or not you let failure be the last thing you do. — Howard Tayler\nMathematics is not about numbers, equations, computations, or algorithms: it is about understanding. — William Paul Thurston\nWhat is mathematics? It is only a systematic effort of solving puzzles posed by nature. — Shakuntala Devi\nDoing mathematics should always mean finding patterns and crafting beautiful and meaningful explanations. — Paul Lockhart\nObvious is the most dangerous word in mathematics. — Eric Temple Bell\nMathematics is the poetry of logical ideas. — Albert Einstein\nOnly one thing can comfort your feelings like music, please your heart like paintings, stir your soul like poetry, enhance your wisdom like philosophy, and improve your life like technology — Mathematics. — Klein\n","permalink":"http://yunpengtai.top/posts/quotes/","summary":"Life is complex, and it has both real and imaginary parts. — Someone Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell To not know maths is","title":"Quotes of Mathematicians"},{"content":"该网站所用所有源码均在此仓库，欢迎使用：\nMyPaperMod This is the demo of my improved PaperMod theme. You can visit the introduction: https://yunpengtai.top/posts/hello-world/ HTML 这是基于 Hugo 系列主题第一篇文章，因为之前是在 Jekyll 上进行渲染，故而 Hello World 也有更新\n为啥变动 那么为啥从 Jekyll 变到 Hugo 呢？原因其实有几点：\n之前用的主题看着不好看，感觉很拥挤，之前想改没空，想要个简洁干净，专注于内容的主题 Jekyll 在服务器端的渲染速度实在是不快（本地却很快，不懂） Hugo 的设计更符合我的直觉，而且方便自定义好玩的功能，比如 shortcode 功能 主要是被另一位博主 Li’s Blog 用的主题吸引，就立马换成了新的主题 PaperMod，然而这个主题虽然简洁，但是少了一些我认为必须得有的功能，比如渲染公式，侧边目录等。于是任着喜欢「折腾」的性子，从零开始学 Hugo 的语法，然后四处借鉴学习，花了两天时间添加了一些功能，当然，这篇文章主要介绍相关的 feature，不会涉及到具体的实现\n没有一个主题能完全满足个人的需求，还是从底层原理出发，这样才能「自由自在」地进行更改\n支持 features 基于的主题 PaperMod 的相关 features 便不在继续介绍，介绍一些我加入的特性\n公式渲染：这个主题一开始是没有公式渲染功能的，我通过引入 MathJax 来完成这一点，行内公式如：$a+b=c$，行间公式效果如下： $$ e^{\\pi i} + 1 = 0 $$ 然而，Hugo 引擎本身是有些问题，有些时候多行公式没办法渲染成公式，此时需要先将公式用代码 block 框起来，等网页渲染好之后再去掉代码 block，开始渲染公式 借鉴了 yihui 的实现 评论功能：网上关于评论系统的实现大致分为三种：GitHub 为基础，Disqus 这种官方托管的，以及自我进行托管的。网站之前是用了第三种的 Waline，后来换到了 Artalk，因为后者可以在评论中使用数学公式以及提供邮箱提醒。因为个人比较喜欢玩表情，还在 Artalk 中加入了一些好玩的表情。 侧边目录：目录在原主题中是固定放在文章顶部，这样不利于读者对于长文整体的把握，故而将目录移动到了侧边 借鉴了 sulvblog ，并且修改了一些 css 设置，自动高亮当前目录并且给它添加下划线，这样可以清楚地发现此时的章节。 MarginNote 借鉴了 kennethfriedman 和 scripter ：以往 markdown 都是仅支持脚注，这样不利于文章的阅读，查看相关引用信息需要移至页面底部再返回，而 MarginNote 则给了作者更多自由的空间和方便读者阅读 而且移动至相关的标注会自动高亮对应的 MarginNote，更加方便读者阅读；同时过长会自动转行，方便进行较长的标注 Sidenotes give more life and variety to the page and are the easiest of all to find and read, If carefully designed, they need not enlarge either the page or the cost of printing it. 代码渲染：舍弃了 PaperMod 内置的 an-old-hope.min，重新选择了 atom-one-dark/light 系列 示例代码 class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() # for convenience self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\u0026#34;\\tIn Model: input size\u0026#34;, input.size(), \u0026#34;output size\u0026#34;, output.size()) return output 更新时间：原先主题根本没有更新时间的设置，这样不利于读者查看文章最新的时间，文章的开头便有更新的时间，可以查看 著作权声明：现在网上各种抄袭成风，需要对自己的文章进行声明，添加至文章末尾，例如本文用的是 CC BY-NC-SA 4.0 Attribution-NonCommercial-ShareAlike 4.0 International 字体：原来的字体看着有点不舒服，这里将中文换成了霞鹜文楷，观感很不错，英文的话是在几个字体中选一个：-apple-system, BlinkMacSystemFont, segoe ui, Roboto, Oxygen, Ubuntu, Cantarell 这样看起来也很舒服，选取 Lorem Ipsum 进行测试 Just dummy text Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n中英文空白：汉学家称中文和英文之间的空白字元为「盘古之白」，大抵是因为劈开了全形字和半形字之间的混沌。在书写时适当地留白会提升观感，这个是通过盘古的 js来实现的，手敲空格噼里啪啦的，倒不是很方便 notice：通过 shortcode 添加了各种的提示框 借鉴了 Hugo-notice ，看起来很不错，比如： 一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n一生疏狂尽余欢，半剖肝胆入剑寒。 剑至高危如蜀道，生逢穷途行路难。\n添加了友链功能 借鉴了 sulvblog ，这样方便添加别人的博客，加强朋友之间的联系，可查看主页的 Friends 对于 markdown 引用的改进，原来的引用比较丑，下面是改进后的引用： 借鉴了 Guan Qirui Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell\n添加了 GitHub 仓库的小卡片，比较美观，用了本网站的源码仓库作示例： MyPaperMod This is the demo of my improved PaperMod theme. You can visit the introduction: https://yunpengtai.top/posts/hello-world/ HTML 点击图片进行放大，对于图片大，缩放多的情况，点击放大是很有必要的，不妨试试看： Black Holes: Monsters in Space\n致谢 尽管上文对所用工具均已写明出处，但是这些精妙绝伦的工具值得再单独列出来致谢，同时也方便读者自己 DIY 时进行选择性添加：\nPaperMod，这是一开始基于的主题，感谢作者的开源 评论系统的支持，感谢 Artalk，做的很干净 关于对代码主题的更改，主要借鉴了 HightLight.js 官方的 demo 公式的渲染方面 MathJax 做的很 nice 侧边目录和一些 shortcode 的实现借鉴了 Sulv’s Blog 以及 Guan Qirui 字体还得感谢 霞骛文楷，比那些收费的丑字体好太多了 盘古之白是通过 pangu.js 实现的，点赞 MarginNote 的实现借鉴了 kennethfriedman 和 scripter 图片放大主要用了 fancybox 的功能，用起来也比较方便 ","permalink":"http://yunpengtai.top/posts/hello-world/","summary":"该网站所用所有源码均在此仓库，欢迎使用： MyPaperMod This is the demo of my improved PaperMod theme. You can visit the introduction: https://yunpengtai.top/posts/hello-world/ HTML 这是基于 Hugo 系列主题第一篇文章，因为之前是在 Jekyll 上进行渲染，故而 Hello World 也","title":"新的主题"},{"content":"Here is Yunpeng Tai. I am a geek about mathematics, machine learning and NLP. I’m currently working on the foundamental mechanism of LLMs. If you like my post, you can contact me.\nI obtained a bachelor\u0026rsquo;s degree in Computer Science from Suzhou University of Science and Technology. Currently, I am pursuing a Computer Science MPhil at the Chinese University of Hong Kong, Shenzhen. I am familiar with the topics below:\nOpen-Domained QA Noise Learning Pre-training of LLMs Sentence Rephrasing Machine Reading Comprehension Supervised Fine-Tuning In-context Learning Text Generation Data Augmentation For me, learning is a process of getting freedom. I am a perfectionist and hate the feeling of being limited, thus I will do something by myself for maximum freedom. That is why I have some interesting projects.\nI am easily curious about anything interesting, thus always fail to refuse the thoughts of exploring the unknown. I am super passionate about finding real answers for Artificial Intelligence. I am interested in understanding interesting things. And often to understand something you have to work it out yourself. That’s why I like research.\nI am going to use this blog to record my thoughts. Also, I have been trying really hard to make the knowledge easy to grasp. Possible visualization or examples can be used to explain some topic.\nThere’s lots of ways to be, as a person. And some people express their deep appreciation in different ways. But one of the ways that I believe people express their appreciation to the rest of humanity is to make something wonderful and put it out there. — Steve Jobs\nPlease feel free to reach out to me. Any interesting topic is welcome.\nWeChat: Yunpengtai Email: yunpengtai.typ@gmail.com PGP Fingerprint: CDC0 3AD7 8045 48A4 43CF 0FA4 9ACD 7CA0 3F31 D24B By the way, if you like to contact me by PGP, the content below is my public key.\nPGP Public Key -----BEGIN PGP PUBLIC KEY BLOCK----- mQGNBGRGPZgBDADHN9SKoELU55fEcc5XKPi4cc9W9ksB99Wha9mH2c3m2nHuRrOj ezEZ0lDLL7/DyJfd796ho4+sE8eZkbO0+UvtVShMElQqVFP87XjdsvKjQgN2urEd pUG0h5Wso5h6FT1G8n3uOWG+5w0T7movAtZ4B7nJKAmpc3rBbMejAkVQU00RoQAO zr2DrbrboHzb+SQyspChIvG34goG4y6ovHpVg8qnsWdZgTWWX7i9nB3ryNVAjo+3 ZFMAAZXqo9anoIqma1OTQineXSU7E/yY5K143UWLAgtEOeC8ZH8R5LJctPTdzfGJ d99ZvSwrSc9qXxeEW0DM9dt/q6IA8pVwz1HEEaj9zQtknZtaQapAzizRadwTvcUS VqrrbG6meE0OrZxAmezKYRdCd+Vcgd6jHUyOtuJEHXJ7fmLnXiylzP0xAMHmtIUG 5+o/ht2R5E68irv60NcdJ16FW50GNOMdsEVo1Oy5r/oRsMziD9lc+NCD3YVDrpGk p++3p7Ip6VGT8wMAEQEAAbRQeXVucGVuZ3RhaSAoUGxlYXNlIHNlbmQgbWUgZW1h aWxzIGJ5IHBncCBwdWJsaWMga2V5LikgPHl1bnBlbmd0YWkudHlwQGdtYWlsLmNv bT6JAdEEEwEIADsWIQTNwDrXgEVIpEPPD6SazXygPzHSSwUCZEY9mAIbAwULCQgH AgIiAgYVCgkICwIEFgIDAQIeBwIXgAAKCRCazXygPzHSS8esC/0WkZs+4PzT1zR7 O7RFI2ebuBwY0wRI30KMXvmhbfw+2rnLPnyMk/tvyq6n+xc/njsMDOQd3fR/c/4W pBZ5v4j9F6OMao5pJg3P3Wbm1hM2hkHT66EPHL24kXZE+yzL5FTLYWJCGFkBEzvI VebeKNFalNd78Sg/LeZvF8uiaR+s0+BU1E3nGu8m8vCnMpOrWKmZVMyn8wWsSHUB vINrltqVHuduyvGauwlU+kngvvNTX8LAOGLhR/Z+5Qi5kDwKue7As9gInDao/gI6 WhsTnl32LKF8wB3EWc1si2u1P6KdnqGjwzA+6wz0/WmPLMnMJ0wklKfHdRRe0XoB qucWjFedXbiPLl8AYRdNHOUre5BDpZryC41Ca/VKaAMMaXy1SEoPmM/qClJrDD2U onGkbOuzi82C70K9uJi4PrCdtt2xi5AfjoeQ/MfqRqx1ohSOqF9CCl8e4pBemEux 2wJYvr/vaSucQbFy0CYKBFgFexpQAZtdH4dwLQ3vmmDIt84axGS5AY0EZEY9mAEM AMOxBjf7xe3NfM5667qp+Cz8hXYHOYKMerOl63WzWopHMUoggsCc1hYC9Vnps0kc ACDofNNEfIU/cSQtNd7tpLckHUAGMXERlmoQnrBe+xLg65kVVBwhZht1ypusc2GT iqSMcqQqavVwnVZ3INgHsKqRDkHG/jkgGuCcV1b4tarpUHRS6xsTYpw1BcH/Jcwj BjEN2UsjNSyYqraP9NRwVLHer7rplNPFG+c+AsUFdSDJIlh5Y9rXpUgwh6dS0GRa hgPJu3Tn1t9KjSmrPd/HY1gi1f0ITrNnJnP6FCI4Pdfcg8ZyAOA/bk6QXFo0q/8r uKm7AFcFnNeD6leIZ8G4/LlovWjO6hfSg2d1NByCUu5hDLq5jLi3AyWckO5jOqn3 dbx7M4/T8bg50sTRkjCoGgMoRBVDqlcFjBtF8GAp3IIAQ2/ZUHYMLaMIzpP1bSLV HADPTSzFYIheyVNVjXk4DL0pVTslAVRowTOWabluecWLo/WXBa5CjDrd9eozz6Yo OQARAQABiQG2BBgBCAAgFiEEzcA614BFSKRDzw+kms18oD8x0ksFAmRGPZgCGwwA CgkQms18oD8x0kt5zAwAsPiHRDtz99w3K3wY8ULbWfBOwTekJ4/RJBGRn6dWJB/e FCxBNu2ug4MJSd4QKecmJ4y5fJaoyToPCzWZ2pZDncnfEd3ji4vYJO8hLdePccys KIK9FHImyv/xe7Vb79Xqpi/NmkxAOkCCwgXt87YiaHlDM3Bo615sOAaMO7dYqyjB RAgEcjhBGxFS8uc0q2Vc0AJfpD7/N0zAYg47DYht2QwzmUUatSgolFOt5M3tLdAh HYFs97+0cmWg/8Xva3ug6GeutXH4Pmr8vAX46wuZeylfdFaUtPpwFBlk3q8pGkrU eV6iIZamBlIPY+eX3FeqMPg0xVCzUGHAAcsZiTQ5OvQ3326AiqLrehqxCP7BRBCv 1ASN0CTINCKn8fj37oKlUrvW9eSd+sO2VR2o6oqmWIOh4ItZPKkqf1lVap+xaBQ0 D2oY+2BYnPMP96IWmjjDlkNW5FaE4FuE2uf3UZebcmRS1w+c04p6PpyIFPJfAMr6 Zhpe5QegrHd27n3hksLP =QsdK -----END PGP PUBLIC KEY BLOCK----- ","permalink":"http://yunpengtai.top/about/","summary":"about","title":"🙋🏻‍♂️About"},{"content":" Aaron Swartz 用以缅怀自由斗士 Aaron 科学空间 致力于分享科学之美 依云 Happy coding, happy living! 林一二 林一二的模因和想法 学习者的数字花园 pimgeek 的笔记与思考 CC 康纳百川 CC 康纳百川的小窝 EXEC 嗷呜～～～ 山葵酱 变幻莫测的山葵酱 ","permalink":"http://yunpengtai.top/friends/","summary":"friends","title":"🤝友人"}]